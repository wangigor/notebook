#!/usr/bin/env python3
"""
LLMæœåŠ¡éªŒè¯è„šæœ¬

ç”¨äºéªŒè¯LLMClientServiceçš„å®é™…åŠŸèƒ½
"""
import sys
import os
import asyncio
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.llm_client_service import LLMClientService
from app.core.llm_config import LLMConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_llm_service():
    """æµ‹è¯•LLMæœåŠ¡åŠŸèƒ½"""
    
    print("=" * 50)
    print("LLMæœåŠ¡éªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•å•ä¾‹æ¨¡å¼
        print("\n1. æµ‹è¯•å•ä¾‹æ¨¡å¼...")
        service1 = LLMClientService()
        service2 = LLMClientService()
        assert service1 is service2, "å•ä¾‹æ¨¡å¼å¤±è´¥"
        print("âœ“ å•ä¾‹æ¨¡å¼æ­£å¸¸")
        
        # 2. æµ‹è¯•é…ç½®è·å–
        print("\n2. æµ‹è¯•é…ç½®è·å–...")
        config = LLMConfig.get_default_config(streaming=False)
        print(f"âœ“ é»˜è®¤é…ç½®: {config}")
        
        # 3. æµ‹è¯•LLMå®ä¾‹è·å–
        print("\n3. æµ‹è¯•LLMå®ä¾‹è·å–...")
        
        # éæµå¼LLM
        llm_non_streaming = service1.get_llm(streaming=False)
        print(f"âœ“ éæµå¼LLMå®ä¾‹: {type(llm_non_streaming).__name__}")
        
        # æµå¼LLM
        llm_streaming = service1.get_llm(streaming=True)
        print(f"âœ“ æµå¼LLMå®ä¾‹: {type(llm_streaming).__name__}")
        
        # 4. æµ‹è¯•ä¸“ç”¨æ–¹æ³•
        print("\n4. æµ‹è¯•ä¸“ç”¨æ–¹æ³•...")
        
        chat_llm = service1.get_chat_llm()
        print(f"âœ“ å¯¹è¯LLMå®ä¾‹: {type(chat_llm).__name__}")
        
        processing_llm = service1.get_processing_llm()
        print(f"âœ“ å¤„ç†LLMå®ä¾‹: {type(processing_llm).__name__}")
        
        # 5. æµ‹è¯•ç¼“å­˜æœºåˆ¶
        print("\n5. æµ‹è¯•ç¼“å­˜æœºåˆ¶...")
        
        # å†æ¬¡è·å–ç›¸åŒé…ç½®çš„å®ä¾‹ï¼Œåº”è¯¥ä»ç¼“å­˜è¿”å›
        llm_cached = service1.get_llm(streaming=False)
        assert llm_cached is llm_non_streaming, "ç¼“å­˜æœºåˆ¶å¤±è´¥"
        print("âœ“ ç¼“å­˜æœºåˆ¶æ­£å¸¸")
        
        # 6. æµ‹è¯•ç¼“å­˜ä¿¡æ¯
        print("\n6. æµ‹è¯•ç¼“å­˜ä¿¡æ¯...")
        cache_info = service1.get_cache_info()
        print(f"âœ“ ç¼“å­˜ä¿¡æ¯: {cache_info}")
        
        # 7. æµ‹è¯•æ¸…ç†ç¼“å­˜
        print("\n7. æµ‹è¯•æ¸…ç†ç¼“å­˜...")
        service1.clear_cache()
        cache_info_after = service1.get_cache_info()
        print(f"âœ“ æ¸…ç†åç¼“å­˜ä¿¡æ¯: {cache_info_after}")
        
        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMæœåŠ¡å·¥ä½œæ­£å¸¸")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.exception("æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        return False
    
    return True

def test_configuration():
    """æµ‹è¯•é…ç½®åŠŸèƒ½"""
    
    print("\n" + "=" * 30)
    print("é…ç½®æµ‹è¯•")
    print("=" * 30)
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {"streaming": False},
        {"streaming": True},
        {"streaming": False, "temperature": 0.5},
        {"streaming": True, "model": "gpt-4"},
    ]
    
    for i, config in enumerate(configs, 1):
        print(f"\n{i}. æµ‹è¯•é…ç½®: {config}")
        full_config = LLMConfig.get_default_config(**config)
        print(f"   å®Œæ•´é…ç½®: {full_config}")

if __name__ == "__main__":
    print("å¼€å§‹LLMæœåŠ¡éªŒè¯...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        print("   æŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    
    # è¿è¡Œé…ç½®æµ‹è¯•
    test_configuration()
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    success = asyncio.run(test_llm_service())
    
    if success:
        print("\nğŸ‰ LLMæœåŠ¡éªŒè¯å®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ LLMæœåŠ¡éªŒè¯å¤±è´¥ï¼")
        sys.exit(1) 