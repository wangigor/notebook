import os
from typing import Dict
from pydantic_settings import BaseSettings
from dotenv import load_dotenv
import logging
import secrets
from pydantic import Field

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# è®¾ç½®SSLç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç”¨äºå¼€å‘ç¯å¢ƒï¼‰
if os.getenv("PYTHONHTTPSVERIFY") == "0":
    os.environ["PYTHONHTTPSVERIFY"] = "0"
    os.environ["REQUESTS_CA_BUNDLE"] = ""
    os.environ["CURL_CA_BUNDLE"] = ""
    import ssl
    import urllib3
    # ç¦ç”¨SSLè­¦å‘Š
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

class Settings(BaseSettings):
    # åº”ç”¨åŸºæœ¬é…ç½®
    PROJECT_NAME: str = "Notebook AI Backend"
    VERSION: str = "0.1.0"
    DEBUG: bool = os.getenv("DEBUG", "False").lower() in ("true", "1", "t")
    API_BASE_URL: str = os.getenv("API_BASE_URL", "http://localhost:8000/api")

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/notebook_ai")

    # è®¤è¯é…ç½®
    SECRET_KEY: str = os.getenv("SECRET_KEY", "a_very_secret_key_that_should_be_changed") # ç”Ÿäº§ç¯å¢ƒåŠ¡å¿…ä¿®æ”¹
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7 # 7 days
    
    # å†…éƒ¨APIé…ç½®
    INTERNAL_API_KEY: str = os.getenv("INTERNAL_API_KEY", secrets.token_urlsafe(32)) # é»˜è®¤ç”Ÿæˆéšæœºå¯†é’¥

    # è·¨åŸŸé…ç½®
    CORS_ORIGINS: list[str] = os.getenv("CORS_ORIGINS", "*").split(",") # ç”Ÿäº§ç¯å¢ƒåº”æŒ‡å®šæ˜ç¡®çš„æ¥æº
    
    # WebSocketé…ç½®
    WS_MAX_CONNECTIONS_PER_TASK: int = Field(default=10, description="æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§WebSocketè¿æ¥æ•°")
    WS_PING_INTERVAL: float = float(os.getenv("WS_PING_INTERVAL", "30.0"))  # WebSocketå¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰
    WS_PING_TIMEOUT: float = float(os.getenv("WS_PING_TIMEOUT", "10.0"))    # WebSocketå¿ƒè·³è¶…æ—¶ï¼ˆç§’ï¼‰

    # å‘é‡åµŒå…¥é…ç½®
    VECTOR_SIZE: int = 1536

    # DashScope é…ç½®
    DASHSCOPE_API_KEY: str = os.getenv("DASHSCOPE_API_KEY", "")
    DASHSCOPE_EMBEDDING_MODEL: str = os.getenv("DASHSCOPE_EMBEDDING_MODEL", "text-embedding-v1")

    # Neo4j é…ç½®
    NEO4J_URI: str = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USERNAME: str = os.getenv("NEO4J_USERNAME", "neo4j")
    NEO4J_PASSWORD: str = os.getenv("NEO4J_PASSWORD", "password")
    NEO4J_DATABASE: str = os.getenv("NEO4J_DATABASE", "neo4j")
    
    # å›¾è°±æ„å»ºé…ç½®
    GRAPH_NODE_LABELS: list[str] = os.getenv("GRAPH_NODE_LABELS", "Entity,Concept,Person,Organization").split(",")
    GRAPH_RELATIONSHIP_TYPES: list[str] = os.getenv("GRAPH_RELATIONSHIP_TYPES", "RELATES_TO,CONTAINS,MENTIONS").split(",")
    GRAPH_BATCH_SIZE: int = int(os.getenv("GRAPH_BATCH_SIZE", "100"))
    
    # çŸ¥è¯†æŠ½å–é…ç½®
    KNOWLEDGE_EXTRACTION_BATCH_SIZE: int = int(os.getenv("KNOWLEDGE_EXTRACTION_BATCH_SIZE", "10"))
    KNOWLEDGE_EXTRACTION_MIN_CONFIDENCE: float = float(os.getenv("KNOWLEDGE_EXTRACTION_MIN_CONFIDENCE", "0.5"))
    KNOWLEDGE_EXTRACTION_MAX_RETRIES: int = int(os.getenv("KNOWLEDGE_EXTRACTION_MAX_RETRIES", "3"))
    KNOWLEDGE_EXTRACTION_DELAY_SECONDS: float = float(os.getenv("KNOWLEDGE_EXTRACTION_DELAY_SECONDS", "0.1"))
    
    # å®ä½“æŠ½å–é…ç½®
    ENTITY_TYPES: list[str] = os.getenv("ENTITY_TYPES", "äººç‰©,ç»„ç»‡,åœ°ç‚¹,äº‹ä»¶,æ¦‚å¿µ,æŠ€æœ¯,äº§å“,æ—¶é—´,æ•°å­—,æ³•å¾‹æ¡æ–‡,æ”¿ç­–,é¡¹ç›®,ç³»ç»Ÿ,æ–¹æ³•,ç†è®º").split(",")
    ENTITY_MIN_LENGTH: int = int(os.getenv("ENTITY_MIN_LENGTH", "2"))
    ENTITY_MAX_LENGTH: int = int(os.getenv("ENTITY_MAX_LENGTH", "100"))
    
    # å…³ç³»æŠ½å–é…ç½®  
    RELATIONSHIP_TYPES: list[str] = os.getenv("RELATIONSHIP_TYPES", "å±äº,åŒ…å«,ä½äº,å·¥ä½œäº,åˆ›ç«‹,ç®¡ç†,åˆä½œ,æåŠ,æè¿°,å¼•ç”¨,å¯¼è‡´,å½±å“,ä½¿ç”¨,ä¾èµ–,å®ç°,ç›¸å…³,è¿æ¥,å…³è”").split(",")
    RELATIONSHIP_MIN_CONFIDENCE: float = float(os.getenv("RELATIONSHIP_MIN_CONFIDENCE", "0.5"))
    
    # ğŸ†• å®ä½“ç»Ÿä¸€æ™ºèƒ½åŒ–é…ç½®
    # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®
    ENTITY_UNIFICATION_HIGH_THRESHOLD: float = float(os.getenv("ENTITY_UNIFICATION_HIGH_THRESHOLD", "0.85"))  # é«˜ç½®ä¿¡åº¦è‡ªåŠ¨åˆå¹¶
    ENTITY_UNIFICATION_MEDIUM_THRESHOLD: float = float(os.getenv("ENTITY_UNIFICATION_MEDIUM_THRESHOLD", "0.65"))  # ä¸­ç­‰ç½®ä¿¡åº¦å†²çªæ£€æµ‹
    ENTITY_UNIFICATION_LOW_THRESHOLD: float = float(os.getenv("ENTITY_UNIFICATION_LOW_THRESHOLD", "0.50"))  # ä½ç½®ä¿¡åº¦æ‹’ç»åˆå¹¶
    
    # å¤šç»´åº¦ç›¸ä¼¼åº¦æƒé‡é…ç½®
    ENTITY_SIMILARITY_SEMANTIC_WEIGHT: float = float(os.getenv("ENTITY_SIMILARITY_SEMANTIC_WEIGHT", "0.4"))  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
    ENTITY_SIMILARITY_LEXICAL_WEIGHT: float = float(os.getenv("ENTITY_SIMILARITY_LEXICAL_WEIGHT", "0.3"))   # è¯æ±‡ç›¸ä¼¼åº¦æƒé‡
    ENTITY_SIMILARITY_CONTEXTUAL_WEIGHT: float = float(os.getenv("ENTITY_SIMILARITY_CONTEXTUAL_WEIGHT", "0.3"))  # ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦æƒé‡
    
    # æ‰¹é‡å¤„ç†é…ç½®
    ENTITY_UNIFICATION_BATCH_SIZE: int = int(os.getenv("ENTITY_UNIFICATION_BATCH_SIZE", "100"))  # æ‰¹é‡å¤„ç†å¤§å°
    ENTITY_EMBEDDING_BATCH_SIZE: int = int(os.getenv("ENTITY_EMBEDDING_BATCH_SIZE", "50"))  # embeddingæ‰¹é‡ç”Ÿæˆå¤§å°
    ENTITY_SIMILARITY_CACHE_SIZE: int = int(os.getenv("ENTITY_SIMILARITY_CACHE_SIZE", "1000"))  # ç›¸ä¼¼åº¦ç¼“å­˜å¤§å°
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    ENTITY_UNIFICATION_MAX_MATRIX_SIZE: int = int(os.getenv("ENTITY_UNIFICATION_MAX_MATRIX_SIZE", "10000"))  # æœ€å¤§ç›¸ä¼¼åº¦çŸ©é˜µå¤§å°
    ENTITY_UNIFICATION_PARALLEL_WORKERS: int = int(os.getenv("ENTITY_UNIFICATION_PARALLEL_WORKERS", "4"))  # å¹¶è¡Œå¤„ç†å·¥ä½œè€…æ•°
    ENTITY_UNIFICATION_MEMORY_LIMIT_MB: int = int(os.getenv("ENTITY_UNIFICATION_MEMORY_LIMIT_MB", "2048"))  # å†…å­˜ä½¿ç”¨é™åˆ¶(MB)
    
    # è´¨é‡æ§åˆ¶é…ç½®
    ENTITY_ALIAS_MAX_COUNT: int = int(os.getenv("ENTITY_ALIAS_MAX_COUNT", "20"))  # æ¯ä¸ªå®ä½“æœ€å¤§åˆ«åæ•°é‡
    ENTITY_QUALITY_MIN_SCORE: float = float(os.getenv("ENTITY_QUALITY_MIN_SCORE", "0.3"))  # æœ€ä½è´¨é‡åˆ†æ•°é˜ˆå€¼
    
    # ğŸ†• ç±»å‹åˆ†ç»„ç»Ÿä¸€é…ç½®
    ENTITY_UNIFICATION_ENABLE_TYPE_GROUPING: bool = os.getenv("ENTITY_UNIFICATION_ENABLE_TYPE_GROUPING", "True").lower() in ("true", "1", "t")  # å¯ç”¨ç±»å‹åˆ†ç»„
    ENTITY_UNIFICATION_MAX_ENTITIES_PER_TYPE_BATCH: int = int(os.getenv("ENTITY_UNIFICATION_MAX_ENTITIES_PER_TYPE_BATCH", "50"))  # æ¯ç§ç±»å‹çš„æœ€å¤§æ‰¹å¤„ç†å¤§å°
    
    # ğŸ“Š æ™ºèƒ½å®ä½“ç»Ÿä¸€å·²å…¨é¢å¯ç”¨ï¼ˆç§»é™¤ä¼ ç»Ÿå»é‡å’Œå¼€å…³é…ç½®ï¼‰
    ENTITY_UNIFICATION_DEBUG_MODE: bool = os.getenv("ENTITY_UNIFICATION_DEBUG_MODE", "False").lower() in ("true", "1", "t")  # è°ƒè¯•æ¨¡å¼
    
    # ğŸš€ å¢é‡ç»Ÿä¸€ä¸“ç”¨é…ç½®
    INCREMENTAL_UNIFICATION_ENABLED: bool = os.getenv("INCREMENTAL_UNIFICATION_ENABLED", "True").lower() in ("true", "1", "t")
    INCREMENTAL_BATCH_SIZE: int = int(os.getenv("INCREMENTAL_BATCH_SIZE", "1000"))
    INCREMENTAL_PROCESSING_TIMEOUT: int = int(os.getenv("INCREMENTAL_PROCESSING_TIMEOUT", "300"))  # 5åˆ†é’Ÿ
    INCREMENTAL_QUEUE_MAX_SIZE: int = int(os.getenv("INCREMENTAL_QUEUE_MAX_SIZE", "10000"))
    
    # ğŸ·ï¸ ç±»å‹æ„ŸçŸ¥ç´¢å¼•é…ç½®
    TYPE_AWARE_INDEX_ENABLED: bool = os.getenv("TYPE_AWARE_INDEX_ENABLED", "True").lower() in ("true", "1", "t")
    INDEX_CACHE_SIZE: int = int(os.getenv("INDEX_CACHE_SIZE", "10000"))
    INDEX_CACHE_TTL: int = int(os.getenv("INDEX_CACHE_TTL", "3600"))  # 1å°æ—¶
    FUZZY_MATCH_NGRAM_SIZE: int = int(os.getenv("FUZZY_MATCH_NGRAM_SIZE", "3"))
    
    # ğŸ¯ æ™ºèƒ½å€™é€‰ç”Ÿæˆé…ç½®
    CANDIDATE_GENERATION_MAX_CANDIDATES: int = int(os.getenv("CANDIDATE_GENERATION_MAX_CANDIDATES", "50"))
    CANDIDATE_GENERATION_MIN_SCORE: float = float(os.getenv("CANDIDATE_GENERATION_MIN_SCORE", "0.1"))
    CANDIDATE_STRATEGY_WEIGHTS: Dict[str, float] = {
        'exact_match': float(os.getenv("EXACT_MATCH_WEIGHT", "1.0")),
        'fuzzy_match': float(os.getenv("FUZZY_MATCH_WEIGHT", "0.8")),
        'semantic_match': float(os.getenv("SEMANTIC_MATCH_WEIGHT", "0.9")),
        'graph_structure': float(os.getenv("GRAPH_STRUCTURE_WEIGHT", "0.7"))
    }
    
    # ğŸ” å®ä½“æŒ‡çº¹é…ç½®
    ENTITY_FINGERPRINT_ALGORITHM: str = os.getenv("ENTITY_FINGERPRINT_ALGORITHM", "md5")  # md5, sha1, sha256, xxhash
    ENTITY_FINGERPRINT_TYPE: str = os.getenv("ENTITY_FINGERPRINT_TYPE", "extended")  # basic, extended, semantic, full
    FINGERPRINT_CACHE_SIZE: int = int(os.getenv("FINGERPRINT_CACHE_SIZE", "5000"))
    FINGERPRINT_CACHE_TTL: int = int(os.getenv("FINGERPRINT_CACHE_TTL", "7200"))  # 2å°æ—¶
    
    # ğŸ› ï¸ ç´¢å¼•ç®¡ç†é…ç½®
    INDEX_MANAGEMENT_ENABLED: bool = os.getenv("INDEX_MANAGEMENT_ENABLED", "True").lower() in ("true", "1", "t")
    INDEX_MAINTENANCE_INTERVAL: int = int(os.getenv("INDEX_MAINTENANCE_INTERVAL", "14400"))  # 4å°æ—¶
    INDEX_PERFORMANCE_THRESHOLD: float = float(os.getenv("INDEX_PERFORMANCE_THRESHOLD", "0.8"))
    INDEX_FRAGMENTATION_THRESHOLD: float = float(os.getenv("INDEX_FRAGMENTATION_THRESHOLD", "0.3"))
    INDEX_AUTO_OPTIMIZE_ENABLED: bool = os.getenv("INDEX_AUTO_OPTIMIZE_ENABLED", "True").lower() in ("true", "1", "t")
    INDEX_MAINTENANCE_WORKERS: int = int(os.getenv("INDEX_MAINTENANCE_WORKERS", "2"))
    
    # ğŸ“Š æŠ½æ ·æ£€æµ‹é…ç½®
    SAMPLING_DETECTION_ENABLED: bool = os.getenv("SAMPLING_DETECTION_ENABLED", "True").lower() in ("true", "1", "t")
    SAMPLING_INTERVAL_HOURS: int = int(os.getenv("SAMPLING_INTERVAL_HOURS", "4"))
    SAMPLING_SIZE_PER_TYPE: int = int(os.getenv("SAMPLING_SIZE_PER_TYPE", "1000"))
    SAMPLING_STRATEGIES: Dict[str, float] = {
        'random': float(os.getenv("SAMPLING_RANDOM_RATIO", "0.3")),
        'quality_based': float(os.getenv("SAMPLING_QUALITY_RATIO", "0.4")),
        'time_based': float(os.getenv("SAMPLING_TIME_RATIO", "0.2")),
        'conflict_prone': float(os.getenv("SAMPLING_CONFLICT_RATIO", "0.1"))
    }
    
    # ğŸš€ å…¨å±€è¯­ä¹‰ç»Ÿä¸€é…ç½® (v2)
    GLOBAL_SEMANTIC_UNIFICATION_ENABLED: bool = os.getenv("GLOBAL_SEMANTIC_UNIFICATION_ENABLED", "True").lower() in ("true", "1", "t")
    GLOBAL_UNIFICATION_MAX_SAMPLE_PER_TYPE: int = int(os.getenv("GLOBAL_UNIFICATION_MAX_SAMPLE_PER_TYPE", "50"))
    GLOBAL_UNIFICATION_MIN_ENTITIES: int = int(os.getenv("GLOBAL_UNIFICATION_MIN_ENTITIES", "2"))
    GLOBAL_UNIFICATION_LLM_CONFIDENCE_THRESHOLD: float = float(os.getenv("GLOBAL_UNIFICATION_LLM_CONFIDENCE_THRESHOLD", "0.7"))
    GLOBAL_UNIFICATION_MAX_BATCH_SIZE: int = int(os.getenv("GLOBAL_UNIFICATION_MAX_BATCH_SIZE", "20"))
    GLOBAL_UNIFICATION_ENABLE_QUALITY_BOOST: bool = os.getenv("GLOBAL_UNIFICATION_ENABLE_QUALITY_BOOST", "True").lower() in ("true", "1", "t")
    GLOBAL_UNIFICATION_ENABLE_CROSS_DOCUMENT: bool = os.getenv("GLOBAL_UNIFICATION_ENABLE_CROSS_DOCUMENT", "True").lower() in ("true", "1", "t")
    
    # ğŸ¤– LLMè¯­ä¹‰å»é‡é…ç½®
    LLM_DEDUPLICATION_MAX_RETRY: int = int(os.getenv("LLM_DEDUPLICATION_MAX_RETRY", "3"))
    LLM_DEDUPLICATION_TIMEOUT: int = int(os.getenv("LLM_DEDUPLICATION_TIMEOUT", "120"))  # 2åˆ†é’Ÿ
    LLM_DEDUPLICATION_TEMPERATURE: float = float(os.getenv("LLM_DEDUPLICATION_TEMPERATURE", "0.1"))
    LLM_DEDUPLICATION_MAX_TOKENS: int = int(os.getenv("LLM_DEDUPLICATION_MAX_TOKENS", "4000"))
    
    # ğŸ—„ï¸ Neo4jé‡‡æ ·é…ç½®
    NEO4J_SAMPLING_CONNECTION_POOL_SIZE: int = int(os.getenv("NEO4J_SAMPLING_CONNECTION_POOL_SIZE", "5"))
    NEO4J_SAMPLING_TIMEOUT: int = int(os.getenv("NEO4J_SAMPLING_TIMEOUT", "30"))
    NEO4J_SAMPLING_CACHE_SIZE: int = int(os.getenv("NEO4J_SAMPLING_CACHE_SIZE", "1000"))
    NEO4J_SAMPLING_CACHE_TTL: int = int(os.getenv("NEO4J_SAMPLING_CACHE_TTL", "1800"))  # 30åˆ†é’Ÿ
    
    # ğŸ“‹ å›¾è°±åå¤„ç†é…ç½®
    POST_GRAPH_UNIFICATION_MODE: str = os.getenv("POST_GRAPH_UNIFICATION_MODE", "global_semantic")  # sampling, incremental, global_semantic
    
    # ğŸ”„ å®ä½“ç”Ÿå‘½å‘¨æœŸé…ç½®
    ENTITY_LIFECYCLE_NEW_THRESHOLD_HOURS: int = int(os.getenv("ENTITY_LIFECYCLE_NEW_THRESHOLD_HOURS", "24"))
    ENTITY_LIFECYCLE_DEPRECATED_THRESHOLD_DAYS: int = int(os.getenv("ENTITY_LIFECYCLE_DEPRECATED_THRESHOLD_DAYS", "90"))
    ENTITY_LIFECYCLE_SUSPICIOUS_QUALITY_THRESHOLD: float = float(os.getenv("ENTITY_LIFECYCLE_SUSPICIOUS_QUALITY_THRESHOLD", "0.6"))
    
    # ğŸ“ˆ æ€§èƒ½ç›‘æ§é…ç½®
    PERFORMANCE_MONITORING_ENABLED: bool = os.getenv("PERFORMANCE_MONITORING_ENABLED", "True").lower() in ("true", "1", "t")
    PERFORMANCE_HISTORY_RETENTION_DAYS: int = int(os.getenv("PERFORMANCE_HISTORY_RETENTION_DAYS", "7"))
    PERFORMANCE_METRICS_COLLECTION_INTERVAL: int = int(os.getenv("PERFORMANCE_METRICS_COLLECTION_INTERVAL", "300"))  # 5åˆ†é’Ÿ
    
    # ğŸ›ï¸ è°ƒè¯•å’Œæ—¥å¿—é…ç½®
    INCREMENTAL_UNIFICATION_VERBOSE_LOGGING: bool = os.getenv("INCREMENTAL_UNIFICATION_VERBOSE_LOGGING", "False").lower() in ("true", "1", "t")
    INDEX_PERFORMANCE_LOGGING: bool = os.getenv("INDEX_PERFORMANCE_LOGGING", "False").lower() in ("true", "1", "t")
    CANDIDATE_GENERATION_LOGGING: bool = os.getenv("CANDIDATE_GENERATION_LOGGING", "False").lower() in ("true", "1", "t")
    
    # ğŸš¦ é™æµå’Œä¿æŠ¤é…ç½®
    ENTITY_PROCESSING_RATE_LIMIT: int = int(os.getenv("ENTITY_PROCESSING_RATE_LIMIT", "1000"))  # æ¯ç§’å¤„ç†å®ä½“æ•°
    MEMORY_USAGE_THRESHOLD_MB: int = int(os.getenv("MEMORY_USAGE_THRESHOLD_MB", "4096"))  # 4GB
    INDEX_SIZE_LIMIT_MB: int = int(os.getenv("INDEX_SIZE_LIMIT_MB", "1024"))  # 1GB

    # ç¤¾åŒºæ£€æµ‹é…ç½®
    COMMUNITY_MAX_LEVELS: int = int(os.getenv("COMMUNITY_MAX_LEVELS", "3"))
    COMMUNITY_MIN_SIZE: int = int(os.getenv("COMMUNITY_MIN_SIZE", "1"))
    COMMUNITY_MAX_WORKERS: int = int(os.getenv("COMMUNITY_MAX_WORKERS", "10"))
    COMMUNITY_LLM_MODEL: str = os.getenv("COMMUNITY_LLM_MODEL", "gpt-4o")
    COMMUNITY_EMBEDDING_MODEL: str = os.getenv("COMMUNITY_EMBEDDING_MODEL", "all-MiniLM-L6-v2")
    COMMUNITY_BATCH_SIZE: int = int(os.getenv("COMMUNITY_BATCH_SIZE", "100"))
    COMMUNITY_TIMEOUT_MINUTES: int = int(os.getenv("COMMUNITY_TIMEOUT_MINUTES", "30"))

    # MinIO é…ç½®
    MINIO_ENDPOINT: str = os.getenv("MINIO_ENDPOINT", "127.0.0.1:9000")
    MINIO_ACCESS_KEY: str = os.getenv("MINIO_ACCESS_KEY", "minio")
    MINIO_SECRET_KEY: str = os.getenv("MINIO_SECRET_KEY", "minioxxx")
    MINIO_SECURE: bool = os.getenv("MINIO_SECURE", "False").lower() in ("true", "1", "t")
    MINIO_BUCKET_NAME: str = os.getenv("MINIO_BUCKET_NAME", "notebook-ai")
    DOCUMENT_BUCKET: str = os.getenv("DOCUMENT_BUCKET", os.getenv("MINIO_BUCKET_NAME", "notebook-ai"))

    # Agent é…ç½®
    AGENT_MAX_TOKEN_LIMIT: int = int(os.getenv("AGENT_MAX_TOKEN_LIMIT", "2000"))
    AGENT_RETURN_MESSAGES: bool = os.getenv("AGENT_RETURN_MESSAGES", "True").lower() in ("true", "1", "t")
    AGENT_RETURN_SOURCE_DOCUMENTS: bool = os.getenv("AGENT_RETURN_SOURCE_DOCUMENTS", "True").lower() in ("true", "1", "t")
    AGENT_K: int = int(os.getenv("AGENT_K", "5")) # ç›¸ä¼¼æ–‡æ¡£æ•°é‡
    
    # OpenAI é…ç½®
    OPENAI_API_KEY: str | None = os.getenv("OPENAI_API_KEY")
    OPENAI_API_BASE: str | None = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    
    # Redis å’Œ Celery é…ç½®
    REDIS_HOST: str = os.getenv("REDIS_HOST", "localhost")
    REDIS_PORT: int = int(os.getenv("REDIS_PORT", "6379"))
    REDIS_DB: int = int(os.getenv("REDIS_DB", "0"))
    REDIS_PASSWORD: str | None = os.getenv("REDIS_PASSWORD")
    CELERY_BROKER_URL: str = f"redis://{':' + REDIS_PASSWORD + '@' if REDIS_PASSWORD else ''}{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}"
    CELERY_RESULT_BACKEND: str = CELERY_BROKER_URL
    
    # æœåŠ¡å™¨é…ç½®
    HOST: str = os.getenv("HOST", "0.0.0.0")
    PORT: int = int(os.getenv("PORT", "8000"))

    # æœç´¢è°ƒè¯•å’Œæ€§èƒ½ç›‘æ§é…ç½®
    SEARCH_DEBUG_MODE: bool = Field(default=False, description="æœç´¢è°ƒè¯•æ¨¡å¼å¼€å…³")
    SEARCH_LOG_LEVEL: str = Field(default="INFO", description="æœç´¢æ—¥å¿—çº§åˆ«")
    SEARCH_PERFORMANCE_LOG: bool = Field(default=True, description="å¯ç”¨æœç´¢æ€§èƒ½æ—¥å¿—")
    SEARCH_NODE_DETAIL_LOG: bool = Field(default=False, description="å¯ç”¨èŠ‚ç‚¹è¯¦ç»†æ—¥å¿—")
    
    # æœç´¢æ€§èƒ½é˜ˆå€¼è®¾ç½®
    SEARCH_SLOW_QUERY_THRESHOLD: float = Field(default=2.0, description="æ…¢æŸ¥è¯¢é˜ˆå€¼(ç§’)")
    SEARCH_MEMORY_WARNING_THRESHOLD: float = Field(default=100.0, description="å†…å­˜ä½¿ç”¨è­¦å‘Šé˜ˆå€¼(MB)")
    SEARCH_RESULT_QUALITY_THRESHOLD: float = Field(default=0.7, description="æœç´¢ç»“æœè´¨é‡é˜ˆå€¼")
    
    # æœç´¢æŒ‡æ ‡æ”¶é›†é…ç½®
    SEARCH_METRICS_ENABLED: bool = Field(default=True, description="å¯ç”¨æœç´¢æŒ‡æ ‡æ”¶é›†")
    SEARCH_METRICS_HISTORY_SIZE: int = Field(default=100, description="æœç´¢æŒ‡æ ‡å†å²è®°å½•å¤§å°")
    SEARCH_METRICS_REPORT_INTERVAL: int = Field(default=10, description="æœç´¢æŒ‡æ ‡æŠ¥å‘Šé—´éš”(æ¬¡æ•°)")

    # ğŸ†• æ–‡æ¡£è§£æåå®ä½“ç»Ÿä¸€é…ç½®
    ENABLE_POST_EXTRACTION_UNIFICATION: bool = True
    ENABLE_POST_GRAPH_UNIFICATION: bool = True
    DEFAULT_UNIFICATION_MODE: str = "incremental"  # 'incremental' æˆ– 'sampling'
    POST_GRAPH_UNIFICATION_MODE: str = "sampling"  # å›¾è°±æ„å»ºåä½¿ç”¨æŠ½æ ·æ¨¡å¼

    model_config = {
        "case_sensitive": True,
        "env_file": ".env", 
        "env_file_encoding": "utf-8",
        "extra": "ignore"  # å…è®¸é¢å¤–å­—æ®µ
    }

# åˆ›å»º Settings å®ä¾‹
settings = Settings()

# æ‰“å°éƒ¨åˆ†é…ç½®ä¿¡æ¯ä»¥ä¾›è°ƒè¯• (ä»…åœ¨ DEBUG æ¨¡å¼ä¸‹)
if settings.DEBUG:
    logger.info("--- åº”ç”¨é…ç½® ---")
    logger.info(f"Project Name: {settings.PROJECT_NAME}")
    logger.info(f"Debug Mode: {settings.DEBUG}")
    logger.info(f"API Base URL: {settings.API_BASE_URL}")
    logger.info(f"Database URL (masked): {settings.DATABASE_URL.split('@')[-1] if '@' in settings.DATABASE_URL else '...'}")
    logger.info(f"CORS Origins: {settings.CORS_ORIGINS}")
    logger.info(f"WebSocket Max Connections Per Task: {settings.WS_MAX_CONNECTIONS_PER_TASK}")
    logger.info(f"Vector Size: {settings.VECTOR_SIZE}")
    logger.info(f"DashScope Embedding Model: {settings.DASHSCOPE_EMBEDDING_MODEL}")
    logger.info(f"Neo4j URI: {settings.NEO4J_URI}")
    logger.info(f"Neo4j Database: {settings.NEO4J_DATABASE}")
    logger.info(f"Graph Node Labels: {settings.GRAPH_NODE_LABELS}")
    logger.info(f"MinIO Endpoint: {settings.MINIO_ENDPOINT}")
    logger.info(f"MinIO Bucket: {settings.MINIO_BUCKET_NAME}")
    logger.info(f"Document Bucket: {settings.DOCUMENT_BUCKET}")
    logger.info(f"Redis URL: {settings.REDIS_HOST}:{settings.REDIS_PORT}")
    logger.info("----------------") 