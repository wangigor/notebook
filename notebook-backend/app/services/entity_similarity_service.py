# -*- coding: utf-8 -*-
"""
å®ä½“ç›¸ä¼¼åº¦è®¡ç®—æœåŠ¡
å®ç°åŸºäºå¤šç»´åº¦çš„å®ä½“ç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ”¯æŒè¯­ä¹‰ã€è¯æ±‡å’Œä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
"""
import logging
import asyncio
import difflib
import re
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from app.core.config import settings
from app.services.embedding_service import get_embedding_service

logger = logging.getLogger(__name__)


@dataclass
class SimilarityResult:
    """ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ"""
    total_similarity: float
    semantic_similarity: float
    lexical_similarity: float
    contextual_similarity: float
    confidence: float
    details: Dict[str, Any]


class EntitySimilarityCalculator:
    """
    å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨
    
    æ”¯æŒå¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—ï¼š
    - è¯­ä¹‰ç›¸ä¼¼åº¦ï¼šåŸºäºembeddingå‘é‡çš„cosineç›¸ä¼¼åº¦ï¼ˆæƒé‡40%ï¼‰
    - è¯æ±‡ç›¸ä¼¼åº¦ï¼šç¼–è¾‘è·ç¦»+åˆ«ååŒ¹é…ï¼ˆæƒé‡30%ï¼‰
    - ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ï¼šç±»å‹+æè¿°+å…±ç°åˆ†æï¼ˆæƒé‡30%ï¼‰
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨"""
        self.embedding_service = get_embedding_service()
        
        # ä»é…ç½®åŠ è½½æƒé‡
        self.semantic_weight = settings.ENTITY_SIMILARITY_SEMANTIC_WEIGHT
        self.lexical_weight = settings.ENTITY_SIMILARITY_LEXICAL_WEIGHT
        self.contextual_weight = settings.ENTITY_SIMILARITY_CONTEXTUAL_WEIGHT
        
        # ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜
        self._similarity_cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        logger.info(f"å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨å·²åˆå§‹åŒ–ï¼Œæƒé‡é…ç½®: è¯­ä¹‰{self.semantic_weight:.1f}, "
                   f"è¯æ±‡{self.lexical_weight:.1f}, ä¸Šä¸‹æ–‡{self.contextual_weight:.1f}")
    
    async def calculate_similarity(self, entity1, entity2) -> SimilarityResult:
        """
        è®¡ç®—ä¸¤ä¸ªå®ä½“çš„ç»¼åˆç›¸ä¼¼åº¦
        
        Args:
            entity1: ç¬¬ä¸€ä¸ªå®ä½“
            entity2: ç¬¬äºŒä¸ªå®ä½“
            
        Returns:
            SimilarityResult: ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_similarity_cache_key(entity1, entity2)
            if cache_key in self._similarity_cache:
                self._cache_hits += 1
                logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜å‘½ä¸­: {entity1.name} <-> {entity2.name}")
                return self._similarity_cache[cache_key]
            
            self._cache_misses += 1
            
            # 1. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºembeddingå‘é‡ï¼‰
            semantic_sim = await self._calculate_semantic_similarity(entity1, entity2)
            
            # 2. è®¡ç®—è¯æ±‡ç›¸ä¼¼åº¦ï¼ˆç¼–è¾‘è·ç¦»+åˆ«ååŒ¹é…ï¼‰
            lexical_sim = self._calculate_lexical_similarity(entity1, entity2)
            
            # 3. è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ï¼ˆç±»å‹+æè¿°+å…±ç°ï¼‰
            contextual_sim = self._calculate_contextual_similarity(entity1, entity2)
            
            # 4. è®¡ç®—åŠ æƒæ€»ç›¸ä¼¼åº¦
            total_sim = (
                self.semantic_weight * semantic_sim +
                self.lexical_weight * lexical_sim +
                self.contextual_weight * contextual_sim
            )
            
            # 5. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå„ç»´åº¦çš„ä¸€è‡´æ€§ï¼‰
            confidence = self._calculate_confidence(semantic_sim, lexical_sim, contextual_sim)
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = SimilarityResult(
                total_similarity=total_sim,
                semantic_similarity=semantic_sim,
                lexical_similarity=lexical_sim,
                contextual_similarity=contextual_sim,
                confidence=confidence,
                details={
                    "entity1_name": entity1.name,
                    "entity2_name": entity2.name,
                    "entity1_type": entity1.type,
                    "entity2_type": entity2.type,
                    "weights_used": {
                        "semantic": self.semantic_weight,
                        "lexical": self.lexical_weight,
                        "contextual": self.contextual_weight
                    }
                }
            )
            
            # ç¼“å­˜ç»“æœ
            self._similarity_cache[cache_key] = result
            
            # ç¼“å­˜å¤§å°æ§åˆ¶
            if len(self._similarity_cache) > settings.ENTITY_SIMILARITY_CACHE_SIZE:
                self._clean_similarity_cache()
            
            logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: {entity1.name} <-> {entity2.name} = {total_sim:.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {entity1.name} <-> {entity2.name}, é”™è¯¯: {str(e)}")
            # è¿”å›é»˜è®¤çš„ä½ç›¸ä¼¼åº¦ç»“æœ
            return SimilarityResult(
                total_similarity=0.0,
                semantic_similarity=0.0,
                lexical_similarity=0.0,
                contextual_similarity=0.0,
                confidence=0.0,
                details={"error": str(e)}
            )
    
    async def _calculate_semantic_similarity(self, entity1, entity2) -> float:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºembeddingå‘é‡çš„cosineç›¸ä¼¼åº¦ï¼‰
        
        Args:
            entity1: ç¬¬ä¸€ä¸ªå®ä½“
            entity2: ç¬¬äºŒä¸ªå®ä½“
            
        Returns:
            è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•° [0.0, 1.0]
        """
        try:
            # ç¡®ä¿ä¸¤ä¸ªå®ä½“éƒ½æœ‰embeddingå‘é‡
            embedding1 = getattr(entity1, 'embedding', None)
            embedding2 = getattr(entity2, 'embedding', None)
            
            # å¦‚æœç¼ºå°‘embeddingï¼Œå°è¯•ç”Ÿæˆ
            if embedding1 is None or embedding2 is None:
                entities_to_embed = []
                texts_to_embed = []
                
                if embedding1 is None:
                    entities_to_embed.append(entity1)
                    texts_to_embed.append(self._get_entity_text_representation(entity1))
                    
                if embedding2 is None:
                    entities_to_embed.append(entity2)
                    texts_to_embed.append(self._get_entity_text_representation(entity2))
                
                # æ‰¹é‡ç”Ÿæˆembedding
                if texts_to_embed:
                    new_embeddings = await self.embedding_service.embed_documents_batch(texts_to_embed)
                    
                    # æ›´æ–°å®ä½“çš„embedding
                    for i, entity in enumerate(entities_to_embed):
                        if i < len(new_embeddings):
                            entity.embedding = new_embeddings[i]
                
                # é‡æ–°è·å–embedding
                embedding1 = getattr(entity1, 'embedding', None)
                embedding2 = getattr(entity2, 'embedding', None)
            
            # å¦‚æœä»ç„¶ç¼ºå°‘embeddingï¼Œè¿”å›0
            if embedding1 is None or embedding2 is None:
                logger.warning(f"æ— æ³•è·å–embeddingå‘é‡: {entity1.name} or {entity2.name}")
                return 0.0
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            vec1 = np.array(embedding1, dtype=np.float32)
            vec2 = np.array(embedding2, dtype=np.float32)
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            if len(vec1) != len(vec2):
                logger.warning(f"Embeddingç»´åº¦ä¸åŒ¹é…: {len(vec1)} vs {len(vec2)}")
                return 0.0
            
            # è®¡ç®—cosineç›¸ä¼¼åº¦
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                logger.warning("å‘ç°é›¶å‘é‡ï¼Œæ— æ³•è®¡ç®—cosineç›¸ä¼¼åº¦")
                return 0.0
            
            cosine_sim = np.dot(vec1, vec2) / (norm1 * norm2)
            
            # å°†cosineç›¸ä¼¼åº¦ä»[-1, 1]æ˜ å°„åˆ°[0, 1]
            normalized_sim = (cosine_sim + 1.0) / 2.0
            
            # ç¡®ä¿ç»“æœåœ¨[0, 1]èŒƒå›´å†…
            return max(0.0, min(1.0, float(normalized_sim)))
            
        except Exception as e:
            logger.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_lexical_similarity(self, entity1, entity2) -> float:
        """
        è®¡ç®—è¯æ±‡ç›¸ä¼¼åº¦ï¼ˆç¼–è¾‘è·ç¦»+åˆ«ååŒ¹é…ï¼‰
        
        Args:
            entity1: ç¬¬ä¸€ä¸ªå®ä½“
            entity2: ç¬¬äºŒä¸ªå®ä½“
            
        Returns:
            è¯æ±‡ç›¸ä¼¼åº¦åˆ†æ•° [0.0, 1.0]
        """
        try:
            # 1. ä¸»åç§°ç›¸ä¼¼åº¦ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
            name_sim = self._calculate_string_similarity(entity1.name, entity2.name)
            
            # 2. åˆ«ååŒ¹é…ç›¸ä¼¼åº¦
            alias_sim = self._calculate_alias_similarity(entity1, entity2)
            
            # 3. åˆå¹¶è¯æ±‡ç›¸ä¼¼åº¦ï¼ˆå–æœ€å¤§å€¼ï¼‰
            lexical_sim = max(name_sim, alias_sim)
            
            logger.debug(f"è¯æ±‡ç›¸ä¼¼åº¦: {entity1.name} <-> {entity2.name} = {lexical_sim:.3f} "
                        f"(åç§°: {name_sim:.3f}, åˆ«å: {alias_sim:.3f})")
            
            return lexical_sim
            
        except Exception as e:
            logger.warning(f"è¯æ±‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_contextual_similarity(self, entity1, entity2) -> float:
        """
        è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ï¼ˆç±»å‹+æè¿°+å…±ç°ï¼‰
        
        Args:
            entity1: ç¬¬ä¸€ä¸ªå®ä½“
            entity2: ç¬¬äºŒä¸ªå®ä½“
            
        Returns:
            ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦åˆ†æ•° [0.0, 1.0]
        """
        try:
            # 1. å®ä½“ç±»å‹åŒ¹é…ï¼ˆæƒé‡50%ï¼‰
            type_sim = 1.0 if entity1.type == entity2.type else 0.0
            
            # 2. æè¿°ç›¸ä¼¼åº¦ï¼ˆæƒé‡30%ï¼‰
            desc_sim = self._calculate_description_similarity(entity1, entity2)
            
            # 3. æºæ–‡æœ¬ä¸Šä¸‹æ–‡é‡å åº¦ï¼ˆæƒé‡20%ï¼‰
            context_sim = self._calculate_context_overlap(entity1, entity2)
            
            # 4. åˆå¹¶ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
            contextual_sim = 0.5 * type_sim + 0.3 * desc_sim + 0.2 * context_sim
            
            logger.debug(f"ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦: {entity1.name} <-> {entity2.name} = {contextual_sim:.3f} "
                        f"(ç±»å‹: {type_sim:.3f}, æè¿°: {desc_sim:.3f}, ä¸Šä¸‹æ–‡: {context_sim:.3f})")
            
            return contextual_sim
            
        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0
    
    def _get_entity_text_representation(self, entity) -> str:
        """è·å–å®ä½“çš„æ–‡æœ¬è¡¨ç¤ºï¼Œç”¨äºç”Ÿæˆembedding"""
        parts = [entity.name]
        
        if entity.type:
            parts.append(f"ç±»å‹:{entity.type}")
        
        if entity.description:
            parts.append(f"æè¿°:{entity.description}")
        
        return " ".join(parts)
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰"""
        if not str1 or not str2:
            return 0.0
        
        # æ ‡å‡†åŒ–å­—ç¬¦ä¸²
        norm_str1 = self._normalize_string(str1)
        norm_str2 = self._normalize_string(str2)
        
        if norm_str1 == norm_str2:
            return 1.0
        
        # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, norm_str1, norm_str2).ratio()
        return float(similarity)
    
    def _calculate_alias_similarity(self, entity1, entity2) -> float:
        """è®¡ç®—åˆ«ååŒ¹é…ç›¸ä¼¼åº¦"""
        aliases1 = getattr(entity1, 'aliases', []) or []
        aliases2 = getattr(entity2, 'aliases', []) or []
        
        # æ„å»ºæ‰€æœ‰å¯èƒ½çš„åç§°é›†åˆ
        names1 = [entity1.name] + aliases1
        names2 = [entity2.name] + aliases2
        
        max_similarity = 0.0
        
        # è®¡ç®—æ‰€æœ‰åç§°ç»„åˆçš„æœ€å¤§ç›¸ä¼¼åº¦
        for name1 in names1:
            for name2 in names2:
                sim = self._calculate_string_similarity(name1, name2)
                max_similarity = max(max_similarity, sim)
        
        return max_similarity
    
    def _calculate_description_similarity(self, entity1, entity2) -> float:
        """è®¡ç®—æè¿°ç›¸ä¼¼åº¦"""
        desc1 = getattr(entity1, 'description', '') or ''
        desc2 = getattr(entity2, 'description', '') or ''
        
        if not desc1 or not desc2:
            return 0.0
        
        return self._calculate_string_similarity(desc1, desc2)
    
    def _calculate_context_overlap(self, entity1, entity2) -> float:
        """è®¡ç®—æºæ–‡æœ¬ä¸Šä¸‹æ–‡é‡å åº¦"""
        context1 = getattr(entity1, 'source_text', '') or ''
        context2 = getattr(entity2, 'source_text', '') or ''
        
        if not context1 or not context2:
            return 0.0
        
        # æå–å…³é”®è¯
        words1 = set(self._extract_keywords(context1))
        words2 = set(self._extract_keywords(context2))
        
        if not words1 or not words2:
            return 0.0
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _normalize_string(self, text: str) -> str:
        """æ ‡å‡†åŒ–å­—ç¬¦ä¸²"""
        # è½¬å°å†™
        text = text.lower()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', '', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼šç§»é™¤åœè¯ï¼Œä¿ç•™é•¿åº¦>2çš„è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'ç„¶è€Œ', 'å› æ­¤', 'æ‰€ä»¥'}
        
        words = re.findall(r'\w+', text.lower())
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords
    
    def _calculate_confidence(self, semantic_sim: float, lexical_sim: float, contextual_sim: float) -> float:
        """
        è®¡ç®—ç›¸ä¼¼åº¦ç»“æœçš„ç½®ä¿¡åº¦
        
        åŸºäºå„ç»´åº¦ç›¸ä¼¼åº¦çš„ä¸€è‡´æ€§å’Œåˆ†å¸ƒæ¥è¯„ä¼°ç½®ä¿¡åº¦
        """
        similarities = [semantic_sim, lexical_sim, contextual_sim]
        
        # è®¡ç®—æ ‡å‡†å·®ï¼ˆä¸€è‡´æ€§æŒ‡æ ‡ï¼‰
        mean_sim = np.mean(similarities)
        std_sim = np.std(similarities)
        
        # ç½®ä¿¡åº¦ä¸ä¸€è‡´æ€§æˆæ­£æ¯”ï¼Œä¸æ ‡å‡†å·®æˆåæ¯”
        consistency_score = 1.0 - min(std_sim / 0.5, 1.0)  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        magnitude_score = mean_sim  # å¹³å‡ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        
        confidence = 0.7 * consistency_score + 0.3 * magnitude_score
        
        return max(0.0, min(1.0, confidence))
    
    def _generate_similarity_cache_key(self, entity1, entity2) -> str:
        """ç”Ÿæˆç›¸ä¼¼åº¦ç¼“å­˜é”®"""
        # ç¡®ä¿ç¼“å­˜é”®çš„ä¸€è‡´æ€§ï¼ˆä¸è€ƒè™‘å®ä½“é¡ºåºï¼‰
        key1 = f"{entity1.name}_{entity1.type}"
        key2 = f"{entity2.name}_{entity2.type}"
        
        # æŒ‰å­—å…¸åºæ’åºï¼Œç¡®ä¿ç›¸åŒå®ä½“å¯¹çš„ç¼“å­˜é”®ä¸€è‡´
        if key1 <= key2:
            return f"{key1}___{key2}"
        else:
            return f"{key2}___{key1}"
    
    def _clean_similarity_cache(self):
        """æ¸…ç†ç›¸ä¼¼åº¦ç¼“å­˜"""
        # ç®€å•çš„LRUç­–ç•¥ï¼šä¿ç•™ä¸€åŠç¼“å­˜
        cache_items = list(self._similarity_cache.items())
        keep_count = len(cache_items) // 2
        
        # ä¿ç•™åä¸€åŠï¼ˆå‡è®¾æ›´æ–°ï¼‰
        new_cache = dict(cache_items[-keep_count:])
        self._similarity_cache = new_cache
        
        logger.debug(f"ç›¸ä¼¼åº¦ç¼“å­˜æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(self._similarity_cache)} é¡¹")
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_size": len(self._similarity_cache),
            "total_requests": total_requests,
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "hit_rate": hit_rate
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._similarity_cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0
        logger.info("ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜å·²æ¸…ç©º")


# ğŸ†• é›†æˆæ¥å£å’Œå·¥å‚æ–¹æ³•
_similarity_calculator_instance = None

def get_entity_similarity_calculator() -> EntitySimilarityCalculator:
    """
    è·å–å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        EntitySimilarityCalculator: ç›¸ä¼¼åº¦è®¡ç®—å™¨å®ä¾‹
    """
    global _similarity_calculator_instance
    if _similarity_calculator_instance is None:
        _similarity_calculator_instance = EntitySimilarityCalculator()
    return _similarity_calculator_instance


class EntitySimilarityMatrix:
    """
    å®ä½“ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå™¨
    
    ç”¨äºæ‰¹é‡è®¡ç®—å®ä½“é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œæ”¯æŒå¤§è§„æ¨¡å®ä½“å¤„ç†
    """
    
    def __init__(self, calculator: Optional[EntitySimilarityCalculator] = None):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå™¨"""
        self.calculator = calculator or get_entity_similarity_calculator()
        logger.info("å®ä½“ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå™¨å·²åˆå§‹åŒ–")
    
    async def build_similarity_matrix(self, entities: List[Any], 
                                    threshold: float = 0.0,
                                    max_matrix_size: Optional[int] = None) -> Dict[str, Any]:
        """
        æ„å»ºå®ä½“ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ä¸è®¡ç®—
            max_matrix_size: æœ€å¤§çŸ©é˜µå¤§å°é™åˆ¶
            
        Returns:
            åŒ…å«ç›¸ä¼¼åº¦çŸ©é˜µå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        import time
        from app.core.config import settings
        
        start_time = time.time()
        n_entities = len(entities)
        max_size = max_matrix_size or settings.ENTITY_UNIFICATION_MAX_MATRIX_SIZE
        
        logger.info(f"å¼€å§‹æ„å»º {n_entities}Ã—{n_entities} å®ä½“ç›¸ä¼¼åº¦çŸ©é˜µ")
        
        # æ£€æŸ¥çŸ©é˜µå¤§å°é™åˆ¶
        if n_entities * n_entities > max_size:
            logger.warning(f"çŸ©é˜µå¤§å° {n_entities}Ã—{n_entities} è¶…è¿‡é™åˆ¶ {max_size}ï¼Œå°†è¿›è¡Œåˆ†å—å¤„ç†")
            return await self._build_large_matrix(entities, threshold, max_size)
        
        # æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = {}
        comparison_count = 0
        valid_pairs = 0
        
        try:
            # æ‰¹é‡å¤„ç†ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
            batch_size = min(100, n_entities)
            
            for i in range(0, n_entities, batch_size):
                batch_i_end = min(i + batch_size, n_entities)
                
                for j in range(i, n_entities, batch_size):
                    batch_j_end = min(j + batch_size, n_entities)
                    
                    # å¤„ç†å½“å‰æ‰¹æ¬¡
                    await self._process_matrix_batch(
                        entities, similarity_matrix,
                        i, batch_i_end, j, batch_j_end,
                        threshold
                    )
                    
                    comparison_count += (batch_i_end - i) * (batch_j_end - j)
                    
                    # å†…å­˜å‹åŠ›æ§åˆ¶
                    if comparison_count % 10000 == 0:
                        logger.debug(f"å·²å¤„ç† {comparison_count} å¯¹å®ä½“æ¯”è¾ƒ")
            
            # ç»Ÿè®¡æœ‰æ•ˆç›¸ä¼¼åº¦å¯¹
            valid_pairs = sum(len(similarities) for similarities in similarity_matrix.values())
            
            build_duration = time.time() - start_time
            
            logger.info(f"ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå®Œæˆ: {comparison_count} æ¬¡æ¯”è¾ƒ, "
                       f"{valid_pairs} å¯¹æœ‰æ•ˆç›¸ä¼¼åº¦, è€—æ—¶: {build_duration:.3f}ç§’")
            
            return {
                "matrix": similarity_matrix,
                "metadata": {
                    "entity_count": n_entities,
                    "comparison_count": comparison_count,
                    "valid_pairs": valid_pairs,
                    "threshold": threshold,
                    "build_duration": build_duration,
                    "matrix_density": valid_pairs / (comparison_count / 2) if comparison_count > 0 else 0
                }
            }
            
        except Exception as e:
            logger.error(f"æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µå¤±è´¥: {str(e)}")
            raise
    
    async def _process_matrix_batch(self, entities: List[Any], similarity_matrix: Dict[str, Dict[str, float]],
                                  i_start: int, i_end: int, j_start: int, j_end: int,
                                  threshold: float):
        """å¤„ç†çŸ©é˜µæ‰¹æ¬¡"""
        tasks = []
        
        for i in range(i_start, i_end):
            entity_i = entities[i]
            entity_i_id = entity_i.id
            
            if entity_i_id not in similarity_matrix:
                similarity_matrix[entity_i_id] = {}
            
            for j in range(max(j_start, i), j_end):  # åªè®¡ç®—ä¸Šä¸‰è§’çŸ©é˜µ
                if i == j:
                    similarity_matrix[entity_i_id][entities[j].id] = 1.0
                    continue
                
                entity_j = entities[j]
                
                # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                task = self.calculator.calculate_similarity(entity_i, entity_j)
                tasks.append((i, j, task))
        
        # æ‰¹é‡æ‰§è¡Œç›¸ä¼¼åº¦è®¡ç®—
        if tasks:
            results = await asyncio.gather(*[task for _, _, task in tasks], return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for k, (i, j, _) in enumerate(tasks):
                try:
                    if isinstance(results[k], Exception):
                        logger.warning(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {entities[i].name} <-> {entities[j].name}")
                        continue
                    
                    similarity_result = results[k]
                    total_sim = similarity_result.total_similarity
                    
                    if total_sim >= threshold:
                        entity_i_id = entities[i].id
                        entity_j_id = entities[j].id
                        
                        # åŒå‘å­˜å‚¨
                        similarity_matrix[entity_i_id][entity_j_id] = total_sim
                        
                        if entity_j_id not in similarity_matrix:
                            similarity_matrix[entity_j_id] = {}
                        similarity_matrix[entity_j_id][entity_i_id] = total_sim
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†ç›¸ä¼¼åº¦ç»“æœå¤±è´¥: {str(e)}")
                    continue
    
    async def _build_large_matrix(self, entities: List[Any], threshold: float, max_size: int) -> Dict[str, Any]:
        """æ„å»ºå¤§å‹ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆåˆ†å—å¤„ç†ï¼‰"""
        import math
        
        n_entities = len(entities)
        chunk_size = int(math.sqrt(max_size / 4))  # ä¿å®ˆä¼°ç®—
        
        logger.info(f"ä½¿ç”¨åˆ†å—ç­–ç•¥æ„å»ºå¤§å‹çŸ©é˜µï¼Œåˆ†å—å¤§å°: {chunk_size}")
        
        similarity_matrix = {}
        total_comparisons = 0
        valid_pairs = 0
        
        # åˆ†å—å¤„ç†
        for i in range(0, n_entities, chunk_size):
            i_end = min(i + chunk_size, n_entities)
            chunk_entities = entities[i:i_end]
            
            # æ„å»ºå½“å‰å—çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            chunk_result = await self.build_similarity_matrix(
                chunk_entities, threshold, max_size
            )
            
            # åˆå¹¶ç»“æœ
            chunk_matrix = chunk_result["matrix"]
            for entity_id, similarities in chunk_matrix.items():
                if entity_id not in similarity_matrix:
                    similarity_matrix[entity_id] = {}
                similarity_matrix[entity_id].update(similarities)
            
            total_comparisons += chunk_result["metadata"]["comparison_count"]
            valid_pairs += chunk_result["metadata"]["valid_pairs"]
            
            logger.debug(f"å®Œæˆåˆ†å— {i//chunk_size + 1}/{(n_entities + chunk_size - 1)//chunk_size}")
        
        return {
            "matrix": similarity_matrix,
            "metadata": {
                "entity_count": n_entities,
                "comparison_count": total_comparisons,
                "valid_pairs": valid_pairs,
                "threshold": threshold,
                "chunked_processing": True,
                "chunk_size": chunk_size
            }
        }
    
    def get_top_similar_entities(self, similarity_matrix: Dict[str, Dict[str, float]], 
                               entity_id: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        è·å–ä¸æŒ‡å®šå®ä½“æœ€ç›¸ä¼¼çš„top-kå®ä½“
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            entity_id: ç›®æ ‡å®ä½“ID
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼å®ä½“æ•°é‡
            
        Returns:
            [(entity_id, similarity_score), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if entity_id not in similarity_matrix:
            return []
        
        similarities = similarity_matrix[entity_id]
        
        # æ’åºå¹¶è¿”å›top-k
        sorted_similarities = sorted(
            similarities.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return sorted_similarities[:top_k]


# ğŸ†• æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
async def test_entity_similarity_calculator() -> Dict[str, Any]:
    """
    æµ‹è¯•å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨çš„åŠŸèƒ½
    
    Returns:
        æµ‹è¯•ç»“æœ
    """
    from app.services.knowledge_extraction_service import Entity
    
    test_results = {
        "basic_similarity": False,
        "semantic_similarity": False,
        "lexical_similarity": False,
        "contextual_similarity": False,
        "matrix_construction": False,
        "error_messages": []
    }
    
    try:
        # åˆ›å»ºæµ‹è¯•å®ä½“
        entity1 = Entity(
            id="test_1",
            name="è‹¹æœå…¬å¸",
            type="ç»„ç»‡",
            description="ä¸€å®¶ç¾å›½è·¨å›½æŠ€æœ¯å…¬å¸",
            properties={},
            confidence=0.9,
            source_text="è‹¹æœå…¬å¸æ˜¯ä¸€å®¶ä½äºåŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºçš„ç¾å›½è·¨å›½æŠ€æœ¯å…¬å¸",
            start_pos=0,
            end_pos=10,
            aliases=["Apple Inc.", "Apple", "è‹¹æœ"]
        )
        
        entity2 = Entity(
            id="test_2", 
            name="Apple Inc.",
            type="ç»„ç»‡",
            description="American multinational technology company",
            properties={},
            confidence=0.85,
            source_text="Apple Inc. is an American multinational technology company headquartered in Cupertino, California",
            start_pos=0,
            end_pos=10,
            aliases=["Apple", "è‹¹æœå…¬å¸"]
        )
        
        entity3 = Entity(
            id="test_3",
            name="é¦™æ¸¯",
            type="åœ°ç‚¹", 
            description="ä¸­å›½ç‰¹åˆ«è¡Œæ”¿åŒº",
            properties={},
            confidence=0.8,
            source_text="é¦™æ¸¯æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„ä¸€ä¸ªç‰¹åˆ«è¡Œæ”¿åŒº",
            start_pos=0,
            end_pos=2,
            aliases=["Hong Kong", "HK"]
        )
        
        # åˆå§‹åŒ–è®¡ç®—å™¨
        calculator = get_entity_similarity_calculator()
        
        # 1. æµ‹è¯•åŸºæœ¬ç›¸ä¼¼åº¦è®¡ç®—
        result1 = await calculator.calculate_similarity(entity1, entity2)
        test_results["basic_similarity"] = result1.total_similarity > 0.7  # ç›¸åŒå…¬å¸åº”è¯¥é«˜ç›¸ä¼¼åº¦
        
        # 2. æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼åº¦
        test_results["semantic_similarity"] = result1.semantic_similarity >= 0.0
        
        # 3. æµ‹è¯•è¯æ±‡ç›¸ä¼¼åº¦  
        test_results["lexical_similarity"] = result1.lexical_similarity > 0.5  # åˆ«ååŒ¹é…
        
        # 4. æµ‹è¯•ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
        test_results["contextual_similarity"] = result1.contextual_similarity > 0.5  # ç›¸åŒç±»å‹
        
        # 5. æµ‹è¯•çŸ©é˜µæ„å»º
        matrix_builder = EntitySimilarityMatrix(calculator)
        matrix_result = await matrix_builder.build_similarity_matrix([entity1, entity2, entity3])
        test_results["matrix_construction"] = len(matrix_result["matrix"]) == 3
        
        logger.info("âœ… å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        test_results["error_messages"].append(str(e))
        logger.error(f"âŒ å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
    
    return test_results 