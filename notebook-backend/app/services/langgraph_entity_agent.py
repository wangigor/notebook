# -*- coding: utf-8 -*-
"""
LangGraphå®ä½“å»é‡Agentï¼ˆå·¥å…·è°ƒç”¨å¢å¼ºç‰ˆï¼‰
çœŸæ­£ä½¿ç”¨LangGraph StateGraphå®ç°çš„å®ä½“å»é‡ç³»ç»Ÿï¼Œæ”¯æŒLLMè‡ªä¸»å·¥å…·è°ƒç”¨
"""
import json
import logging
import asyncio
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from itertools import combinations

from langgraph.graph import StateGraph, END

from app.models.langgraph_state import (
    EntityDeduplicationState, create_initial_state, update_step, add_error, add_warning, calculate_processing_time
)
from app.services.llm_client_service import LLMClientService
from app.services.tool_execution_service import get_tool_execution_service
from app.services.embedding_service import get_embedding_service
from app.tools import get_entity_analysis_tools
from app.prompts.entity_deduplication_prompts import (
    build_tool_aware_analysis_prompt, parse_tool_aware_analysis_result, process_entity_pairs_from_tool_analysis
)

logger = logging.getLogger(__name__)


class LangGraphEntityDeduplicationAgent:
    """LangGraphå®ä½“å»é‡Agent"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–Agent"""
        self.config = config or self._get_default_config()
        
        # åˆå§‹åŒ–æœåŠ¡
        self.llm_service = LLMClientService()
        self.llm = self.llm_service.get_processing_llm(streaming=False)
        self.embedding_service = get_embedding_service()
        
        # è·å–å·¥å…·å¹¶åˆ›å»ºå·¥å…·æ‰§è¡ŒæœåŠ¡
        self.tools = get_entity_analysis_tools()
        self.tool_executor = get_tool_execution_service(self.tools)
        self.llm_with_tools = self.llm_service.get_llm_with_tools(self.tools, streaming=False)
        
        # åˆ›å»ºçŠ¶æ€å›¾
        self.graph = self._create_state_graph()
        
        logger.info("LangGraphå®ä½“å»é‡Agentåˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "prescreening_threshold": 0.4,  # å‘é‡é¢„ç­›é€‰é˜ˆå€¼
            "max_pairs_per_batch": 30,  # æ¯æ‰¹å¤„ç†çš„æœ€å¤§å®ä½“å¯¹æ•°ï¼ˆé™ä½ä»¥é€‚åº”å·¥å…·è°ƒç”¨ï¼‰
            "max_retries": 2,  # æœ€å¤§é‡è¯•æ¬¡æ•°
            "enable_vector_prescreening": True,  # å¯ç”¨å‘é‡é¢„ç­›é€‰
            "conservative_mode": True,  # ä¿å®ˆæ¨¡å¼
            "tool_calling_enabled": True,  # å¯ç”¨å·¥å…·è°ƒç”¨
            "max_tool_calls_per_analysis": 10,  # æ¯æ¬¡åˆ†ææœ€å¤§å·¥å…·è°ƒç”¨æ•°
            "similarity_weights": {  # ç›¸ä¼¼åº¦æƒé‡
                "vector": 0.3,
                "llm": 0.7  # å¢åŠ LLMæƒé‡ï¼Œå› ä¸ºåŒ…å«å·¥å…·è°ƒç”¨
            }
        }
    
    def _create_state_graph(self) -> StateGraph:
        """åˆ›å»ºLangGraphçŠ¶æ€å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(EntityDeduplicationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("vector_prescreening", self.vector_prescreening_node)
        workflow.add_node("intelligent_analysis", self.intelligent_analysis_node)
        workflow.add_node("final_decision", self.final_decision_node)
        workflow.add_node("error_handler", self.error_handler_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("vector_prescreening")
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "vector_prescreening",
            self.should_proceed_to_analysis,
            {
                "intelligent_analysis": "intelligent_analysis",
                "skip_to_decision": "final_decision",
                "error": "error_handler"
            }
        )
        
        workflow.add_conditional_edges(
            "intelligent_analysis", 
            self.should_proceed_to_decision,
            {
                "final_decision": "final_decision",
                "error": "error_handler"
            }
        )
        
        # æœ€ç»ˆèŠ‚ç‚¹è¿æ¥åˆ°END
        workflow.add_edge("final_decision", END)
        workflow.add_edge("error_handler", END)
        
        return workflow.compile()
    
    # === èŠ‚ç‚¹å®ç° ===
    
    async def vector_prescreening_node(self, state: EntityDeduplicationState) -> EntityDeduplicationState:
        """å‘é‡é¢„ç­›é€‰èŠ‚ç‚¹ - ç¬¬ä¸€é˜¶æ®µ"""
        logger.info("å¼€å§‹å‘é‡é¢„ç­›é€‰é˜¶æ®µ")
        state = update_step(state, "vector_prescreening")
        
        try:
            if not self.config["enable_vector_prescreening"]:
                # è·³è¿‡å‘é‡é¢„ç­›é€‰ï¼Œç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å®ä½“å¯¹
                state["prescreened_pairs"] = self._generate_all_pairs(state["entities"])
                state["prescreening_stats"] = {
                    "total_possible_pairs": len(state["prescreened_pairs"]),
                    "filtered_pairs": len(state["prescreened_pairs"]),
                    "prescreening_enabled": False
                }
                return state
            
            # ä¸ºå®ä½“ç”Ÿæˆembedding
            entities_with_embeddings = await self._ensure_embeddings(state["entities"])
            
            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = await self._compute_similarity_matrix(entities_with_embeddings)
            
            # åŸºäºé˜ˆå€¼ç­›é€‰å®ä½“å¯¹
            prescreened_pairs = self._filter_pairs_by_similarity(
                entities_with_embeddings, 
                similarity_matrix, 
                state["prescreening_threshold"]
            )
            
            state["prescreened_pairs"] = prescreened_pairs
            state["prescreening_stats"] = {
                "total_possible_pairs": len(list(combinations(range(len(entities_with_embeddings)), 2))),
                "filtered_pairs": len(prescreened_pairs),
                "filtering_rate": 1 - (len(prescreened_pairs) / max(1, len(list(combinations(range(len(entities_with_embeddings)), 2))))),
                "threshold_used": state["prescreening_threshold"],
                "prescreening_enabled": True
            }
            
            logger.info(f"å‘é‡é¢„ç­›é€‰å®Œæˆ: {state['prescreening_stats']['total_possible_pairs']} -> {len(prescreened_pairs)} å¯¹")
            return state
            
        except Exception as e:
            error_msg = f"å‘é‡é¢„ç­›é€‰å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return add_error(state, error_msg)
    
    async def intelligent_analysis_node(self, state: EntityDeduplicationState) -> EntityDeduplicationState:
        """æ™ºèƒ½åˆ†æèŠ‚ç‚¹ - è®©LLMè‡ªä¸»å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·"""
        logger.info("å¼€å§‹æ™ºèƒ½åˆ†æé˜¶æ®µï¼ˆå«å·¥å…·è°ƒç”¨ï¼‰")
        state = update_step(state, "intelligent_analysis")
        
        try:
            # æ„å»ºåˆå§‹åˆ†ææ¶ˆæ¯
            initial_message = build_tool_aware_analysis_prompt(
                state["prescreened_pairs"], 
                state["entity_type"]
            )
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šå‘å¾€Agentçš„Prompt
            logger.info("=" * 80)
            logger.info(f"ğŸ” å‘å¾€LangGraph Agentçš„Prompt - {state['entity_type']} ç±»å‹")
            logger.info("=" * 80)
            logger.info(f"å®ä½“å¯¹æ•°é‡: {len(state['prescreened_pairs'])}")
            logger.info(f"å®ä½“ç±»å‹: {state['entity_type']}")
            logger.info(f"Prompté•¿åº¦: {len(initial_message)} å­—ç¬¦")
            logger.info("ğŸ“ å®Œæ•´Promptå†…å®¹:")
            logger.info("-" * 40)
            logger.info(initial_message)
            logger.info("-" * 40)
            logger.info("=" * 80)
            
            # åˆå§‹åŒ–å¯¹è¯æ¶ˆæ¯
            messages = [{"role": "system", "content": initial_message}]
            state["analysis_messages"] = messages
            state["reasoning_steps"].append("å¼€å§‹æ™ºèƒ½å®ä½“å»é‡åˆ†æ")
            
            # è¿›è¡Œå¤šè½®å¯¹è¯åˆ†æï¼Œå…è®¸LLMè‡ªä¸»è°ƒç”¨å·¥å…·
            max_iterations = 5  # æœ€å¤§å¯¹è¯è½®æ•°
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                logger.info(f"æ™ºèƒ½åˆ†æè¿­ä»£ {iteration}/{max_iterations}")
                
                # è°ƒç”¨å¸¦å·¥å…·çš„LLM
                response = await self.llm_with_tools.ainvoke(messages)
                
                # å¤„ç†å“åº”
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    # LLMå†³å®šè°ƒç”¨å·¥å…·
                    logger.info(f"LLMè¯·æ±‚è°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")
                    
                    # è®°å½•æ™ºèƒ½æœç´¢å†³ç­–
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get('name', 'unknown')
                        tool_args = tool_call.get('args', {})
                        entity_name = tool_args.get('entity_name', 'unknown')
                        logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å†³ç­–: LLMé€‰æ‹©æœç´¢ '{entity_name}' (å·¥å…·: {tool_name})")
                    
                    # è®°å½•å·¥å…·è°ƒç”¨
                    state["tool_calls_made"].extend([
                        {
                            "iteration": iteration,
                            "tool_call": tool_call,
                            "timestamp": datetime.now().isoformat(),
                            "decision_type": "llm_initiated"  # æ ‡è®°ä¸ºLLMè‡ªä¸»å†³ç­–
                        }
                        for tool_call in response.tool_calls
                    ])
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = await self.tool_executor.execute_tool_calls(response.tool_calls)
                    
                    # è®°å½•å·¥å…·ç»“æœ
                    state["tool_results"].extend([
                        {
                            "iteration": iteration,
                            "tool_message": tool_message,
                            "timestamp": datetime.now().isoformat()
                        }
                        for tool_message in tool_messages
                    ])
                    
                    # å°†å“åº”å’Œå·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                    messages.append({"role": "assistant", "content": response.content, "tool_calls": response.tool_calls})
                    messages.extend([{"role": "tool", "content": tm.content, "tool_call_id": tm.tool_call_id} for tm in tool_messages])
                    
                    # è®°å½•æ¨ç†æ­¥éª¤
                    state["reasoning_steps"].append(f"è¿­ä»£{iteration}: è°ƒç”¨äº†{len(response.tool_calls)}ä¸ªå·¥å…·ï¼Œè·å¾—ç»“æœ")
                    
                else:
                    # LLMå®Œæˆåˆ†æï¼Œæ²¡æœ‰æ›´å¤šå·¥å…·è°ƒç”¨
                    logger.info("ğŸ§  æ™ºèƒ½æœç´¢å†³ç­–: LLMåŸºäºå†…åœ¨çŸ¥è¯†å®Œæˆåˆ†æï¼Œæ— éœ€å¤–éƒ¨æœç´¢")
                    
                    # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šAgentè¿”å›ç»“æœ
                    logger.info("=" * 80)
                    logger.info(f"ğŸ” LangGraph Agentè¿”å›ç»“æœ - {state['entity_type']} ç±»å‹")
                    logger.info("=" * 80)
                    logger.info(f"è¿­ä»£æ¬¡æ•°: {iteration}")
                    logger.info(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
                    logger.info("ğŸ“ å®Œæ•´Agentå“åº”:")
                    logger.info("-" * 40)
                    logger.info(response.content)
                    logger.info("-" * 40)
                    
                    # è§£ææœ€ç»ˆåˆ†æç»“æœ
                    analysis_result = parse_tool_aware_analysis_result(response.content)
                    
                    # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šè§£æåçš„åˆ†æç»“æœ
                    logger.info("ğŸ“Š è§£æåçš„åˆ†æç»“æœ:")
                    logger.info(f"  - åˆå¹¶ç»„æ•°é‡: {len(analysis_result.get('merge_groups', []))}")
                    logger.info(f"  - ç‹¬ç«‹å®ä½“æ•°é‡: {len(analysis_result.get('independent_entities', []))}")
                    logger.info(f"  - ä¸ç¡®å®šæ¡ˆä¾‹æ•°é‡: {len(analysis_result.get('uncertain_cases', []))}")
                    
                    # æ˜¾ç¤ºåˆå¹¶ç»„è¯¦æƒ…
                    if analysis_result.get('merge_groups'):
                        logger.info("ğŸ”— åˆå¹¶ç»„è¯¦æƒ…:")
                        for i, group in enumerate(analysis_result['merge_groups']):
                            logger.info(f"  åˆå¹¶ç»„ {i+1}:")
                            logger.info(f"    - ä¸»å®ä½“: {group.get('primary_entity', 'N/A')}")
                            logger.info(f"    - é‡å¤å®ä½“: {group.get('duplicates', [])}")
                            logger.info(f"    - åˆå¹¶åç§°: {group.get('merged_name', 'N/A')}")
                            logger.info(f"    - åˆå¹¶æè¿°: {group.get('merged_description', 'N/A')[:100]}...")
                            logger.info(f"    - ç½®ä¿¡åº¦: {group.get('confidence', 'N/A')}")
                            logger.info(f"    - ç†ç”±: {group.get('reason', 'N/A')[:100]}...")
                            logger.info(f"    - Wikipediaè¯æ®: {group.get('wikipedia_evidence', 'N/A')[:100]}...")
                    
                    # æ˜¾ç¤ºç‹¬ç«‹å®ä½“
                    if analysis_result.get('independent_entities'):
                        logger.info(f"ğŸ”¸ ç‹¬ç«‹å®ä½“: {analysis_result['independent_entities']}")
                    
                    # æ˜¾ç¤ºä¸ç¡®å®šæ¡ˆä¾‹
                    if analysis_result.get('uncertain_cases'):
                        logger.info(f"â“ ä¸ç¡®å®šæ¡ˆä¾‹: {analysis_result['uncertain_cases']}")
                    
                    logger.info("=" * 80)
                    
                    state["analysis_result"] = analysis_result
                    
                    # å¤„ç†å®ä½“å¯¹ç»“æœ
                    entity_pairs = process_entity_pairs_from_tool_analysis(analysis_result)
                    state["entity_pairs"] = entity_pairs
                    state["pairs_analyzed"] = len(entity_pairs)
                    
                    # è®°å½•æœ€ç»ˆæ¨ç†æ­¥éª¤
                    state["reasoning_steps"].append(f"è¿­ä»£{iteration}: åŸºäºå†…åœ¨çŸ¥è¯†å®Œæˆåˆ†æï¼Œè¯†åˆ«{len(entity_pairs)}ä¸ªå®ä½“å¯¹")
                    
                    break
            
            # æ›´æ–°å¯¹è¯å†å²
            state["analysis_messages"] = messages
            
            logger.info(f"æ™ºèƒ½åˆ†æå®Œæˆ: {state['pairs_analyzed']} ä¸ªå®ä½“å¯¹ï¼Œ{len(state['tool_calls_made'])} æ¬¡å·¥å…·è°ƒç”¨")
            
            # ç»Ÿè®¡æœç´¢å†³ç­–
            tool_calls_count = len(state.get('tool_calls_made', []))
            total_entities = len(state.get('entities', []))
            search_rate = (tool_calls_count / max(1, total_entities)) * 100
            logger.info(f"ğŸ“Š æœç´¢å†³ç­–ç»Ÿè®¡: {tool_calls_count}/{total_entities} å®ä½“éœ€è¦å¤–éƒ¨æœç´¢ ({search_rate:.1f}%)")
            
            return state
            
        except Exception as e:
            error_msg = f"æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return add_error(state, error_msg)
    
    async def final_decision_node(self, state: EntityDeduplicationState) -> EntityDeduplicationState:
        """æœ€ç»ˆå†³ç­–èŠ‚ç‚¹ - åŸºäºæ™ºèƒ½åˆ†æç»“æœåšæœ€ç»ˆå†³ç­–"""
        logger.info("å¼€å§‹æœ€ç»ˆå†³ç­–é˜¶æ®µ")
        state = update_step(state, "final_decision")
        
        try:
            # åŸºäºæ™ºèƒ½åˆ†æç»“æœè¿›è¡Œæœ€ç»ˆå†³ç­–
            if state.get("analysis_result"):
                # ç›´æ¥ä½¿ç”¨æ™ºèƒ½åˆ†æçš„ç»“æœ
                analysis_result = state["analysis_result"]
                
                # æå–åˆå¹¶ç»„å’Œç‹¬ç«‹å®ä½“
                merge_groups = analysis_result.get("merge_groups", [])
                independent_entities = analysis_result.get("independent_entities", [])
                uncertain_cases = analysis_result.get("uncertain_cases", [])
                
                # è¶…ä¿å®ˆéªŒè¯ï¼šè¿›ä¸€æ­¥éªŒè¯åˆå¹¶å†³ç­–
                validated_merge_groups = self._validate_merge_decisions_ultra_conservative(
                    merge_groups, state
                )
                
                state["merge_groups"] = validated_merge_groups
                state["independent_entities"] = independent_entities
                state["uncertain_cases"] = uncertain_cases
                
                # è®¾ç½®æœ€ç»ˆå†³ç­–ç»“æœ
                state["final_decision_result"] = {
                    "decision_summary": f"æ™ºèƒ½åˆ†æå®Œæˆ: {len(validated_merge_groups)} ä¸ªåˆå¹¶ç»„, {len(independent_entities)} ä¸ªç‹¬ç«‹å®ä½“",
                    "merge_groups": validated_merge_groups,
                    "independent_entities": independent_entities,
                    "uncertain_cases": uncertain_cases,
                    "tool_calls_used": len(state.get("tool_calls_made", [])),
                    "reasoning_steps": state.get("reasoning_steps", [])
                }
            else:
                # æ²¡æœ‰åˆ†æç»“æœï¼Œä¿å®ˆå¤„ç†
                logger.warning("æ— æ™ºèƒ½åˆ†æç»“æœï¼Œä¿å®ˆå¤„ç†æ‰€æœ‰å®ä½“ä¸ºç‹¬ç«‹")
                state["merge_groups"] = []
                state["independent_entities"] = list(range(len(state["entities"]))) 
                state["uncertain_cases"] = []
                state["final_decision_result"] = {
                    "decision_summary": "æ— æœ‰æ•ˆåˆ†æç»“æœï¼Œä¿å®ˆå¤„ç†",
                    "merge_groups": [],
                    "independent_entities": state["independent_entities"],
                    "uncertain_cases": []
                }
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            state = calculate_processing_time(state)
            state = update_step(state, "completed")
            
            logger.info(f"æœ€ç»ˆå†³ç­–å®Œæˆ: {len(state['merge_groups'])} ä¸ªåˆå¹¶ç»„, {len(state['independent_entities'])} ä¸ªç‹¬ç«‹å®ä½“")
            return state
            
        except Exception as e:
            error_msg = f"æœ€ç»ˆå†³ç­–å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return add_error(state, error_msg)
    
    async def error_handler_node(self, state: EntityDeduplicationState) -> EntityDeduplicationState:
        """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
        logger.error("è¿›å…¥é”™è¯¯å¤„ç†é˜¶æ®µ")
        state = update_step(state, "error")
        
        # è®°å½•é”™è¯¯ç»Ÿè®¡
        state["final_decision_result"] = {
            "decision_summary": "å¤„ç†å¤±è´¥ï¼Œè¿›å…¥é”™è¯¯æ¢å¤æ¨¡å¼",
            "merge_groups": [],
            "independent_entities": list(range(len(state["entities"]))),
            "uncertain_cases": [],
            "error_recovery": True
        }
        
        state["merge_groups"] = []
        state["independent_entities"] = list(range(len(state["entities"])))
        state["uncertain_cases"] = []
        
        state = calculate_processing_time(state)
        
        logger.warning(f"é”™è¯¯æ¢å¤: å°†æ‰€æœ‰ {len(state['entities'])} ä¸ªå®ä½“æ ‡è®°ä¸ºç‹¬ç«‹")
        return state
    
    # === æ¡ä»¶è·¯ç”±å‡½æ•° ===
    
    def should_proceed_to_analysis(self, state: EntityDeduplicationState) -> str:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥æ™ºèƒ½åˆ†æ"""
        if state["current_step"] == "error":
            return "error"
        
        if not state.get("prescreened_pairs"):
            logger.info("æ— é¢„ç­›é€‰å®ä½“å¯¹ï¼Œè·³è¿‡æ™ºèƒ½åˆ†æ")
            return "skip_to_decision"
        
        if len(state["prescreened_pairs"]) > self.config["max_pairs_per_batch"]:
            logger.warning(f"å®ä½“å¯¹è¿‡å¤š ({len(state['prescreened_pairs'])}), å¯èƒ½å½±å“æ€§èƒ½")
        
        return "intelligent_analysis"
    
    def should_proceed_to_decision(self, state: EntityDeduplicationState) -> str:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥æœ€ç»ˆå†³ç­–"""
        if state["current_step"] == "error":
            return "error"
        
        # æ™ºèƒ½åˆ†æå®Œæˆï¼Œç›´æ¥è¿›å…¥æœ€ç»ˆå†³ç­–
        return "final_decision"
    
    # === è¾…åŠ©æ–¹æ³• ===
    
    def _generate_all_pairs(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å®ä½“å¯¹"""
        pairs = []
        for i, j in combinations(range(len(entities)), 2):
            pairs.append({
                "entity1_index": i,
                "entity2_index": j,
                "entity1_name": entities[i]["name"],
                "entity2_name": entities[j]["name"],
                "vector_similarity": 1.0,  # é»˜è®¤é«˜ç›¸ä¼¼åº¦ï¼Œå¼ºåˆ¶LLMåˆ†æ
                "from_prescreening": False
            })
        return pairs
    
    async def _ensure_embeddings(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç¡®ä¿å®ä½“æœ‰embeddingå‘é‡"""
        entities_need_embedding = []
        texts_to_embed = []
        
        for entity in entities:
            if not hasattr(entity, 'embedding') or entity.get('embedding') is None:
                entities_need_embedding.append(entity)
                text_repr = self._get_entity_text_representation(entity)
                texts_to_embed.append(text_repr)
        
        if texts_to_embed:
            embeddings = await self.embedding_service.embed_documents_batch(texts_to_embed)
            for i, entity in enumerate(entities_need_embedding):
                if i < len(embeddings):
                    entity['embedding'] = embeddings[i]
        
        return entities
    
    async def _compute_similarity_matrix(self, entities: List[Dict[str, Any]]) -> np.ndarray:
        """è®¡ç®—å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µ"""
        embeddings = []
        for entity in entities:
            embedding = entity.get('embedding', [])
            if embedding:
                embeddings.append(np.array(embedding))
            else:
                # åˆ›å»ºé›¶å‘é‡ä½œä¸ºé»˜è®¤
                embeddings.append(np.zeros(384))  # å‡è®¾384ç»´embedding
        
        if not embeddings:
            return np.zeros((len(entities), len(entities)))
        
        embeddings_matrix = np.array(embeddings)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        norms = np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)
        norms[norms == 0] = 1  # é¿å…é™¤é›¶
        normalized_embeddings = embeddings_matrix / norms
        
        similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)
        return similarity_matrix
    
    def _filter_pairs_by_similarity(self, entities: List[Dict[str, Any]], 
                                   similarity_matrix: np.ndarray, 
                                   threshold: float) -> List[Dict[str, Any]]:
        """åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼ç­›é€‰å®ä½“å¯¹"""
        pairs = []
        n = len(entities)
        
        for i in range(n):
            for j in range(i + 1, n):
                similarity = similarity_matrix[i, j]
                if similarity >= threshold:
                    pairs.append({
                        "entity1_index": i,
                        "entity2_index": j,
                        "entity1_name": entities[i]["name"],
                        "entity2_name": entities[j]["name"], 
                        "vector_similarity": float(similarity),
                        "from_prescreening": True
                    })
        
        return pairs
    
    def _create_list_mode_initial_state(self, entities: List[Dict[str, Any]], entity_type: str) -> EntityDeduplicationState:
        """åˆ›å»ºåˆ—è¡¨æ¨¡å¼çš„åˆå§‹çŠ¶æ€"""
        from app.models.langgraph_state import create_initial_state
        
        # ä½¿ç”¨åŸæœ‰çš„çŠ¶æ€åˆ›å»ºï¼Œä½†è·³è¿‡å‘é‡é¢„ç­›é€‰
        initial_state = create_initial_state(entities, entity_type, self.config)
        
        # ç›´æ¥è®¾ç½®ä¸ºè·³è¿‡å‘é‡é¢„ç­›é€‰
        initial_state["skip_vector_prescreening"] = True
        initial_state["entities_ready_for_analysis"] = entities
        
        return initial_state
    
    async def _execute_list_mode_graph(self, initial_state: EntityDeduplicationState) -> EntityDeduplicationState:
        """æ‰§è¡Œç®€åŒ–çš„çŠ¶æ€å›¾æµç¨‹"""
        state = initial_state
        
        try:
            # è·³è¿‡å‘é‡é¢„ç­›é€‰ï¼Œç›´æ¥è¿›è¡Œæ™ºèƒ½åˆ†æ
            state = await self.list_mode_analysis_node(state)
            
            # æœ€ç»ˆå†³ç­–
            if state["current_step"] != "error":
                state = await self.final_decision_node(state)
            else:
                state = await self.error_handler_node(state)
            
            return state
            
        except Exception as e:
            error_msg = f"åˆ—è¡¨æ¨¡å¼æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["current_step"] = "error"
            return await self.error_handler_node(state)
    
    async def list_mode_analysis_node(self, state: EntityDeduplicationState) -> EntityDeduplicationState:
        """åˆ—è¡¨æ¨¡å¼çš„æ™ºèƒ½åˆ†æèŠ‚ç‚¹"""
        logger.info("å¼€å§‹åˆ—è¡¨æ¨¡å¼æ™ºèƒ½åˆ†æ")
        state = update_step(state, "list_mode_analysis")
        
        try:
            # æ„å»ºåˆ—è¡¨æ¨¡å¼çš„åˆ†ææ¶ˆæ¯
            analysis_message = self._build_list_mode_analysis_prompt(
                state["entities_ready_for_analysis"], 
                state["entity_type"]
            )
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šå‘å¾€Agentçš„Prompt
            logger.info("=" * 80)
            logger.info(f"ğŸ” å‘å¾€LangGraph Agentçš„Prompt [åˆ—è¡¨æ¨¡å¼] - {state['entity_type']} ç±»å‹")
            logger.info("=" * 80)
            logger.info(f"å®ä½“æ•°é‡: {len(state['entities_ready_for_analysis'])}")
            logger.info(f"å®ä½“ç±»å‹: {state['entity_type']}")
            logger.info(f"Prompté•¿åº¦: {len(analysis_message)} å­—ç¬¦")
            logger.info("ğŸ“ å®Œæ•´Promptå†…å®¹:")
            logger.info("-" * 40)
            logger.info(analysis_message)
            logger.info("-" * 40)
            logger.info("=" * 80)
            
            # åˆå§‹åŒ–å¯¹è¯æ¶ˆæ¯
            messages = [{"role": "system", "content": analysis_message}]
            state["analysis_messages"] = messages
            state["reasoning_steps"].append("å¼€å§‹åˆ—è¡¨æ¨¡å¼å®ä½“å»é‡åˆ†æ")
            
            # è¿›è¡Œå¤šè½®å¯¹è¯åˆ†æ
            max_iterations = 3  # åˆ—è¡¨æ¨¡å¼å‡å°‘è¿­ä»£æ¬¡æ•°
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                logger.info(f"åˆ—è¡¨æ¨¡å¼åˆ†æè¿­ä»£ {iteration}/{max_iterations}")
                
                # è°ƒç”¨å¸¦å·¥å…·çš„LLM
                response = await self.llm_with_tools.ainvoke(messages)
                
                # å¤„ç†å“åº”
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    # LLMå†³å®šè°ƒç”¨å·¥å…·
                    logger.info(f"LLMè¯·æ±‚è°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = await self.tool_executor.execute_tool_calls(response.tool_calls)
                    
                    # æ›´æ–°å¯¹è¯å†å²
                    messages.append({"role": "assistant", "content": response.content, "tool_calls": response.tool_calls})
                    messages.extend([{"role": "tool", "content": tm.content, "tool_call_id": tm.tool_call_id} for tm in tool_messages])
                    
                    # è®°å½•å·¥å…·è°ƒç”¨
                    state["tool_calls_made"].extend([{
                        "iteration": iteration,
                        "tool_call": tool_call,
                        "timestamp": datetime.now().isoformat()
                    } for tool_call in response.tool_calls])
                    
                    state["reasoning_steps"].append(f"è¿­ä»£{iteration}: è°ƒç”¨äº†{len(response.tool_calls)}ä¸ªå·¥å…·")
                    
                else:
                    # LLMå®Œæˆåˆ†æ
                    logger.info("ğŸ§  æ™ºèƒ½åˆ†æå®Œæˆï¼šåŸºäºå®ä½“åˆ—è¡¨çš„è¯­ä¹‰åˆ†æ")
                    
                    # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šAgentè¿”å›ç»“æœ
                    logger.info("=" * 80)
                    logger.info(f"ğŸ” LangGraph Agentè¿”å›ç»“æœ [åˆ—è¡¨æ¨¡å¼] - {state['entity_type']} ç±»å‹")
                    logger.info("=" * 80)
                    logger.info(f"è¿­ä»£æ¬¡æ•°: {iteration}")
                    logger.info(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
                    logger.info("ğŸ“ å®Œæ•´Agentå“åº”:")
                    logger.info("-" * 40)
                    logger.info(response.content)
                    logger.info("-" * 40)
                    
                    # è§£æåˆ†æç»“æœ
                    analysis_result = self._parse_list_mode_analysis_result(response.content, state["entities_ready_for_analysis"])
                    
                    # è¯¦ç»†æ—¥å¿—ï¼šè§£æç»“æœ
                    logger.info("ğŸ“Š è§£æåçš„åˆ†æç»“æœ:")
                    logger.info(f"  - åˆå¹¶ç»„æ•°é‡: {len(analysis_result.get('merge_groups', []))}")
                    logger.info(f"  - ç‹¬ç«‹å®ä½“æ•°é‡: {len(analysis_result.get('independent_entities', []))}")
                    
                    logger.info("=" * 80)
                    
                    state["analysis_result"] = analysis_result
                    break
            
            # æ›´æ–°çŠ¶æ€
            state["analysis_messages"] = messages
            logger.info(f"åˆ—è¡¨æ¨¡å¼åˆ†æå®Œæˆ: {len(state.get('tool_calls_made', []))} æ¬¡å·¥å…·è°ƒç”¨")
            
            return state
            
        except Exception as e:
            error_msg = f"åˆ—è¡¨æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return add_error(state, error_msg)
    
    def _build_list_mode_analysis_prompt(self, entities: List[Dict[str, Any]], entity_type: str) -> str:
        """æ„å»ºåˆ—è¡¨æ¨¡å¼çš„åˆ†æprompt"""
        
        type_mapping = {
            "ç»„ç»‡": "Organization", "äººç‰©": "Person", "åœ°ç‚¹": "Location", 
            "äº§å“": "Product", "æŠ€æœ¯": "Technology", "æ—¶é—´": "Time", "äº‹ä»¶": "Event"
        }
        english_type = type_mapping.get(entity_type, entity_type)
        
        # æ„å»ºå®ä½“åˆ—è¡¨å­—ç¬¦ä¸²
        entities_text = ""
        for i, entity in enumerate(entities, 1):
            name = entity.get('name', 'Unknown')
            description = entity.get('description', '')
            aliases = entity.get('aliases', [])
            entity_id = entity.get('id', f'entity_{i}')
            
            entities_text += f"{i}. **{name}** (ID: {entity_id})\n"
            if description:
                entities_text += f"   - æè¿°: {description}\n"
            if aliases:
                entities_text += f"   - åˆ«å: {', '.join(aliases)}\n"
            entities_text += "\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“å·¥ç¨‹å¸ˆï¼Œæ“…é•¿å®ä½“æ¶ˆæ­§ä¸çŸ¥è¯†èåˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹ç”¨æˆ·ä¸Šä¼ çš„å¼‚æ„æ•°æ®è¿›è¡Œæ™ºèƒ½åˆå¹¶ï¼Œè¯†åˆ«è¡¨è¿°ä¸åŒä½†æŒ‡å‘ç›¸åŒå®ä½“çš„èŠ‚ç‚¹ã€‚\n\n**å®ä½“ç±»å‹**: {english_type} ({entity_type})\n**å¤„ç†å®ä½“æ•°é‡**: {len(entities)}\n\n**å®ä½“åˆ—è¡¨**:\n{entities_text}\n\n**å·¥ä½œæµç¨‹**:\n\n1. **å®ä½“æå–**: è¯†åˆ«æ¯ä¸ªå®ä½“çš„æ ¸å¿ƒç‰¹å¾\n2. **ç‰¹å¾åˆ†æ**: å¯¹æ¯ä¸ªå®ä½“æå–ä»¥ä¸‹ç‰¹å¾ï¼š\n   - æ ¸å¿ƒåç§°ï¼ˆæ ‡å‡†åŒ–å½¢å¼ï¼‰\n   - åˆ«å/ç¼©å†™/å˜ä½“\n   - ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆç›¸å…³å±æ€§ã€å…³ç³»ï¼‰\n   - é¢†åŸŸåˆ†ç±»\n\n3. **æ¶ˆæ­§å¤„ç†**: å¯¹å­˜åœ¨æ­§ä¹‰çš„å®ä½“ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤å¤„ç†ï¼š\n   a) æ£€æŸ¥ä¸Šä¸‹æ–‡è¯­ä¹‰ç‰¹å¾\n   b) ä½¿ç”¨Wikipediaå·¥å…·æŸ¥è¯¢å€™é€‰è§£é‡Š\n   c) æ„å»ºæ¶ˆæ­§å†³ç­–æ ‘ï¼š\n      - å¦‚æœWikipediaå­˜åœ¨æ˜ç¡®åŒºåˆ†é¡µâ†’é‡‡ç”¨æ ‡å‡†åç§°\n      - å¦‚æœå­˜åœ¨æ¶ˆæ­§é¡µâ†’é€‰æ‹©ä¸ä¸Šä¸‹æ–‡æœ€åŒ¹é…çš„é€‰é¡¹\n      - æ— åŒ¹é…â†’æ ‡è®°ä¸ºå¾…éªŒè¯å®ä½“\n\n4. **åˆå¹¶è§„åˆ™**:\n   - **å¼ºåˆ¶æ€§åˆå¹¶**: å®Œå…¨ç›¸åŒçš„è§„èŒƒåŒ–åç§°\n   - **é«˜ç½®ä¿¡åº¦åˆå¹¶**: \n     * åˆ«åå­—å…¸åŒ¹é…ï¼ˆå¦‚\"MIT\"â†’\"éº»çœç†å·¥å­¦é™¢\"ï¼‰\n     * Wikipediaé‡å®šå‘åŒ¹é…\n     * ç›¸åŒå±æ€§å€¼\n   - **éœ€äººå·¥å®¡æ ¸çš„åˆå¹¶**: \n     * éƒ¨åˆ†å±æ€§å†²çª\n     * è·¨è¯­è¨€å®ä½“ï¼ˆå¦‚\"Beijing\" vs \"åŒ—äº¬\"ï¼‰\n\n**ç‰¹åˆ«æ³¨æ„**:\n- å¯¹äºäººç‰©å®ä½“ï¼Œä¸åŒçš„äººå³ä½¿èŒä½ç›¸ä¼¼ä¹Ÿç»ä¸èƒ½åˆå¹¶\n- å¯¹äºç»„ç»‡å®ä½“ï¼Œä¸åŒçš„å…¬å¸/æœºæ„å³ä½¿è¡Œä¸šç›¸åŒä¹Ÿç»ä¸èƒ½åˆå¹¶\n- åªæœ‰åœ¨æœ‰æ˜ç¡®è¯æ®è¯æ˜æ˜¯åŒä¸€å®ä½“çš„ä¸åŒè¡¨è¿°æ—¶æ‰èƒ½åˆå¹¶\n\n**è¾“å‡ºæ ¼å¼**:\nè¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š\n\n```json\n{{\n  \"analysis_summary\": \"åˆ†ææ€»ç»“\",\n  \"merge_groups\": [\n    {{\n      \"primary_entity\": \"1\",\n      \"primary_entity_id\": \"entity_13a7bde4\",\n      \"duplicates\": [\"3\", \"5\"],\n      \"duplicate_entity_ids\": [\"entity_01d6297c\", \"entity_4f7e298d\"],\n      \"merged_name\": \"æ ‡å‡†åŒ–åç§°\",\n      \"merged_description\": \"ç»Ÿä¸€æè¿°\",\n      \"confidence\": 0.95,\n      \"reason\": \"åˆå¹¶ç†ç”±ï¼ˆå¿…é¡»åŒ…å«å…·ä½“è¯æ®ï¼‰\",\n      \"wikipedia_evidence\": \"WikipediaæŸ¥è¯¢ç»“æœä½œä¸ºè¯æ®\"\n    }}\n  ],\n  \"independent_entities\": [\"2\", \"4\", \"6\", \"7\"],\n  \"uncertain_cases\": [\n    {{\n      \"entities\": [\"8\", \"9\"], \n      \"reason\": \"éœ€è¦äººå·¥å®¡æ ¸çš„åŸå› \"\n    }}\n  ]\n}}\n```\n\n**å¼€å§‹åˆ†æ**: è¯·å¯¹ä¸Šè¿° {len(entities)} ä¸ª{entity_type}å®ä½“è¿›è¡Œæ™ºèƒ½æ¶ˆæ­§ä¸çŸ¥è¯†èåˆåˆ†æã€‚"""
        
        return prompt
    
    def _parse_list_mode_analysis_result(self, response_content: str, entities: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """è§£æåˆ—è¡¨æ¨¡å¼çš„åˆ†æç»“æœ"""
        try:
            # å°è¯•æå–JSONå†…å®¹
            import re
            import json
            
            # æŸ¥æ‰¾JSONä»£ç å—
            json_match = re.search(r'```json\s*(.*?)\s*```', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    logger.warning("æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°JSONæ ¼å¼çš„ç»“æœ")
                    return self._create_default_analysis_result(response_content)
            
            try:
                parsed_result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if not isinstance(parsed_result, dict):
                    raise ValueError("è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
                
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                required_fields = ["merge_groups", "independent_entities"]
                for field in required_fields:
                    if field not in parsed_result:
                        parsed_result[field] = []
                
                # ç¡®ä¿optionalå­—æ®µå­˜åœ¨
                if "uncertain_cases" not in parsed_result:
                    parsed_result["uncertain_cases"] = []
                if "analysis_summary" not in parsed_result:
                    parsed_result["analysis_summary"] = "å®ä½“æ¶ˆæ­§ä¸çŸ¥è¯†èåˆåˆ†æå®Œæˆ"
                
                # ğŸ”§ æ–°å¢ï¼šIDæå–é€»è¾‘
                if entities is not None:
                    parsed_result = self._enhance_result_with_entity_ids(parsed_result, entities)
                
                return parsed_result
                
            except json.JSONDecodeError as e:
                logger.warning(f"JSONè§£æå¤±è´¥: {str(e)}")
                return self._create_default_analysis_result(response_content)
                
        except Exception as e:
            logger.error(f"åˆ†æç»“æœè§£æå¤±è´¥: {str(e)}")
            return self._create_default_analysis_result(response_content)
    
    def _enhance_result_with_entity_ids(self, parsed_result: Dict[str, Any], entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¢å¼ºè§£æç»“æœï¼Œæ·»åŠ å®ä½“IDä¿¡æ¯"""
        logger.info("å¼€å§‹å¢å¼ºè§£æç»“æœï¼Œæ·»åŠ å®ä½“IDä¿¡æ¯")
        
        # å®‰å…¨åœ°è½¬æ¢ç´¢å¼•ä¸ºå®ä½“ID
        def safe_index_to_id(index_str: str) -> Tuple[Optional[str], Optional[int]]:
            """å®‰å…¨åœ°å°†ç´¢å¼•å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®ä½“IDå’Œç´¢å¼•"""
            try:
                # è½¬æ¢ä¸º0-basedç´¢å¼•
                index = int(index_str) - 1 if index_str.isdigit() else int(index_str)
                if index < 0:
                    index = int(index_str) - 1 if int(index_str) > 0 else 0
                
                if 0 <= index < len(entities):
                    entity_id = entities[index].get('id', f'entity_{index}')
                    return entity_id, index
                else:
                    logger.warning(f"ç´¢å¼• {index_str} è¶…å‡ºå®ä½“èŒƒå›´ (0-{len(entities)-1})")
                    return None, None
            except (ValueError, TypeError) as e:
                logger.warning(f"æ— æ³•è½¬æ¢ç´¢å¼• '{index_str}': {str(e)}")
                return None, None
        
        # å¢å¼ºåˆå¹¶ç»„
        enhanced_merge_groups = []
        for group in parsed_result.get("merge_groups", []):
            if not isinstance(group, dict):
                continue
            
            enhanced_group = dict(group)
            
            # å¤„ç†ä¸»å®ä½“ID
            primary_entity = group.get("primary_entity", "1")
            primary_entity_id, primary_index = safe_index_to_id(str(primary_entity))
            if primary_entity_id:
                enhanced_group["primary_entity_id"] = primary_entity_id
                enhanced_group["primary_entity_index"] = primary_index
            
            # å¤„ç†é‡å¤å®ä½“ID
            duplicates = group.get("duplicates", [])
            duplicate_entity_ids = []
            duplicate_indices = []
            
            for dup in duplicates:
                dup_id, dup_index = safe_index_to_id(str(dup))
                if dup_id and dup_index is not None:
                    duplicate_entity_ids.append(dup_id)
                    duplicate_indices.append(dup_index)
            
            enhanced_group["duplicate_entity_ids"] = duplicate_entity_ids
            enhanced_group["duplicate_indices"] = duplicate_indices
            
            enhanced_merge_groups.append(enhanced_group)
            
            logger.debug(f"å¢å¼ºåˆå¹¶ç»„: ä¸»å®ä½“ {primary_entity} -> ID {primary_entity_id}, "
                        f"é‡å¤å®ä½“ {duplicates} -> IDs {duplicate_entity_ids}")
        
        parsed_result["merge_groups"] = enhanced_merge_groups
        
        logger.info(f"å®Œæˆå®ä½“IDå¢å¼ºï¼Œå¤„ç†äº† {len(enhanced_merge_groups)} ä¸ªåˆå¹¶ç»„")
        return parsed_result
    
    def _create_default_analysis_result(self, response_content: str) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤çš„åˆ†æç»“æœ"""
        return {
            "analysis_summary": f"è§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†æ‰€æœ‰å®ä½“ä¸ºç‹¬ç«‹: {response_content[:100]}...",
            "merge_groups": [],
            "independent_entities": [],
            "uncertain_cases": [],
            "parsing_error": True
        }
    
    def _get_entity_text_representation(self, entity: Dict[str, Any]) -> str:
        """è·å–å®ä½“çš„æ–‡æœ¬è¡¨ç¤º"""
        parts = [entity.get("name", "")]
        
        if entity.get("type"):
            parts.append(f"ç±»å‹:{entity['type']}")
        
        if entity.get("description"):
            parts.append(f"æè¿°:{entity['description']}")
        
        return " ".join(parts)
    
    # === å…¬å…±æ¥å£ ===
    
    async def deduplicate_entities_list(self, entities: List[Dict[str, Any]], entity_type: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®ä½“å»é‡ï¼ˆæ–°çš„å®ä½“åˆ—è¡¨æ¨¡å¼ï¼‰
        
        Args:
            entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameã€descriptionã€typeç­‰ä¿¡æ¯
            entity_type: å®ä½“ç±»å‹
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹LangGraphå®ä½“å»é‡: {entity_type} ç±»å‹, {len(entities)} ä¸ªå®ä½“ [åˆ—è¡¨æ¨¡å¼]")
        
        # åˆ›å»ºç®€åŒ–çš„åˆå§‹çŠ¶æ€ï¼ˆè·³è¿‡å‘é‡é¢„ç­›é€‰ï¼‰
        initial_state = self._create_list_mode_initial_state(entities, entity_type)
        
        # æ‰§è¡Œç®€åŒ–çš„çŠ¶æ€å›¾ï¼ˆè·³è¿‡å‘é‡é¢„ç­›é€‰ï¼‰
        final_state = await self._execute_list_mode_graph(initial_state)
        
        # æ ¼å¼åŒ–ç»“æœ
        return self._format_result(final_state)
    
    async def deduplicate_entities(self, entities: List[Dict[str, Any]], entity_type: str) -> Dict[str, Any]:
        """æ‰§è¡Œå®ä½“å»é‡ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
        logger.info(f"å¼€å§‹LangGraphå®ä½“å»é‡: {entity_type} ç±»å‹, {len(entities)} ä¸ªå®ä½“ [å…¼å®¹æ¨¡å¼]")
        
        # å…¼å®¹æ€§å¤„ç†ï¼šä½¿ç”¨æ–°çš„åˆ—è¡¨æ¨¡å¼
        return await self.deduplicate_entities_list(entities, entity_type)
    
    def _format_result(self, state: EntityDeduplicationState) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ç»“æœä¸ºæ ‡å‡†æ ¼å¼"""
        
        def safe_int_conversion(value, default=0):
            """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°"""
            if isinstance(value, int):
                return value
            elif isinstance(value, (str, float)):
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return default
            else:
                return default
        
        def safe_format_merge_groups(groups):
            """å®‰å…¨åœ°æ ¼å¼åŒ–åˆå¹¶ç»„"""
            formatted_groups = []
            for group in groups:
                if not isinstance(group, dict):
                    continue
                
                # å¤„ç†LLMè¿”å›çš„ç´¢å¼•æ ¼å¼
                primary_entity = group.get("primary_entity", "1")
                duplicates = group.get("duplicates", [])
                
                # ç¡®ä¿duplicatesæ˜¯åˆ—è¡¨æ ¼å¼
                if not isinstance(duplicates, list):
                    duplicates = []
                
                formatted_groups.append({
                    "primary_entity": str(primary_entity),
                    "duplicates": [str(dup) for dup in duplicates],
                    "primary_entity_id": group.get("primary_entity_id", ""),
                    "duplicate_entity_ids": group.get("duplicate_entity_ids", []),
                    "merged_name": group.get("merged_name", ""),
                    "merged_description": group.get("merged_description", ""),
                    "confidence": group.get("confidence", 0.0),
                    "reason": group.get("reason", ""),
                    "wikipedia_evidence": group.get("wikipedia_evidence", "")
                })
            
            return formatted_groups
        
        def safe_format_independent_entities(entities):
            """å®‰å…¨åœ°æ ¼å¼åŒ–ç‹¬ç«‹å®ä½“"""
            if not isinstance(entities, list):
                return []
            
            formatted_entities = []
            for idx in entities:
                safe_idx = safe_int_conversion(idx)
                formatted_entities.append(str(safe_idx + 1))
            
            return formatted_entities
        
        try:
            return {
                "analysis_summary": state.get("final_decision_result", {}).get("decision_summary", "LangGraphåˆ†æå®Œæˆ"),
                "merge_groups": safe_format_merge_groups(state.get("merge_groups", [])),
                "independent_entities": safe_format_independent_entities(state.get("independent_entities", [])),
                "uncertain_cases": state.get("uncertain_cases", []),
                "statistics": {
                    "processing_strategy": "langgraph_agent",
                    "total_entities": state.get("total_entities", 0),
                    "pairs_analyzed": state.get("pairs_analyzed", 0), 
                    "tool_calls_performed": len(state.get("tool_calls_made", [])),
                    "processing_time": state.get("processing_time", 0.0),
                    "prescreening_stats": state.get("prescreening_stats", {}),
                    "current_step": state.get("current_step", "unknown"),
                    "steps_completed": len(state.get("step_history", [])),
                    "reasoning_steps": len(state.get("reasoning_steps", []))
                },
                "errors": state.get("errors", []),
                "warnings": state.get("warnings", [])
            }
        except Exception as e:
            logger.error(f"ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤ç»“æœ
            return {
                "analysis_summary": f"ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}",
                "merge_groups": [],
                "independent_entities": [str(i + 1) for i in range(len(state.get("entities", [])))],
                "uncertain_cases": [],
                "statistics": {
                    "processing_strategy": "langgraph_agent_with_error",
                    "total_entities": len(state.get("entities", [])),
                    "pairs_analyzed": 0,
                    "tool_calls_performed": 0,
                    "processing_time": 0.0,
                    "prescreening_stats": {},
                    "current_step": "error",
                    "steps_completed": 0,
                    "reasoning_steps": 0
                },
                "errors": state.get("errors", []) + [f"ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}"],
                "warnings": state.get("warnings", [])
            }


# === ä¿å®ˆåˆ†ææ–¹æ³• ===

    def _build_conservative_analysis_prompt(self, prescreened_pairs: List[Dict[str, Any]], entity_type: str) -> str:
        """æ„å»ºè¶…ä¿å®ˆçš„åˆ†æprompt"""
        
        type_mapping = {
            "ç»„ç»‡": "Organization", "äººç‰©": "Person", "åœ°ç‚¹": "Location", 
            "äº§å“": "Product", "æŠ€æœ¯": "Technology", "æ—¶é—´": "Time", "äº‹ä»¶": "Event"
        }
        english_type = type_mapping.get(entity_type, entity_type)
        
        prompt = f"""You are an ULTRA-CONSERVATIVE {english_type} entity deduplication expert for a LangGraph Agent system.\n\nğŸš¨ CRITICAL MISSION: Prevent ANY incorrect merges. False negatives are acceptable, false positives are CATASTROPHIC.\n\nâ›” ABSOLUTE PROHIBITIONS - NEVER MERGE:\n- Different companies: Apple â‰  Google â‰  Microsoft â‰  Amazon â‰  Stanford University â‰  OpenAI\n- Different people: Steve Jobs â‰  Tim Cook â‰  Sundar Pichai â‰  Satya Nadella â‰  Elon Musk\n- Competitors in ANY industry\n- Different organization types: University â‰  Corporation â‰  Government â‰  NGO\n- Different time periods or contexts\n- ANY entities where you have even 1% doubt\n\nâœ… ONLY SUGGEST MERGING IF:\n- IDENTICAL names in different languages (Apple Inc â†” è‹¹æœå…¬å¸)\n- OBVIOUS abbreviations of EXACT SAME entity (IBM â†” International Business Machines)\n- CONFIRMED aliases with 100% certainty\n\nCONFIDENCE LEVELS (Use EXTREMELY sparingly):\n- high: 99.9% certain identical entity (e.g., \"Apple Inc\" vs \"Apple Incorporated\")\n- medium: Possible same entity, MUST have Wikipedia verification  \n- low: Uncertain, requires extensive research\n\nTARGET: Maximum 5% of pairs should be anything above 'low'. Default to rejecting merges.\n\nEntity Pairs to Analyze:\n"""
        
        for i, pair in enumerate(prescreened_pairs[:50]):  # é™åˆ¶æ•°é‡é¿å…promptè¿‡é•¿
            prompt += f"\nPair {i+1}:\n"
            prompt += f"  - Entity A: {pair['entity1_name']}\n"
            prompt += f"  - Entity B: {pair['entity2_name']}\n"
            prompt += f"  - Vector Similarity: {pair.get('vector_similarity', 0.0):.3f}\n"
        
        if len(prescreened_pairs) > 50:
            prompt += f"\n... and {len(prescreened_pairs) - 50} more pairs (truncated for analysis)\n"
        
        prompt += """\nReturn JSON format analysis:\n```json\n{\n  \"analysis_summary\": \"Ultra-conservative analysis with extreme prejudice against merging\",\n  \"entity_pairs\": [\n    {\n      \"entity1_index\": 0,\n      \"entity2_index\": 1,\n      \"entity1_name\": \"Entity 1 name\",\n      \"entity2_name\": \"Entity 2 name\", \n      \"confidence\": \"low\",\n      \"similarity_score\": 0.3,\n      \"reason\": \"Specific reason why they might be related\",\n      \"needs_verification\": true,\n      \"rejection_reason\": \"Why they should NOT be merged (always consider this)\"\n    }\n  ]\n}\n```\n\nREMEMBER: When in doubt, REJECT the merge. Better to have 1000 duplicates than 1 wrong merge."""
        
        return prompt
    
    def _process_entity_pairs_conservative(self, raw_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†å®ä½“å¯¹ï¼Œå¼ºåˆ¶æ›´ä¿å®ˆçš„ç½®ä¿¡åº¦"""
        processed_pairs = []
        
        for pair in raw_pairs:
            # å¼ºåˆ¶é™çº§æ‰€æœ‰high confidenceåˆ°medium
            original_confidence = pair.get("confidence", "low")
            if original_confidence == "high":
                # åªæœ‰æå°‘æ•°æƒ…å†µæ‰ä¿æŒhigh confidence
                entity1_name = pair.get("entity1_name", "")
                entity2_name = pair.get("entity2_name", "")
                
                # æ›´ä¸¥æ ¼çš„high confidenceæ ‡å‡†
                is_truly_identical = (
                    entity1_name.lower().strip() == entity2_name.lower().strip() or
                    (len(entity1_name) <= 4 and entity1_name.upper() in entity2_name.upper()) or
                    (len(entity2_name) <= 4 and entity2_name.upper() in entity1_name.upper())
                )
                
                if not is_truly_identical:
                    pair["confidence"] = "medium"
                    pair["reason"] = f"é™çº§: {pair.get('reason', '')} (è‡ªåŠ¨ä¿å®ˆåŒ–)"
                    logger.info(f"å¼ºåˆ¶é™çº§ç½®ä¿¡åº¦: {entity1_name} vs {entity2_name}")
            
            # å¼ºåˆ¶æ‰€æœ‰å®ä½“å¯¹éƒ½éœ€è¦éªŒè¯
            pair["needs_verification"] = True
            
            processed_pairs.append(pair)
        
        return processed_pairs
    
    def _build_final_decision_prompt(self, state: EntityDeduplicationState) -> str:
        """æ„å»ºæœ€ç»ˆå†³ç­–prompt"""
        
        type_mapping = {
            "ç»„ç»‡": "Organization", "äººç‰©": "Person", "åœ°ç‚¹": "Location", 
            "äº§å“": "Product", "æŠ€æœ¯": "Technology", "æ—¶é—´": "Time", "äº‹ä»¶": "Event"
        }
        english_type = type_mapping.get(state["entity_type"], state["entity_type"])
        
        prompt = f"""FINAL DECISION for {english_type} entity merging in LangGraph Agent.\n\nğŸš¨ ULTRA-CONSERVATIVE FINAL VALIDATION ğŸš¨\n\nPrevious Analysis Results:\n{json.dumps(state.get("analysis_result", {}), ensure_ascii=False, indent=2)}\n\nWikipedia Verification Results:\n"""
        
        # ä»tool_resultsä¸­è·å–Wikipediaæœç´¢ç»“æœ
        for tool_result in state.get("tool_results", []):
            if tool_result.get("tool_name") == "search_wikipedia_entity":
                tool_input = tool_result.get("input", {})
                entity_name = tool_input.get("entity_name", "Unknown")
                result_data = tool_result.get("result", {})
                
                prompt += f"\nEntity: {entity_name}\n"
                if result_data.get("found"):
                    prompt += f"  - Title: {result_data.get('title', 'N/A')}\n"
                    prompt += f"  - Summary: {result_data.get('summary', 'N/A')[:200]}...\n"
                    prompt += f"  - URL: {result_data.get('url', 'N/A')}\n"
                else:
                    prompt += f"  - No Wikipedia entry found\n"
                    if result_data.get("error"):
                        prompt += f"  - Error: {result_data['error']}\n"
        
        prompt += f"""\nğŸ”’ FINAL MERGE CONDITIONS (ALL must be TRUE):\n1. Wikipedia EXPLICITLY confirms they are IDENTICAL entities\n2. One redirects to other OR explicitly states aliases\n3. ZERO contradictory evidence found\n4. Confidence â‰¥ 0.98 (98% certainty minimum)\n5. Common sense verification passes\n6. No competing interpretations exist\n\nâŒ IMMEDIATE REJECTION IF:\n- Different Wikipedia pages exist for both\n- No clear Wikipedia confirmation\n- ANY conflicting information detected\n- Different entity categories/types\n- ANY doubt whatsoever exists\n\nğŸ” FINAL CONTRADICTION CHECK:\nBefore ANY merge suggestion, verify:\n- Are they competitors or rivals?\n- Do they serve different functions?\n- Are they from different domains?\n- Could they coexist independently?\n\nReturn JSON format FINAL decision:\n```json\n{{\n  \"decision_summary\": \"Ultra-conservative final decision with exhaustive verification\",\n  \"merge_groups\": [\n    {{\n      \"primary_entity_index\": 0,\n      \"duplicate_indices\": [1],\n      \"merged_name\": \"Verified identical entity name\",\n      \"merged_description\": \"Verified description\", \n      \"confidence\": 0.98,\n      \"reason\": \"Wikipedia explicitly confirms identical entity with redirect\",\n      \"wikipedia_evidence\": \"Specific Wikipedia evidence\",\n      \"final_verification\": \"Passed ultra-conservative validation\"\n    }}\n  ],\n  \"independent_entities\": [2, 3, 4, 5, 6, 7, 8],\n  \"uncertain_cases\": [\n    {{\n      \"entities\": [9, 10], \n      \"reason\": \"Insufficient evidence for safe merging - keeping separate\"\n    }}\n  ]\n}}\n```\n\nDEFAULT DECISION: Keep entities separate unless overwhelming evidence proves they are identical."""
        
        return prompt
    
    def _parse_llm_analysis(self, response_content: str) -> Dict[str, Any]:
        """è§£æLLMåˆ†æå“åº”"""
        try:
            json_match = self._extract_json_from_response(response_content)
            if json_match:
                return json.loads(json_match)
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
        except Exception as e:
            logger.error(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {str(e)}")
            return {"analysis_summary": "è§£æå¤±è´¥", "entity_pairs": []}
    
    def _parse_final_decision(self, response_content: str) -> Dict[str, Any]:
        """è§£ææœ€ç»ˆå†³ç­–å“åº”"""
        try:
            json_match = self._extract_json_from_response(response_content)
            if json_match:
                return json.loads(json_match)
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
        except Exception as e:
            logger.error(f"è§£ææœ€ç»ˆå†³ç­–å“åº”å¤±è´¥: {str(e)}")
            return {
                "decision_summary": "è§£æå¤±è´¥",
                "merge_groups": [],
                "independent_entities": [],
                "uncertain_cases": []
            }
    
    def _extract_json_from_response(self, response_content: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
        import re
        
        # å°è¯•æå–```json...```æ ¼å¼
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_content, re.DOTALL)
        if json_match:
            return json_match.group(1)
        
        # å°è¯•ç›´æ¥æŸ¥æ‰¾JSONå¯¹è±¡
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_content, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        return None
    
    async def _search_wikipedia_for_entity(self, entity: Dict[str, Any], entity_type: str) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªå®ä½“æœç´¢Wikipedia"""
        try:
            search_result = self.wikipedia_server.search_entity(
                entity_name=entity["name"],
                entity_type=entity_type
            )
            return search_result
        except Exception as e:
            logger.error(f"Wikipediaæœç´¢å¤±è´¥: {entity['name']}, é”™è¯¯: {str(e)}")
            return {
                "entity_name": entity["name"],
                "found": False,
                "error": str(e)
            }
    
    def _validate_merge_decisions_ultra_conservative(self, merge_groups: List[Dict[str, Any]], 
                                                   state: EntityDeduplicationState) -> List[Dict[str, Any]]:
        """è¶…ä¿å®ˆçš„åˆå¹¶å†³ç­–éªŒè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        
        # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šè¶…ä¿å®ˆéªŒè¯å¼€å§‹
        logger.info("=" * 80)
        logger.info(f"ğŸ” è¶…ä¿å®ˆéªŒè¯å¼€å§‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰- {state.get('entity_type', 'Unknown')} ç±»å‹")
        logger.info("=" * 80)
        logger.info(f"å¾…éªŒè¯çš„åˆå¹¶ç»„æ•°é‡: {len(merge_groups)}")
        logger.info("éªŒè¯æ ‡å‡†å±‚æ¬¡:")
        logger.info("  ğŸš€ å¼ºåˆ¶åˆå¹¶å±‚çº§:")
        logger.info("    - å®Œå…¨ç›¸åŒçš„åç§°ï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰")
        logger.info("    - æ˜æ˜¾çš„åˆ«åæ˜ å°„ï¼ˆå¦‚ Tim Cook â†” Timothy Cookï¼‰")
        logger.info("    - è·¨è¯­è¨€åŒå®ä½“ï¼ˆå¦‚ Tim Cook â†” è’‚å§†Â·åº“å…‹ï¼‰")
        logger.info("  ğŸ“Š æ ‡å‡†éªŒè¯å±‚çº§:")
        logger.info("    - ç½®ä¿¡åº¦ >= 0.95 (95%)")
        logger.info("    - å¿…é¡»æœ‰Wikipediaè¯æ®")
        logger.info("    - è¯æ®åŒ…å« 'redirect', 'alias', æˆ– 'same' å…³é”®è¯")
        logger.info("-" * 40)
        
        validated_groups = []
        
        def safe_int_conversion(value, default=None):
            """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°"""
            if isinstance(value, int):
                return value
            elif isinstance(value, (str, float)):
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return default
            else:
                return default
        
        def should_force_merge(group: Dict[str, Any], state: EntityDeduplicationState) -> Tuple[bool, str]:
            """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶åˆå¹¶ï¼ˆæ˜æ˜¾é‡å¤çš„å®ä½“ï¼‰"""
            
            try:
                # è·å–å®ä½“åç§°
                primary_entity_idx = safe_int_conversion(group.get("primary_entity", "1"), 1) - 1
                duplicate_indices = []
                for dup in group.get("duplicates", []):
                    dup_idx = safe_int_conversion(dup, 1) - 1
                    if dup_idx >= 0:
                        duplicate_indices.append(dup_idx)
                
                if primary_entity_idx < 0 or not duplicate_indices:
                    return False, "æ— æ•ˆçš„å®ä½“ç´¢å¼•"
                
                entities = state.get("entities_ready_for_analysis") or state.get("entities", [])
                if primary_entity_idx >= len(entities):
                    return False, "ä¸»å®ä½“ç´¢å¼•è¶…å‡ºèŒƒå›´"
                
                primary_name = entities[primary_entity_idx].get("name", "").strip()
                duplicate_names = []
                for dup_idx in duplicate_indices:
                    if dup_idx < len(entities):
                        duplicate_names.append(entities[dup_idx].get("name", "").strip())
                
                if not primary_name or not duplicate_names:
                    return False, "å®ä½“åç§°ä¸ºç©º"
                
                # å¼ºåˆ¶åˆå¹¶è§„åˆ™æ£€æŸ¥
                for dup_name in duplicate_names:
                    
                    # 1. å®Œå…¨ç›¸åŒçš„æ ‡å‡†åŒ–åç§°
                    if primary_name.lower().replace(" ", "").replace("-", "").replace(".", "") == \
                       dup_name.lower().replace(" ", "").replace("-", "").replace(".", ""):
                        return True, f"å®Œå…¨ç›¸åŒçš„æ ‡å‡†åŒ–åç§°: '{primary_name}' â‰¡ '{dup_name}'"
                    
                    # 2. æ˜æ˜¾çš„è‹±æ–‡åˆ«åæ¨¡å¼
                    eng_aliases = [
                        ("Timothy Cook", "Tim Cook"),
                        ("Timothy D. Cook", "Tim Cook"), 
                        ("Jeffrey Bezos", "Jeff Bezos"),
                        ("Jeffrey P. Bezos", "Jeff Bezos"),
                        ("Steven Jobs", "Steve Jobs"),
                        ("Steven P. Jobs", "Steve Jobs"),
                        ("William Gates", "Bill Gates"),
                        ("William H. Gates", "Bill Gates"),
                        ("Mark Zuckerberg", "Mark Elliot Zuckerberg"),
                        ("Sundar Pichai", "Pichai Sundararajan"),
                        ("Elon Musk", "Elon Reeve Musk")
                    ]
                    
                    for full_name, short_name in eng_aliases:
                        if (primary_name.lower() == full_name.lower() and dup_name.lower() == short_name.lower()) or \
                           (primary_name.lower() == short_name.lower() and dup_name.lower() == full_name.lower()):
                            return True, f"æ˜æ˜¾çš„è‹±æ–‡åˆ«å: '{primary_name}' â†” '{dup_name}'"
                    
                    # 3. è·¨è¯­è¨€åŒå®ä½“æ¨¡å¼ï¼ˆä¸­è‹±æ–‡ï¼‰
                    cross_lang_pairs = [
                        ("Tim Cook", "è’‚å§†Â·åº“å…‹"),
                        ("Timothy Cook", "è’‚å§†Â·åº“å…‹"),
                        ("Jeff Bezos", "æ°å¤«Â·è´ç´¢æ–¯"),
                        ("Jeffrey Bezos", "æ°å¤«Â·è´ç´¢æ–¯"),
                        ("Steve Jobs", "å²è’‚å¤«Â·ä¹”å¸ƒæ–¯"),
                        ("Steven Jobs", "å²è’‚å¤«Â·ä¹”å¸ƒæ–¯"),
                        ("Bill Gates", "æ¯”å°”Â·ç›–èŒ¨"),
                        ("William Gates", "æ¯”å°”Â·ç›–èŒ¨"),
                        ("Mark Zuckerberg", "é©¬å…‹Â·æ‰å…‹ä¼¯æ ¼"),
                        ("Elon Musk", "åŸƒéš†Â·é©¬æ–¯å…‹"),
                        ("Sundar Pichai", "æ¡‘è¾¾å°”Â·çš®æŸ¥ä¼Š"),
                        ("Apple", "è‹¹æœå…¬å¸"),
                        ("Apple Inc", "è‹¹æœå…¬å¸"),
                        ("Microsoft", "å¾®è½¯å…¬å¸"),
                        ("Google", "è°·æ­Œå…¬å¸"),
                        ("Amazon", "äºšé©¬é€Šå…¬å¸")
                    ]
                    
                    for eng_name, chn_name in cross_lang_pairs:
                        if (primary_name.lower() == eng_name.lower() and dup_name == chn_name) or \
                           (primary_name == chn_name and dup_name.lower() == eng_name.lower()):
                            return True, f"è·¨è¯­è¨€åŒå®ä½“: '{primary_name}' â†” '{dup_name}'"
                    
                    # 4. å…¬å¸åç§°åç¼€å˜ä½“
                    if state.get('entity_type') in ['ç»„ç»‡', 'Organization']:
                        # ç§»é™¤å¸¸è§å…¬å¸åç¼€è¿›è¡Œæ¯”è¾ƒ
                        def normalize_company_name(name):
                            suffixes = [" Inc", " Inc.", " Corporation", " Corp", " Corp.", " Company", " Co", " Co.", 
                                       " Limited", " Ltd", " Ltd.", " LLC", "å…¬å¸", "é›†å›¢", "æœ‰é™å…¬å¸"]
                            normalized = name
                            for suffix in suffixes:
                                if normalized.endswith(suffix):
                                    normalized = normalized[:-len(suffix)].strip()
                            return normalized.lower()
                        
                        if normalize_company_name(primary_name) == normalize_company_name(dup_name):
                            return True, f"å…¬å¸åç§°åç¼€å˜ä½“: '{primary_name}' â†” '{dup_name}'"
                    
                    # 5. æ£€æŸ¥æ˜¯å¦æ˜¯é«˜ç½®ä¿¡åº¦çš„æ˜æ˜¾é‡å¤
                    confidence = group.get("confidence", 0.0)
                    if confidence >= 0.98:  # 98%ä»¥ä¸Šç½®ä¿¡åº¦
                        # æ£€æŸ¥åç§°ç›¸ä¼¼åº¦
                        similarity_indicators = [
                            primary_name.lower() in dup_name.lower(),
                            dup_name.lower() in primary_name.lower(),
                            len(set(primary_name.lower().split()) & set(dup_name.lower().split())) >= 2
                        ]
                        
                        if any(similarity_indicators):
                            return True, f"è¶…é«˜ç½®ä¿¡åº¦ç›¸ä¼¼å®ä½“: '{primary_name}' â†” '{dup_name}' (ç½®ä¿¡åº¦: {confidence})"
                
                return False, "ä¸ç¬¦åˆå¼ºåˆ¶åˆå¹¶æ¡ä»¶"
                
            except Exception as e:
                logger.warning(f"å¼ºåˆ¶åˆå¹¶æ£€æŸ¥å¤±è´¥: {str(e)}")
                return False, f"æ£€æŸ¥å¼‚å¸¸: {str(e)}"
        
        for i, group in enumerate(merge_groups):
            if not isinstance(group, dict):
                logger.warning(f"  åˆå¹¶ç»„ {i+1}: æ— æ•ˆæ ¼å¼ï¼Œè·³è¿‡")
                continue
                
            # æå–éªŒè¯ä¿¡æ¯
            confidence = group.get("confidence", 0.0)
            wikipedia_evidence = group.get("wikipedia_evidence", "")
            merged_name = group.get("merged_name", "Unknown")
            primary_entity = group.get("primary_entity", "Unknown")
            duplicates = group.get("duplicates", [])
            reason = group.get("reason", "")
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šå•ä¸ªåˆå¹¶ç»„éªŒè¯
            logger.info(f"  éªŒè¯åˆå¹¶ç»„ {i+1}: {merged_name}")
            logger.info(f"    - ä¸»å®ä½“: {primary_entity}")
            logger.info(f"    - é‡å¤å®ä½“: {duplicates}")
            logger.info(f"    - ç½®ä¿¡åº¦: {confidence}")
            logger.info(f"    - ç†ç”±: {reason[:100]}..." if reason else "    - ç†ç”±: æ— ")
            logger.info(f"    - Wikipediaè¯æ®: {wikipedia_evidence[:100]}..." if wikipedia_evidence else "    - Wikipediaè¯æ®: æ— ")
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åº”è¯¥å¼ºåˆ¶åˆå¹¶
            should_force, force_reason = should_force_merge(group, state)
            
            if should_force:
                logger.info(f"    ğŸš€ å¼ºåˆ¶åˆå¹¶è§¦å‘: {force_reason}")
                validated_groups.append(group)
                logger.info(f"    ğŸ‰ åˆå¹¶å†³ç­–é€šè¿‡å¼ºåˆ¶åˆå¹¶: {merged_name}")
                continue
            
            # æ ‡å‡†éªŒè¯æµç¨‹
            validation_results = []
            
            # 1. ç½®ä¿¡åº¦éªŒè¯
            confidence_ok = confidence >= 0.95
            validation_results.append(("ç½®ä¿¡åº¦ >= 0.95", confidence_ok, f"å®é™…å€¼: {confidence}"))
            
            # 2. Wikipediaè¯æ®å­˜åœ¨éªŒè¯
            evidence_exists = bool(wikipedia_evidence)
            validation_results.append(("Wikipediaè¯æ®å­˜åœ¨", evidence_exists, f"é•¿åº¦: {len(wikipedia_evidence)}"))
            
            # 3. è¯æ®å…³é”®è¯éªŒè¯ - æ”¾å®½æ ‡å‡†
            evidence_keywords = evidence_exists and (
                any(keyword in wikipedia_evidence.lower() for keyword in ["redirect", "alias", "same"]) or
                # æ–°å¢çš„å®½æ¾å…³é”®è¯
                any(keyword in wikipedia_evidence.lower() for keyword in ["also known", "commonly called", "refers to", "identical", "equivalent"])
            )
            validation_results.append(("è¯æ®åŒ…å«ç›¸å…³å…³é”®è¯", evidence_keywords, f"æ£€æŸ¥: redirect, alias, same, also known, etc."))
            
            # 4. æ”¾å®½çš„éªŒè¯æ ‡å‡†
            passes_validation = confidence_ok and evidence_exists and evidence_keywords
            
            logger.info(f"    éªŒè¯ç»“æœ:")
            for criterion, passed, detail in validation_results:
                status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
                logger.info(f"      - {criterion}: {status} ({detail})")
            
            if passes_validation:
                validated_groups.append(group)
                logger.info(f"    ğŸ‰ åˆå¹¶å†³ç­–é€šè¿‡æ ‡å‡†éªŒè¯: {merged_name}")
            else:
                logger.warning(f"    âš ï¸ åˆå¹¶å†³ç­–æœªé€šè¿‡éªŒè¯: {merged_name}")
                
                # è¯¦ç»†è¯´æ˜æ‹’ç»åŸå› 
                rejection_reasons = []
                if not confidence_ok:
                    rejection_reasons.append(f"ç½®ä¿¡åº¦ä¸è¶³ ({confidence} < 0.95)")
                if not evidence_exists:
                    rejection_reasons.append("ç¼ºå°‘Wikipediaè¯æ®")
                if not evidence_keywords:
                    rejection_reasons.append("è¯æ®ä¸åŒ…å«ç›¸å…³å…³é”®è¯")
                
                logger.warning(f"    æ‹’ç»ç†ç”±: {'; '.join(rejection_reasons)}")
                
                # å°†è¢«æ‹’ç»çš„åˆå¹¶æ·»åŠ åˆ°ç‹¬ç«‹å®ä½“åˆ—è¡¨ï¼ˆå®‰å…¨ç±»å‹è½¬æ¢ï¼‰
                primary_idx = safe_int_conversion(group.get("primary_entity_index"))
                duplicate_indices = group.get("duplicate_indices", [])
                
                if primary_idx is not None and primary_idx not in state["independent_entities"]:
                    state["independent_entities"].append(primary_idx)
                    logger.info(f"    â†’ å°†ä¸»å®ä½“ {primary_idx} æ ‡è®°ä¸ºç‹¬ç«‹")
                    
                if isinstance(duplicate_indices, list):
                    for dup_idx in duplicate_indices:
                        safe_dup_idx = safe_int_conversion(dup_idx)
                        if safe_dup_idx is not None and safe_dup_idx not in state["independent_entities"]:
                            state["independent_entities"].append(safe_dup_idx)
                            logger.info(f"    â†’ å°†é‡å¤å®ä½“ {safe_dup_idx} æ ‡è®°ä¸ºç‹¬ç«‹")
            
            logger.info("")  # ç©ºè¡Œåˆ†éš”
        
        # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šéªŒè¯æ€»ç»“
        logger.info("ğŸ“Š è¶…ä¿å®ˆéªŒè¯æ€»ç»“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰:")
        logger.info(f"  - è¾“å…¥åˆå¹¶ç»„: {len(merge_groups)}")
        logger.info(f"  - é€šè¿‡éªŒè¯: {len(validated_groups)}")
        logger.info(f"  - è¢«æ‹’ç»: {len(merge_groups) - len(validated_groups)}")
        logger.info(f"  - éªŒè¯é€šè¿‡ç‡: {len(validated_groups) / len(merge_groups) * 100:.1f}%" if merge_groups else "  - éªŒè¯é€šè¿‡ç‡: 0%")
        
        if validated_groups:
            logger.info("âœ… é€šè¿‡éªŒè¯çš„åˆå¹¶ç»„:")
            for i, group in enumerate(validated_groups):
                logger.info(f"  {i+1}. {group.get('merged_name', 'Unknown')} (ç½®ä¿¡åº¦: {group.get('confidence', 0.0)})")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰åˆå¹¶ç»„é€šè¿‡éªŒè¯")
            
        logger.info("=" * 80)
        
        return validated_groups


# === å…¨å±€å®ä¾‹å’Œå·¥å‚å‡½æ•° ===

_langgraph_agent_instance = None

def get_langgraph_entity_deduplication_agent(config: Optional[Dict[str, Any]] = None) -> LangGraphEntityDeduplicationAgent:
    """è·å–LangGraphå®ä½“å»é‡Agentå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _langgraph_agent_instance
    if _langgraph_agent_instance is None:
        _langgraph_agent_instance = LangGraphEntityDeduplicationAgent(config)
    return _langgraph_agent_instance