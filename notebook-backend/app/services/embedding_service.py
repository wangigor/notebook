#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€åµŒå…¥å‘é‡æœåŠ¡ - å•ä¾‹ç‰ˆæœ¬
æä¾›ç»Ÿä¸€çš„æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆæ¥å£ï¼Œä½¿ç”¨å…¨å±€DashScopeå•ä¾‹
"""
import logging
import numpy as np
import os
import threading
from typing import List, Optional, Dict, Any
from langchain_core.embeddings import Embeddings
from app.core.config import settings
from app.services.dashscope_singleton import get_dashscope_client, get_dashscope_stats

logger = logging.getLogger(__name__)

# ğŸ”’ çº¿ç¨‹é”ï¼Œé˜²æ­¢å¹¶å‘åˆå§‹åŒ–å†²çª
_init_lock = threading.Lock()
_instance_lock = threading.Lock()

class EmbeddingService:
    """
    ç»Ÿä¸€åµŒå…¥å‘é‡æœåŠ¡ - Forkå®‰å…¨ç‰ˆæœ¬
    
    æä¾›ç»Ÿä¸€çš„æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆæ¥å£ï¼Œæ”¯æŒæ–‡æ¡£å’ŒæŸ¥è¯¢çš„å‘é‡åŒ–
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åµŒå…¥æœåŠ¡"""
        logger.info("åˆå§‹åŒ–ç»Ÿä¸€åµŒå…¥å‘é‡æœåŠ¡ (Forkå®‰å…¨ç‰ˆæœ¬)")
        self.embedding_model = None
        self.is_mock_mode = False
        self._process_id = os.getpid()  # è®°å½•è¿›ç¨‹ID
        self._initialized = False
        
        # ğŸ†• æ·»åŠ ç¼“å­˜å’Œæ‰¹é‡å¤„ç†æ”¯æŒ
        self._embedding_cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self._cache_hit_count = 0
        self._cache_miss_count = 0
        
        # ğŸ›¡ï¸ å®‰å…¨åˆå§‹åŒ–ï¼šåœ¨forkç¯å¢ƒä¸‹å»¶è¿Ÿåˆå§‹åŒ–
        if self._is_in_celery_worker():
            logger.info("æ£€æµ‹åˆ°Celery Workerç¯å¢ƒï¼Œå»¶è¿Ÿåˆå§‹åŒ–embeddingæ¨¡å‹")
            self._lazy_init = True
        else:
            logger.info("éCeleryç¯å¢ƒï¼Œç«‹å³åˆå§‹åŒ–embeddingæ¨¡å‹")
            self._lazy_init = False
            self._init_embedding_model()
    
    def _is_in_celery_worker(self) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨Celery Workerä¸­è¿è¡Œ"""
        # æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œè¿›ç¨‹å
        return (
            'CELERY_WORKER' in os.environ or
            'celery worker' in ' '.join(os.sys.argv) or
            hasattr(os.sys, '_called_from_test')  # æµ‹è¯•ç¯å¢ƒ
        )
    
    def _ensure_initialized(self):
        """ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if self._initialized:
            return
            
        with _init_lock:
            if self._initialized:
                return
                
            current_pid = os.getpid()
            if current_pid != self._process_id:
                logger.warning(f"æ£€æµ‹åˆ°è¿›ç¨‹IDå˜åŒ–: {self._process_id} -> {current_pid}ï¼Œé‡æ–°åˆå§‹åŒ–")
                self._process_id = current_pid
                self._initialized = False
            
            if not self._initialized:
                self._init_embedding_model()
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨DashScopeå•ä¾‹"""
        try:
            logger.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰...")
            
            # ğŸ†• ä½¿ç”¨DashScopeå•ä¾‹è€Œéç‹¬ç«‹å®ä¾‹
            self.embedding_model = get_dashscope_client()
            
            # è·å–å•ä¾‹ç»Ÿè®¡ä¿¡æ¯
            stats = get_dashscope_stats()
            self.is_mock_mode = stats.get("is_mock_mode", False)
            
            if self.is_mock_mode:
                logger.warning(f"âš ï¸ ä½¿ç”¨Mockæ¨¡å¼ï¼ŒåŸå› : {stats.get('initialization_error', 'æœªçŸ¥')}")
            else:
                logger.info(f"âœ… ä½¿ç”¨DashScopeå•ä¾‹æˆåŠŸï¼Œè¿æ¥æ•°: {stats.get('connection_count', 0)}")
                
        except Exception as e:
            logger.error(f"âŒ è·å–DashScopeå•ä¾‹å¤±è´¥: {str(e)}")
            logger.warning("ğŸš¨ å›é€€åˆ°æœ¬åœ°Mockæ¨¡å¼")
            self.is_mock_mode = True
            self.embedding_model = self._create_mock_embeddings()
        finally:
            self._initialized = True
    
    def _create_mock_embeddings(self) -> Embeddings:
        """åˆ›å»ºæ¨¡æ‹ŸåµŒå…¥æ¨¡å‹"""
        
        class MockEmbeddings(Embeddings):
            """MockåµŒå…¥æ¨¡å‹ï¼Œç”¨äºæµ‹è¯•å’Œå¤‡ç”¨"""
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                """ä¸ºæ–‡æœ¬ç”ŸæˆéšæœºåµŒå…¥å‘é‡"""
                logger.warning(f"ğŸ­ ä½¿ç”¨MockEmbeddingså¤„ç† {len(texts)} æ¡æ–‡æœ¬")
                try:
                    import random
                    # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿ä¸€è‡´æ€§
                    random.seed(hash(' '.join(texts)) % 2**32)
                    return [[random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)] for _ in range(len(texts))]
                except Exception as e:
                    logger.error(f"Mock embeddingç”Ÿæˆå¤±è´¥: {e}")
                    return [[0.0] * settings.VECTOR_SIZE for _ in range(len(texts))]
                
            def embed_query(self, text: str) -> List[float]:
                """ä¸ºæŸ¥è¯¢ç”ŸæˆéšæœºåµŒå…¥å‘é‡"""
                logger.warning(f"ğŸ­ ä½¿ç”¨MockEmbeddingså¤„ç†æŸ¥è¯¢: {text[:30]}...")
                try:
                    import random
                    random.seed(hash(text) % 2**32)
                    return [random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)]
                except Exception as e:
                    logger.error(f"Mock query embeddingç”Ÿæˆå¤±è´¥: {e}")
                    return [0.0] * settings.VECTOR_SIZE
        
        logger.info("âœ¨ åˆ›å»º MockEmbeddings ä½œä¸ºå¤‡ä»½æ–¹æ¡ˆ")
        return MockEmbeddings()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡æ–‡æ¡£åµŒå…¥
        
        Args:
            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            logger.warning("è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
            return []
        
        # ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–
        self._ensure_initialized()
        
        try:
            logger.info(f"æ­£åœ¨ä¸º {len(texts)} æ¡æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡...")
            embeddings = self.embedding_model.embed_documents(texts)
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªåµŒå…¥å‘é‡")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ–‡æ¡£åµŒå…¥å¤±è´¥: {str(e)}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºå¤‡ä»½
            logger.warning("ğŸ”„ è¿”å›éšæœºå‘é‡ä½œä¸ºå¤‡ä»½")
            try:
                import random
                return [[random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)] for _ in range(len(texts))]
            except Exception as backup_error:
                logger.error(f"âŒ å¤‡ä»½å‘é‡ç”Ÿæˆä¹Ÿå¤±è´¥: {backup_error}")
                return [[0.0] * settings.VECTOR_SIZE for _ in range(len(texts))]
    
    def embed_query(self, text: str) -> List[float]:
        """
        å•ä¸ªæŸ¥è¯¢åµŒå…¥
        
        Args:
            text: è¦åµŒå…¥çš„æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[float]: åµŒå…¥å‘é‡
        """
        if not text:
            logger.warning("è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return [0.0] * settings.VECTOR_SIZE
        
        # ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–
        self._ensure_initialized()
        
        try:
            logger.info(f"æ­£åœ¨ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡: {text[:50]}...")
            embedding = self.embedding_model.embed_query(text)
            logger.info("âœ… æˆåŠŸç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡")
            return embedding
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥: {str(e)}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºå¤‡ä»½
            logger.warning("ğŸ”„ è¿”å›éšæœºå‘é‡ä½œä¸ºå¤‡ä»½")
            try:
                import random
                random.seed(hash(text) % 2**32)
                return [random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)]
            except Exception as backup_error:
                logger.error(f"âŒ å¤‡ä»½å‘é‡ç”Ÿæˆä¹Ÿå¤±è´¥: {backup_error}")
                return [0.0] * settings.VECTOR_SIZE
    
    def get_vector_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return settings.VECTOR_SIZE
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§"""
        try:
            self._ensure_initialized()
            
            if self.is_mock_mode:
                logger.info("ğŸ­ åµŒå…¥æœåŠ¡è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")
                return False
            
            # è¿›è¡Œç®€å•çš„å¯ç”¨æ€§æµ‹è¯•
            test_embedding = self.embedding_model.embed_query("æµ‹è¯•")
            return len(test_embedding) == settings.VECTOR_SIZE
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    # ğŸ†• æ‰¹é‡embeddingå¤„ç†ç­–ç•¥
    async def embed_documents_batch(self, texts: List[str], 
                                   batch_size: Optional[int] = None,
                                   use_cache: bool = True,
                                   max_retries: int = 3) -> List[List[float]]:
        """
        æ™ºèƒ½æ‰¹é‡æ–‡æ¡£åµŒå…¥ï¼Œæ”¯æŒç¼“å­˜ã€é‡è¯•å’Œæ€§èƒ½ä¼˜åŒ–
        
        Args:
            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®å€¼
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            logger.warning("è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
            return []
        
        import asyncio
        import time
        
        start_time = time.time()
        batch_size = batch_size or getattr(settings, 'ENTITY_EMBEDDING_BATCH_SIZE', 50)
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–
        self._ensure_initialized()
        
        all_embeddings = []
        cache_hits = 0
        cache_misses = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # æ£€æŸ¥ç¼“å­˜
                if use_cache:
                    cached_embeddings, uncached_texts, cache_indices = self._check_batch_cache(batch_texts)
                    cache_hits += len(cached_embeddings)
                    cache_misses += len(uncached_texts)
                else:
                    uncached_texts = batch_texts
                    cache_indices = list(range(len(batch_texts)))
                    cached_embeddings = []
                
                # ç”Ÿæˆæœªç¼“å­˜çš„embeddings
                if uncached_texts:
                    new_embeddings = await self._embed_with_retry(uncached_texts, max_retries)
                    
                    # æ›´æ–°ç¼“å­˜
                    if use_cache:
                        self._update_batch_cache(uncached_texts, new_embeddings)
                else:
                    new_embeddings = []
                
                # åˆå¹¶ç»“æœ
                batch_embeddings = self._merge_batch_results(
                    cached_embeddings, new_embeddings, cache_indices, len(batch_texts)
                )
                all_embeddings.extend(batch_embeddings)
                
                # æ§åˆ¶APIè°ƒç”¨é¢‘ç‡
                if i + batch_size < len(texts):
                    await asyncio.sleep(0.1)
            
            processing_time = time.time() - start_time
            hit_rate = cache_hits / (cache_hits + cache_misses) if (cache_hits + cache_misses) > 0 else 0
            
            logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {processing_time:.2f}ç§’, "
                       f"ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1%} ({cache_hits}/{cache_hits + cache_misses})")
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åµŒå…¥å¤„ç†å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šè¿”å›éšæœºå‘é‡
            logger.warning("ğŸ”„ è¿”å›éšæœºå‘é‡ä½œä¸ºé™çº§å¤„ç†")
            try:
                import random
                return [[random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)] for _ in texts]
            except Exception as backup_error:
                logger.error(f"âŒ é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {backup_error}")
                return [[0.0] * settings.VECTOR_SIZE for _ in texts]
    
    def _check_batch_cache(self, texts: List[str]) -> tuple:
        """æ£€æŸ¥æ‰¹é‡æ–‡æœ¬çš„ç¼“å­˜çŠ¶æ€"""
        cached_embeddings = []
        uncached_texts = []
        cache_indices = []
        
        for i, text in enumerate(texts):
            cache_key = self._generate_cache_key(text)
            if cache_key in self._embedding_cache:
                cached_embeddings.append(self._embedding_cache[cache_key])
                self._cache_hit_count += 1
            else:
                uncached_texts.append(text)
                cache_indices.append(i)
                self._cache_miss_count += 1
        
        return cached_embeddings, uncached_texts, cache_indices
    
    def _merge_batch_results(self, cached_embeddings: List[List[float]], 
                           new_embeddings: List[List[float]], 
                           cache_indices: List[int], 
                           total_count: int) -> List[List[float]]:
        """åˆå¹¶ç¼“å­˜å’Œæ–°ç”Ÿæˆçš„embeddings"""
        result = [None] * total_count
        
        # å¡«å…¥ç¼“å­˜çš„ç»“æœ
        cached_idx = 0
        new_idx = 0
        
        for i in range(total_count):
            if i in cache_indices:
                if new_idx < len(new_embeddings):
                    result[i] = new_embeddings[new_idx]
                    new_idx += 1
                else:
                    # å¤‡ç”¨é›¶å‘é‡
                    result[i] = [0.0] * settings.VECTOR_SIZE
            else:
                if cached_idx < len(cached_embeddings):
                    result[i] = cached_embeddings[cached_idx]
                    cached_idx += 1
                else:
                    # å¤‡ç”¨é›¶å‘é‡
                    result[i] = [0.0] * settings.VECTOR_SIZE
        
        return result
    
    def _update_batch_cache(self, texts: List[str], embeddings: List[List[float]]):
        """æ›´æ–°æ‰¹é‡ç¼“å­˜"""
        for text, embedding in zip(texts, embeddings):
            cache_key = self._generate_cache_key(text)
            self._embedding_cache[cache_key] = embedding
        
        # æ¸…ç†ç¼“å­˜
        self._clean_cache()
    
    async def _embed_with_retry(self, texts: List[str], max_retries: int) -> List[List[float]]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„åµŒå…¥ç”Ÿæˆ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        import asyncio
        
        for attempt in range(max_retries + 1):
            try:
                # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥è°ƒç”¨åŒæ­¥æ–¹æ³•
                loop = asyncio.get_event_loop()
                embeddings = await loop.run_in_executor(
                    None, self.embedding_model.embed_documents, texts
                )
                return embeddings
                
            except Exception as e:
                if attempt < max_retries:
                    wait_time = (2 ** attempt) * 0.5  # æŒ‡æ•°é€€é¿
                    logger.warning(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {str(e)}")
                    logger.info(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                    # è¿”å›éšæœºå‘é‡ä½œä¸ºé™çº§
                    try:
                        import random
                        return [[random.gauss(0, 1) for _ in range(settings.VECTOR_SIZE)] for _ in texts]
                    except Exception as backup_error:
                        logger.error(f"âŒ é™çº§å‘é‡ç”Ÿæˆå¤±è´¥: {backup_error}")
                        return [[0.0] * settings.VECTOR_SIZE for _ in texts]
    
    def _generate_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        # æ ‡å‡†åŒ–æ–‡æœ¬
        normalized_text = text.strip().lower()
        # ç”ŸæˆMD5å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        return hashlib.md5(normalized_text.encode('utf-8')).hexdigest()
    
    def _clean_cache(self):
        """æ¸…ç†ç¼“å­˜ï¼Œä¿ç•™æœ€è¿‘ä½¿ç”¨çš„ä¸€åŠ"""
        cache_limit = getattr(settings, 'ENTITY_SIMILARITY_CACHE_SIZE', 1000)
        if len(self._embedding_cache) <= cache_limit:
            return
        
        # ç®€å•çš„LRUç­–ç•¥ï¼šåˆ é™¤ä¸€åŠæœ€æ—§çš„æ¡ç›®
        cache_items = list(self._embedding_cache.items())
        keep_count = cache_limit // 2
        
        # ä¿ç•™åä¸€åŠï¼ˆå‡è®¾åæ·»åŠ çš„æ›´æ–°ï¼‰
        new_cache = dict(cache_items[-keep_count:])
        self._embedding_cache = new_cache
        
        logger.debug(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(self._embedding_cache)} é¡¹")
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._cache_hit_count + self._cache_miss_count
        hit_rate = self._cache_hit_count / total_requests if total_requests > 0 else 0
        
        return {
            "cache_size": len(self._embedding_cache),
            "cache_limit": getattr(settings, 'ENTITY_SIMILARITY_CACHE_SIZE', 1000),
            "total_requests": total_requests,
            "cache_hits": self._cache_hit_count,
            "cache_misses": self._cache_miss_count,
            "hit_rate": hit_rate,
            "memory_usage_estimate": len(self._embedding_cache) * settings.VECTOR_SIZE * 4,  # float32ä¼°ç®—
            "process_id": self._process_id,
            "is_mock_mode": self.is_mock_mode
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._embedding_cache.clear()
        self._cache_hit_count = 0
        self._cache_miss_count = 0
        logger.info("ğŸ§¹ åµŒå…¥ç¼“å­˜å·²æ¸…ç©º")


# ğŸ”’ åˆ›å»ºå…¨å±€å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
_embedding_service_instance = None

def get_embedding_service() -> EmbeddingService:
    """
    è·å–åµŒå…¥æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
    
    Returns:
        EmbeddingService: åµŒå…¥æœåŠ¡å®ä¾‹
    """
    global _embedding_service_instance
    if _embedding_service_instance is None:
        with _instance_lock:
            if _embedding_service_instance is None:
                _embedding_service_instance = EmbeddingService()
    return _embedding_service_instance


# ğŸ†• æ·»åŠ æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
async def test_embedding_service() -> Dict[str, Any]:
    """
    æµ‹è¯•åµŒå…¥æœåŠ¡çš„å¯ç”¨æ€§å’Œæ€§èƒ½
    
    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    test_results = {
        "service_available": False,
        "vector_dimension": 0,
        "api_connectivity": False,
        "batch_processing": False,
        "error_messages": [],
        "performance_metrics": {}
    }
    
    try:
        import time
        start_time = time.time()
        
        # è·å–åµŒå…¥æœåŠ¡å®ä¾‹
        embedding_service = get_embedding_service()
        
        # 1. æ£€æŸ¥åŸºæœ¬å¯ç”¨æ€§
        test_results["service_available"] = not embedding_service.is_mock_mode
        test_results["vector_dimension"] = embedding_service.get_vector_dimension()
        
        # 2. æµ‹è¯•å•ä¸ªæŸ¥è¯¢å‘é‡ç”Ÿæˆ
        single_test_start = time.time()
        test_query = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢æ–‡æœ¬"
        query_embedding = embedding_service.embed_query(test_query)
        single_test_duration = time.time() - single_test_start
        
        test_results["api_connectivity"] = len(query_embedding) == settings.VECTOR_SIZE
        test_results["performance_metrics"]["single_query_time"] = single_test_duration
        
        # 3. æµ‹è¯•æ‰¹é‡æ–‡æ¡£å‘é‡ç”Ÿæˆ
        batch_test_start = time.time()
        test_texts = [
            "ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹",
            "ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹", 
            "ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹",
            "ç¬¬å››ä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹",
            "ç¬¬äº”ä¸ªæµ‹è¯•æ–‡æ¡£å†…å®¹"
        ]
        batch_embeddings = embedding_service.embed_documents(test_texts)
        batch_test_duration = time.time() - batch_test_start
        
        test_results["batch_processing"] = (
            len(batch_embeddings) == len(test_texts) and
            all(len(emb) == settings.VECTOR_SIZE for emb in batch_embeddings)
        )
        
        test_results["performance_metrics"]["batch_processing_time"] = batch_test_duration
        test_results["performance_metrics"]["avg_time_per_text"] = batch_test_duration / len(test_texts)
        
        # 4. æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        cache_test_start = time.time()
        duplicate_embeddings = embedding_service.embed_documents(test_texts)  # é‡å¤è°ƒç”¨
        cache_test_duration = time.time() - cache_test_start
        
        test_results["performance_metrics"]["cache_test_time"] = cache_test_duration
        test_results["cache_statistics"] = embedding_service.get_cache_statistics()
        
        total_test_duration = time.time() - start_time
        test_results["performance_metrics"]["total_test_time"] = total_test_duration
        
        logger.info(f"âœ… åµŒå…¥æœåŠ¡æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_test_duration:.2f}ç§’")
        
    except Exception as e:
        error_msg = f"åµŒå…¥æœåŠ¡æµ‹è¯•å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        test_results["error_messages"].append(error_msg)
    
    return test_results


def validate_embedding_dimensions(embeddings: List[List[float]]) -> Dict[str, Any]:
    """
    éªŒè¯embeddingå‘é‡çš„ç»´åº¦ä¸€è‡´æ€§
    
    Args:
        embeddings: å‘é‡åˆ—è¡¨
        
    Returns:
        éªŒè¯ç»“æœ
    """
    validation_result = {
        "is_valid": True,
        "expected_dimension": settings.VECTOR_SIZE,
        "actual_dimensions": [],
        "inconsistent_vectors": [],
        "summary": {}
    }
    
    if not embeddings:
        validation_result["is_valid"] = False
        validation_result["summary"]["error"] = "è¾“å…¥å‘é‡åˆ—è¡¨ä¸ºç©º"
        return validation_result
    
    for i, embedding in enumerate(embeddings):
        if not isinstance(embedding, list):
            validation_result["is_valid"] = False
            validation_result["inconsistent_vectors"].append({
                "index": i,
                "issue": "ä¸æ˜¯åˆ—è¡¨ç±»å‹",
                "type": str(type(embedding))
            })
            continue
            
        actual_dim = len(embedding)
        validation_result["actual_dimensions"].append(actual_dim)
        
        if actual_dim != settings.VECTOR_SIZE:
            validation_result["is_valid"] = False
            validation_result["inconsistent_vectors"].append({
                "index": i,
                "expected": settings.VECTOR_SIZE,
                "actual": actual_dim,
                "issue": "ç»´åº¦ä¸åŒ¹é…"
            })
    
    # ç”Ÿæˆæ‘˜è¦
    if validation_result["actual_dimensions"]:
        validation_result["summary"] = {
            "total_vectors": len(embeddings),
            "min_dimension": min(validation_result["actual_dimensions"]),
            "max_dimension": max(validation_result["actual_dimensions"]),
            "inconsistent_count": len(validation_result["inconsistent_vectors"]),
            "consistency_rate": 1.0 - (len(validation_result["inconsistent_vectors"]) / len(embeddings))
        }
    
    return validation_result 