# -*- coding: utf-8 -*-
"""
å®ä½“ç»Ÿä¸€æ ¸å¿ƒæœåŠ¡
å®ç°åŸºäºå¤šç»´åº¦ç›¸ä¼¼åº¦çš„æ™ºèƒ½å®ä½“ç»Ÿä¸€ç®—æ³•ï¼Œæ˜¯æ•´ä¸ªå®ä½“ç»Ÿä¸€ç³»ç»Ÿçš„æ ¸å¿ƒ
"""
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from app.core.config import settings
from app.services.entity_similarity_service import (
    get_entity_similarity_calculator, 
    EntitySimilarityMatrix
)
from app.services.entity_merge_service import (
    get_merge_decision_engine,
    get_entity_merger,
    MergeDecision,
    MergedEntity
)
from app.services.embedding_service import get_embedding_service

logger = logging.getLogger(__name__)


@dataclass
class UnificationResult:
    """ç»Ÿä¸€ç»“æœ"""
    unified_entities: List[Any]
    merge_operations: List[Dict[str, Any]]
    statistics: Dict[str, Any]
    processing_time: float
    quality_metrics: Dict[str, Any]


@dataclass
class UnificationConfig:
    """ç»Ÿä¸€é…ç½®"""
    similarity_threshold: float = 0.65
    batch_size: int = 100
    max_matrix_size: int = 10000
    enable_caching: bool = True
    parallel_workers: int = 4
    memory_limit_mb: int = 2048
    # ğŸ†• ç±»å‹åˆ†ç»„é…ç½®
    enable_type_grouping: bool = True
    type_similarity_thresholds: Dict[str, float] = None
    max_entities_per_type_batch: int = 50
    # ğŸš€ LangGraph Agenté…ç½®
    enable_langgraph_agent: bool = True
    agent_prescreening_threshold: float = 0.4
    force_wikipedia_verification: bool = True
    agent_conservative_mode: bool = True
    max_agent_pairs_per_batch: int = 50
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.type_similarity_thresholds is None:
            self.type_similarity_thresholds = {
                'äººç‰©': 0.75,      # äººåç›¸ä¼¼åº¦è¦æ±‚é«˜
                'ç»„ç»‡': 0.70,      # ç»„ç»‡åå¯èƒ½æœ‰ç¼©å†™
                'åœ°ç‚¹': 0.80,      # åœ°åè¦æ±‚ç²¾ç¡®
                'äº§å“': 0.65,      # äº§å“åå˜ä½“è¾ƒå¤š
                'æŠ€æœ¯': 0.60,      # æŠ€æœ¯åè¯å˜ä½“æœ€å¤š
                'äº‹ä»¶': 0.75,      # äº‹ä»¶åè¦æ±‚è¾ƒé«˜
                'æ—¶é—´': 0.85,      # æ—¶é—´è¡¨è¾¾è¦æ±‚ç²¾ç¡®
                'æ•°é‡': 0.90,      # æ•°é‡è¡¨è¾¾è¦æ±‚å¾ˆç²¾ç¡®
                'default': 0.65    # é»˜è®¤é˜ˆå€¼
            }


class EntityUnificationService:
    """
    å®ä½“ç»Ÿä¸€æ ¸å¿ƒæœåŠ¡
    
    æä¾›å®Œæ•´çš„å®ä½“ç»Ÿä¸€æµç¨‹ï¼š
    1. æ‰¹é‡embeddingç”Ÿæˆ
    2. ç›¸ä¼¼åº¦çŸ©é˜µæ„å»º  
    3. èšç±»åˆå¹¶ç®—æ³•
    4. è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–
    """
    
    def __init__(self, config: Optional[UnificationConfig] = None):
        """åˆå§‹åŒ–å®ä½“ç»Ÿä¸€æœåŠ¡"""
        self.config = config or self._load_default_config()
        
        # åˆå§‹åŒ–ä¼ ç»Ÿç»„ä»¶
        self.embedding_service = get_embedding_service()
        self.similarity_calculator = get_entity_similarity_calculator()
        self.similarity_matrix_builder = EntitySimilarityMatrix()
        self.merge_decision_engine = get_merge_decision_engine()
        self.entity_merger = get_entity_merger()
        
    # ğŸš€ åˆå§‹åŒ–LangGraph Agentï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.langgraph_agent = None
        self.autonomous_agent_integration = None
        
        # ä¼˜å…ˆä½¿ç”¨è‡ªä¸»Agent
        if self.config.enable_langgraph_agent:
            try:
                # å…ˆå°è¯•è‡ªä¸»Agent
                from app.services.autonomous_agent_integration import get_autonomous_agent_integration
                self.autonomous_agent_integration = get_autonomous_agent_integration(self.config)
                logger.info("è‡ªä¸»å®ä½“å»é‡Agentå·²å¯ç”¨")
                
            except Exception as e:
                logger.warning(f"è‡ªä¸»Agentåˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ä¼ ç»ŸLangGraph: {str(e)}")
                try:
                    from app.services.langgraph_entity_agent import get_langgraph_entity_deduplication_agent
                    agent_config = {
                        "prescreening_threshold": self.config.agent_prescreening_threshold,
                        "force_wikipedia_verification": self.config.force_wikipedia_verification,
                        "conservative_mode": self.config.agent_conservative_mode,
                        "max_pairs_per_batch": self.config.max_agent_pairs_per_batch,
                        "enable_vector_prescreening": True,  # å¯ç”¨å‘é‡é¢„ç­›é€‰
                        "max_retries": 2  # æœ€å¤§é‡è¯•æ¬¡æ•°
                    }
                    self.langgraph_agent = get_langgraph_entity_deduplication_agent(agent_config)
                    logger.info("ä¼ ç»ŸLangGraph Agentå·²å¯ç”¨")
                except Exception as e2:
                    logger.warning(f"ä¼ ç»ŸLangGraph Agentåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e2)}")
                    self.config.enable_langgraph_agent = False
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            "total_entities_processed": 0,
            "total_merge_operations": 0,
            "total_processing_time": 0.0,
            "cache_hit_rate": 0.0
        }
        
        strategy = "è‡ªä¸»Agent" if self.autonomous_agent_integration else \
                  ("ä¼ ç»ŸLangGraph Agent" if self.langgraph_agent else "ä¼ ç»Ÿå‘é‡ç›¸ä¼¼åº¦")
        logger.info(f"å®ä½“ç»Ÿä¸€æœåŠ¡å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ç­–ç•¥: {strategy}")
    
    async def unify_entities(self, entities: List[Any]) -> UnificationResult:
        """
        æ‰§è¡Œå®Œæ•´çš„å®ä½“ç»Ÿä¸€æµç¨‹
        
        Args:
            entities: å¾…ç»Ÿä¸€çš„å®ä½“åˆ—è¡¨
            
        Returns:
            UnificationResult: ç»Ÿä¸€ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"å¼€å§‹å®ä½“ç»Ÿä¸€æµç¨‹ï¼Œè¾“å…¥å®ä½“æ•°é‡: {len(entities)}")
        
        try:
            # é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            validated_entities = await self._preprocess_entities(entities)
            logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆå®ä½“: {len(validated_entities)}")
            
            # ğŸ†• é€‰æ‹©ç»Ÿä¸€ç­–ç•¥ï¼šç±»å‹åˆ†ç»„ vs ä¼ ç»Ÿå…¨é‡
            if self.config.enable_type_grouping:
                result = await self._unify_entities_by_type_grouping(validated_entities, start_time)
            else:
                result = await self._unify_entities_traditional(validated_entities, start_time)
            
            logger.info(f"å®ä½“ç»Ÿä¸€å®Œæˆ: {len(entities)} -> {len(result.unified_entities)} "
                       f"(å‡å°‘ {len(entities) - len(result.unified_entities)} ä¸ªé‡å¤), "
                       f"è€—æ—¶: {result.processing_time:.3f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"å®ä½“ç»Ÿä¸€å¤±è´¥: {str(e)}")
            raise
    
    async def _unify_entities_by_type_grouping(self, entities: List[Any], start_time: float) -> UnificationResult:
        """
        ğŸ†• åŸºäºç±»å‹åˆ†ç»„çš„å®ä½“ç»Ÿä¸€ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            entities: é¢„å¤„ç†åçš„å®ä½“åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            UnificationResult: ç»Ÿä¸€ç»“æœ
        """
        logger.info(f"ä½¿ç”¨ç±»å‹åˆ†ç»„ç­–ç•¥è¿›è¡Œå®ä½“ç»Ÿä¸€ï¼Œå…± {len(entities)} ä¸ªå®ä½“")
        
        # æ­¥éª¤1: æŒ‰ç±»å‹åˆ†ç»„
        entities_by_type = self._group_entities_by_type(entities)
        logger.info(f"å®ä½“ç±»å‹åˆ†ç»„å®Œæˆ: {[(type_name, len(entities_list)) for type_name, entities_list in entities_by_type.items()]}")
        
        # æ­¥éª¤2: ä¸ºæ‰€æœ‰å®ä½“æ‰¹é‡ç”Ÿæˆembeddingï¼ˆè·¨ç±»å‹ä¼˜åŒ–ï¼‰
        all_embedded_entities = await self._generate_embeddings_for_entities(entities)
        logger.info(f"è·¨ç±»å‹æ‰¹é‡embeddingç”Ÿæˆå®Œæˆ: {len(all_embedded_entities)} ä¸ªå®ä½“")
        
        # é‡æ–°æŒ‰ç±»å‹åˆ†ç»„embedded entities
        embedded_entities_by_type = self._group_entities_by_type(all_embedded_entities)
        
        # æ­¥éª¤3: å¹¶è¡Œå¤„ç†æ¯ä¸ªç±»å‹çš„å®ä½“ç»Ÿä¸€
        unified_entities = []
        all_merge_operations = []
        type_processing_stats = {}
        
        for entity_type, type_entities in embedded_entities_by_type.items():
            if len(type_entities) < 2:
                # å•ä¸ªå®ä½“ï¼Œæ— éœ€åˆå¹¶
                unified_entities.extend(type_entities)
                type_processing_stats[entity_type] = {
                    "input_count": len(type_entities),
                    "output_count": len(type_entities),
                    "merge_count": 0,
                    "processing_time": 0.0
                }
                continue
            
            logger.info(f"å¤„ç†ç±»å‹ '{entity_type}' çš„ {len(type_entities)} ä¸ªå®ä½“")
            type_start_time = time.time()
            
            try:
                # è·å–ç±»å‹ç‰¹å®šçš„ç›¸ä¼¼åº¦é˜ˆå€¼
                type_threshold = self.config.type_similarity_thresholds.get(
                    entity_type, 
                    self.config.type_similarity_thresholds.get('default', 0.65)
                )
                
                # åˆ†æ‰¹å¤„ç†ï¼ˆå¦‚æœå®ä½“è¿‡å¤šï¼‰
                type_unified_entities, type_merge_operations = await self._process_type_entities_in_batches(
                    type_entities, entity_type, type_threshold
                )
                
                unified_entities.extend(type_unified_entities)
                all_merge_operations.extend(type_merge_operations)
                
                type_processing_time = time.time() - type_start_time
                type_processing_stats[entity_type] = {
                    "input_count": len(type_entities),
                    "output_count": len(type_unified_entities),
                    "merge_count": len(type_merge_operations),
                    "processing_time": type_processing_time,
                    "similarity_threshold": type_threshold,
                    "reduction_rate": (len(type_entities) - len(type_unified_entities)) / len(type_entities) if type_entities else 0
                }
                
                logger.info(f"ç±»å‹ '{entity_type}' å¤„ç†å®Œæˆ: {len(type_entities)} -> {len(type_unified_entities)} "
                           f"(å‡å°‘ {len(type_entities) - len(type_unified_entities)} ä¸ª), "
                           f"ç”¨æ—¶: {type_processing_time:.3f}ç§’")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç±»å‹ '{entity_type}' å¤±è´¥: {str(e)}")
                # é”™è¯¯å¤„ç†ï¼šè·³è¿‡ç»Ÿä¸€ï¼Œä¿ç•™åŸå§‹å®ä½“
                unified_entities.extend(type_entities)
                type_processing_stats[entity_type] = {
                    "input_count": len(type_entities),
                    "output_count": len(type_entities),
                    "merge_count": 0,
                    "processing_time": 0.0,
                    "error": str(e)
                }
        
        # æ­¥éª¤4: è´¨é‡è¯„ä¼°
        quality_metrics = self._evaluate_unification_quality(
            entities, unified_entities, all_merge_operations
        )
        quality_metrics["type_processing_stats"] = type_processing_stats
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_performance_stats(len(entities), len(all_merge_operations), processing_time)
        
        return UnificationResult(
            unified_entities=unified_entities,
            merge_operations=all_merge_operations,
            statistics={
                "input_entity_count": len(entities),
                "output_entity_count": len(unified_entities),
                "merge_operation_count": len(all_merge_operations),
                "reduction_rate": (len(entities) - len(unified_entities)) / len(entities) if entities else 0,
                "processing_strategy": "type_grouping",
                "entity_types_processed": len(entities_by_type),
                "type_processing_stats": type_processing_stats,
                "processing_stages": {
                    "preprocessing": "completed",
                    "type_grouping": "completed",
                    "batch_embedding_generation": "completed",
                    "type_parallel_processing": "completed",
                    "quality_evaluation": "completed"
                }
            },
            processing_time=processing_time,
            quality_metrics=quality_metrics
        )
    
    async def _unify_entities_traditional(self, entities: List[Any], start_time: float) -> UnificationResult:
        """
        ä¼ ç»Ÿçš„å®ä½“ç»Ÿä¸€æµç¨‹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        
        Args:
            entities: é¢„å¤„ç†åçš„å®ä½“åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            UnificationResult: ç»Ÿä¸€ç»“æœ
        """
        logger.info(f"ä½¿ç”¨ä¼ ç»Ÿç­–ç•¥è¿›è¡Œå®ä½“ç»Ÿä¸€ï¼Œå…± {len(entities)} ä¸ªå®ä½“")
        
        # é˜¶æ®µ2: æ‰¹é‡embeddingç”Ÿæˆ
        embedded_entities = await self._generate_embeddings_for_entities(entities)
        logger.info(f"embeddingç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸå¤„ç†: {len(embedded_entities)}")
        
        # é˜¶æ®µ3: æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix_result = await self._build_similarity_matrix(embedded_entities)
        logger.info(f"ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå®Œæˆï¼Œæœ‰æ•ˆç›¸ä¼¼åº¦å¯¹: {similarity_matrix_result['metadata']['valid_pairs']}")
        
        # é˜¶æ®µ4: èšç±»å’Œåˆå¹¶
        unified_entities, merge_operations = await self._cluster_and_merge_entities(
            embedded_entities, similarity_matrix_result
        )
        logger.info(f"èšç±»åˆå¹¶å®Œæˆï¼Œç»Ÿä¸€åå®ä½“æ•°é‡: {len(unified_entities)}, åˆå¹¶æ“ä½œ: {len(merge_operations)}")
        
        # é˜¶æ®µ5: è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–
        quality_metrics = self._evaluate_unification_quality(
            entities, unified_entities, merge_operations
        )
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_performance_stats(len(entities), len(merge_operations), processing_time)
        
        return UnificationResult(
            unified_entities=unified_entities,
            merge_operations=merge_operations,
            statistics={
                "input_entity_count": len(entities),
                "output_entity_count": len(unified_entities),
                "merge_operation_count": len(merge_operations),
                "reduction_rate": (len(entities) - len(unified_entities)) / len(entities) if entities else 0,
                "processing_strategy": "traditional",
                "processing_stages": {
                    "preprocessing": "completed",
                    "embedding_generation": "completed", 
                    "similarity_matrix": "completed",
                    "clustering_merging": "completed",
                    "quality_evaluation": "completed"
                }
            },
            processing_time=processing_time,
            quality_metrics=quality_metrics
        )
    
    async def _preprocess_entities(self, entities: List[Any]) -> List[Any]:
        """
        é¢„å¤„ç†å®ä½“æ•°æ®
        
        Args:
            entities: åŸå§‹å®ä½“åˆ—è¡¨
            
        Returns:
            éªŒè¯å’Œæ¸…ç†åçš„å®ä½“åˆ—è¡¨
        """
        valid_entities = []
        
        for entity in entities:
            try:
                # åŸºæœ¬éªŒè¯
                if not hasattr(entity, 'name') or not entity.name:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“ï¼šç¼ºå°‘åç§°")
                    continue
                
                if not hasattr(entity, 'type') or not entity.type:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“ï¼š{entity.name} ç¼ºå°‘ç±»å‹")
                    continue
                
                # è´¨é‡è¿‡æ»¤
                quality_score = getattr(entity, 'quality_score', 1.0)
                if quality_score < settings.ENTITY_QUALITY_MIN_SCORE:
                    logger.debug(f"è·³è¿‡ä½è´¨é‡å®ä½“ï¼š{entity.name} (è´¨é‡åˆ†æ•°: {quality_score})")
                    continue
                
                # åç§°é•¿åº¦æ£€æŸ¥
                if len(entity.name.strip()) < 2:
                    logger.debug(f"è·³è¿‡åç§°è¿‡çŸ­çš„å®ä½“ï¼š{entity.name}")
                    continue
                
                valid_entities.append(entity)
                
            except Exception as e:
                logger.warning(f"é¢„å¤„ç†å®ä½“æ—¶å‡ºé”™: {str(e)}")
                continue
        
        logger.debug(f"é¢„å¤„ç†å®Œæˆ: {len(entities)} -> {len(valid_entities)}")
        return valid_entities
    
    async def _generate_embeddings_for_entities(self, entities: List[Any]) -> List[Any]:
        """
        ä¸ºå®ä½“æ‰¹é‡ç”Ÿæˆembeddingå‘é‡
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            åŒ…å«embeddingçš„å®ä½“åˆ—è¡¨
        """
        # åˆ†ç¦»å·²æœ‰embeddingå’Œéœ€è¦ç”Ÿæˆembeddingçš„å®ä½“
        entities_with_embedding = []
        entities_need_embedding = []
        texts_to_embed = []
        
        for entity in entities:
            if hasattr(entity, 'embedding') and entity.embedding is not None:
                entities_with_embedding.append(entity)
            else:
                entities_need_embedding.append(entity)
                # ç”Ÿæˆå®ä½“çš„æ–‡æœ¬è¡¨ç¤º
                text_repr = self._get_entity_text_representation(entity)
                texts_to_embed.append(text_repr)
        
        logger.debug(f"EmbeddingçŠ¶æ€: å·²æœ‰ {len(entities_with_embedding)}, éœ€ç”Ÿæˆ {len(entities_need_embedding)}")
        
        # æ‰¹é‡ç”Ÿæˆembedding
        if texts_to_embed:
            try:
                embeddings = await self.embedding_service.embed_documents_batch(
                    texts_to_embed,
                    batch_size=self.config.batch_size,
                    use_cache=self.config.enable_caching
                )
                
                # å°†ç”Ÿæˆçš„embeddingåˆ†é…ç»™å¯¹åº”çš„å®ä½“
                for i, entity in enumerate(entities_need_embedding):
                    if i < len(embeddings):
                        entity.embedding = embeddings[i]
                        entities_with_embedding.append(entity)
                    else:
                        logger.warning(f"å®ä½“ {entity.name} æœªèƒ½è·å¾—embedding")
                        
            except Exception as e:
                logger.error(f"æ‰¹é‡ç”Ÿæˆembeddingå¤±è´¥: {str(e)}")
                # é™çº§å¤„ç†ï¼šè¿”å›åŸå§‹å®ä½“åˆ—è¡¨
                return entities
        
        return entities_with_embedding
    
    async def _build_similarity_matrix(self, entities: List[Any]) -> Dict[str, Any]:
        """
        æ„å»ºå®ä½“ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µç»“æœ
        """
        # æ£€æŸ¥å®ä½“æ•°é‡é™åˆ¶
        if len(entities) * len(entities) > self.config.max_matrix_size:
            logger.warning(f"å®ä½“æ•°é‡è¿‡å¤§ ({len(entities)}^2 > {self.config.max_matrix_size})ï¼Œä½¿ç”¨åˆ†å—ç­–ç•¥")
        
        matrix_result = await self.similarity_matrix_builder.build_similarity_matrix(
            entities,
            threshold=self.config.similarity_threshold,
            max_matrix_size=self.config.max_matrix_size
        )
        
        return matrix_result
    
    async def _cluster_and_merge_entities(self, entities: List[Any], 
                                        similarity_matrix_result: Dict[str, Any]) -> Tuple[List[Any], List[Dict[str, Any]]]:
        """
        åŸºäºç›¸ä¼¼åº¦çŸ©é˜µè¿›è¡Œèšç±»å’Œåˆå¹¶
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            similarity_matrix_result: ç›¸ä¼¼åº¦çŸ©é˜µç»“æœ
            
        Returns:
            (ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨, åˆå¹¶æ“ä½œè®°å½•)
        """
        similarity_matrix = similarity_matrix_result["matrix"]
        
        # 1. æ„å»ºå®ä½“IDåˆ°å®ä½“å¯¹è±¡çš„æ˜ å°„
        entity_map = {entity.id: entity for entity in entities}
        
        # 2. ä½¿ç”¨è¿é€šå›¾ç®—æ³•æ‰¾åˆ°éœ€è¦åˆå¹¶çš„å®ä½“ç»„
        entity_clusters = self._find_entity_clusters(similarity_matrix, self.config.similarity_threshold)
        
        logger.debug(f"å‘ç° {len(entity_clusters)} ä¸ªå®ä½“èšç±»")
        
        # 3. æ‰§è¡Œåˆå¹¶æ“ä½œ
        unified_entities = []
        merge_operations = []
        processed_entity_ids = set()
        
        for cluster in entity_clusters:
            if len(cluster) == 1:
                # å•ä¸ªå®ä½“ï¼Œæ— éœ€åˆå¹¶
                entity_id = cluster[0]
                if entity_id in entity_map and entity_id not in processed_entity_ids:
                    unified_entities.append(entity_map[entity_id])
                    processed_entity_ids.add(entity_id)
            else:
                # å¤šä¸ªå®ä½“éœ€è¦åˆå¹¶
                cluster_entities = [entity_map[eid] for eid in cluster if eid in entity_map]
                
                if len(cluster_entities) >= 2:
                    merged_entity, merge_ops = await self._merge_entity_cluster(cluster_entities)
                    unified_entities.append(merged_entity)
                    merge_operations.extend(merge_ops)
                    processed_entity_ids.update(cluster)
                elif len(cluster_entities) == 1:
                    # èšç±»ä¸­åªæœ‰ä¸€ä¸ªæœ‰æ•ˆå®ä½“
                    unified_entities.append(cluster_entities[0])
                    processed_entity_ids.update(cluster)
        
        # 4. æ·»åŠ æœªå¤„ç†çš„å•ç‹¬å®ä½“
        for entity in entities:
            if entity.id not in processed_entity_ids:
                unified_entities.append(entity)
        
        return unified_entities, merge_operations
    
    def _find_entity_clusters(self, similarity_matrix: Dict[str, Dict[str, float]], 
                            threshold: float) -> List[List[str]]:
        """
        ä½¿ç”¨è¿é€šå›¾ç®—æ³•æ‰¾åˆ°å®ä½“èšç±»
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            å®ä½“èšç±»åˆ—è¡¨ï¼Œæ¯ä¸ªèšç±»æ˜¯å®ä½“IDçš„åˆ—è¡¨
        """
        # æ„å»ºå›¾çš„è¾¹
        edges = []
        for entity1_id, similarities in similarity_matrix.items():
            for entity2_id, similarity in similarities.items():
                if similarity >= threshold and entity1_id != entity2_id:
                    edges.append((entity1_id, entity2_id, similarity))
        
        # ä½¿ç”¨å¹¶æŸ¥é›†ç®—æ³•æ‰¾è¿é€šåˆ†é‡
        clusters = self._union_find_clustering(edges)
        
        # è¿‡æ»¤æ‰ç©ºèšç±»
        clusters = [cluster for cluster in clusters if cluster]
        
        return clusters
    
    def _union_find_clustering(self, edges: List[Tuple[str, str, float]]) -> List[List[str]]:
        """
        ä½¿ç”¨å¹¶æŸ¥é›†ç®—æ³•è¿›è¡Œèšç±»
        
        Args:
            edges: è¾¹åˆ—è¡¨ [(entity1_id, entity2_id, similarity), ...]
            
        Returns:
            èšç±»åˆ—è¡¨
        """
        # åˆå§‹åŒ–å¹¶æŸ¥é›†
        parent = {}
        rank = {}
        
        def find(x):
            if x not in parent:
                parent[x] = x
                rank[x] = 0
                return x
            if parent[x] != x:
                parent[x] = find(parent[x])  # è·¯å¾„å‹ç¼©
            return parent[x]
        
        def union(x, y):
            px, py = find(x), find(y)
            if px == py:
                return
            # æŒ‰ç§©åˆå¹¶
            if rank[px] < rank[py]:
                px, py = py, px
            parent[py] = px
            if rank[px] == rank[py]:
                rank[px] += 1
        
        # å¤„ç†æ‰€æœ‰è¾¹
        for entity1_id, entity2_id, similarity in edges:
            union(entity1_id, entity2_id)
        
        # æ”¶é›†èšç±»
        clusters_map = {}
        for entity_id in parent:
            root = find(entity_id)
            if root not in clusters_map:
                clusters_map[root] = []
            clusters_map[root].append(entity_id)
        
        return list(clusters_map.values())
    
    async def _merge_entity_cluster(self, cluster_entities: List[Any]) -> Tuple[Any, List[Dict[str, Any]]]:
        """
        åˆå¹¶ä¸€ä¸ªå®ä½“èšç±»
        
        Args:
            cluster_entities: èšç±»ä¸­çš„å®ä½“åˆ—è¡¨
            
        Returns:
            (åˆå¹¶åçš„å®ä½“, åˆå¹¶æ“ä½œè®°å½•åˆ—è¡¨)
        """
        if len(cluster_entities) == 1:
            return cluster_entities[0], []
        
        merge_operations = []
        
        # é€‰æ‹©æœ€ä½³çš„ä¸»å®ä½“ï¼ˆç½®ä¿¡åº¦æœ€é«˜çš„ï¼‰
        primary_entity = max(cluster_entities, key=lambda e: e.confidence)
        remaining_entities = [e for e in cluster_entities if e.id != primary_entity.id]
        
        # é€ä¸ªåˆå¹¶å…¶ä»–å®ä½“åˆ°ä¸»å®ä½“
        merged_entity = primary_entity
        
        for secondary_entity in remaining_entities:
            try:
                # è·å–åˆå¹¶å†³ç­–
                merge_decision = await self.merge_decision_engine.should_merge(
                    merged_entity, secondary_entity
                )
                
                if merge_decision.decision in [MergeDecision.AUTO_MERGE, MergeDecision.CONDITIONAL_MERGE]:
                    # æ‰§è¡Œåˆå¹¶
                    new_merged_entity = self.entity_merger.merge_entities(
                        merged_entity, secondary_entity, merge_decision
                    )
                    
                    # è®°å½•åˆå¹¶æ“ä½œ
                    merge_operations.append({
                        "operation_type": "merge",
                        "primary_entity_id": merged_entity.id,
                        "secondary_entity_id": secondary_entity.id,
                        "merged_entity_id": new_merged_entity.id,
                        "decision": merge_decision.decision.value,
                        "similarity_score": merge_decision.similarity_result.total_similarity,
                        "confidence": merge_decision.confidence,
                        "conflicts": [
                            {
                                "type": conflict.conflict_type,
                                "severity": conflict.severity,
                                "description": conflict.description
                            }
                            for conflict in merge_decision.conflicts
                        ]
                    })
                    
                    merged_entity = new_merged_entity
                else:
                    # æ‹’ç»åˆå¹¶ï¼Œè®°å½•åŸå› 
                    merge_operations.append({
                        "operation_type": "reject_merge",
                        "entity1_id": merged_entity.id,
                        "entity2_id": secondary_entity.id,
                        "reason": merge_decision.reasoning,
                        "decision": merge_decision.decision.value
                    })
                    
            except Exception as e:
                logger.error(f"åˆå¹¶å®ä½“å¤±è´¥: {merged_entity.name} + {secondary_entity.name}, é”™è¯¯: {str(e)}")
                merge_operations.append({
                    "operation_type": "merge_error",
                    "entity1_id": merged_entity.id,
                    "entity2_id": secondary_entity.id,
                    "error": str(e)
                })
                continue
        
        return merged_entity, merge_operations
    
    def _evaluate_unification_quality(self, original_entities: List[Any], 
                                    unified_entities: List[Any], 
                                    merge_operations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è¯„ä¼°ç»Ÿä¸€è´¨é‡
        
        Args:
            original_entities: åŸå§‹å®ä½“åˆ—è¡¨
            unified_entities: ç»Ÿä¸€åå®ä½“åˆ—è¡¨
            merge_operations: åˆå¹¶æ“ä½œè®°å½•
            
        Returns:
            è´¨é‡æŒ‡æ ‡å­—å…¸
        """
        # åŸºç¡€ç»Ÿè®¡
        original_count = len(original_entities)
        unified_count = len(unified_entities)
        merge_count = len([op for op in merge_operations if op.get("operation_type") == "merge"])
        
        # è®¡ç®—å‡å°‘ç‡
        reduction_rate = (original_count - unified_count) / original_count if original_count > 0 else 0
        
        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        avg_original_quality = sum(getattr(e, 'quality_score', 1.0) for e in original_entities) / original_count if original_count > 0 else 0
        avg_unified_quality = sum(getattr(e, 'quality_score', 1.0) for e in unified_entities) / unified_count if unified_count > 0 else 0
        
        # è®¡ç®—åˆå¹¶æ“ä½œçš„å¹³å‡ç½®ä¿¡åº¦
        merge_confidences = [op.get("confidence", 0) for op in merge_operations if op.get("operation_type") == "merge"]
        avg_merge_confidence = sum(merge_confidences) / len(merge_confidences) if merge_confidences else 0
        
        # è®¡ç®—å†²çªç»Ÿè®¡
        total_conflicts = sum(len(op.get("conflicts", [])) for op in merge_operations if op.get("operation_type") == "merge")
        avg_conflicts_per_merge = total_conflicts / merge_count if merge_count > 0 else 0
        
        return {
            "reduction_rate": reduction_rate,
            "merge_efficiency": merge_count / original_count if original_count > 0 else 0,
            "quality_improvement": avg_unified_quality - avg_original_quality,
            "avg_merge_confidence": avg_merge_confidence,
            "avg_conflicts_per_merge": avg_conflicts_per_merge,
            "total_conflicts": total_conflicts,
            "rejected_merges": len([op for op in merge_operations if op.get("operation_type") == "reject_merge"]),
            "merge_errors": len([op for op in merge_operations if op.get("operation_type") == "merge_error"])
        }
    
    def _get_entity_text_representation(self, entity) -> str:
        """è·å–å®ä½“çš„æ–‡æœ¬è¡¨ç¤ºï¼Œç”¨äºç”Ÿæˆembedding"""
        parts = [entity.name]
        
        if hasattr(entity, 'type') and entity.type:
            parts.append(f"ç±»å‹:{entity.type}")
        
        if hasattr(entity, 'description') and entity.description:
            parts.append(f"æè¿°:{entity.description}")
        
        return " ".join(parts)
    
    def _load_default_config(self) -> UnificationConfig:
        """åŠ è½½é»˜è®¤é…ç½®"""
        return UnificationConfig(
            similarity_threshold=settings.ENTITY_UNIFICATION_MEDIUM_THRESHOLD,
            batch_size=settings.ENTITY_UNIFICATION_BATCH_SIZE,
            max_matrix_size=settings.ENTITY_UNIFICATION_MAX_MATRIX_SIZE,
            enable_caching=True,
            parallel_workers=settings.ENTITY_UNIFICATION_PARALLEL_WORKERS,
            memory_limit_mb=settings.ENTITY_UNIFICATION_MEMORY_LIMIT_MB,
            # ğŸ†• ç±»å‹åˆ†ç»„é…ç½®
            enable_type_grouping=getattr(settings, 'ENTITY_UNIFICATION_ENABLE_TYPE_GROUPING', True),
            max_entities_per_type_batch=getattr(settings, 'ENTITY_UNIFICATION_MAX_ENTITIES_PER_TYPE_BATCH', 50)
        )
    
    def _group_entities_by_type(self, entities: List[Any]) -> Dict[str, List[Any]]:
        """
        ğŸ†• æŒ‰ç±»å‹åˆ†ç»„å®ä½“
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            æŒ‰ç±»å‹åˆ†ç»„çš„å®ä½“å­—å…¸
        """
        entities_by_type = {}
        
        for entity in entities:
            entity_type = getattr(entity, 'type', 'unknown')
            if entity_type not in entities_by_type:
                entities_by_type[entity_type] = []
            entities_by_type[entity_type].append(entity)
        
        return entities_by_type
    
    async def _process_type_entities_in_batches(self, type_entities: List[Any], entity_type: str, threshold: float) -> Tuple[List[Any], List[Dict[str, Any]]]:
        """
        ğŸ†• åˆ†æ‰¹å¤„ç†åŒç±»å‹å®ä½“
        
        Args:
            type_entities: åŒç±»å‹å®ä½“åˆ—è¡¨
            entity_type: å®ä½“ç±»å‹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            (ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨, åˆå¹¶æ“ä½œè®°å½•)
        """
        max_batch_size = self.config.max_entities_per_type_batch
        
        if len(type_entities) <= max_batch_size:
            # å•æ‰¹å¤„ç†
            return await self._process_single_type_batch(type_entities, entity_type, threshold)
        else:
            # å¤šæ‰¹å¤„ç†
            logger.info(f"ç±»å‹ '{entity_type}' å®ä½“è¿‡å¤š ({len(type_entities)}), åˆ†æ‰¹å¤„ç† (æ‰¹å¤§å°: {max_batch_size})")
            
            unified_entities = []
            all_merge_operations = []
            
            for i in range(0, len(type_entities), max_batch_size):
                batch_entities = type_entities[i:i + max_batch_size]
                batch_unified, batch_operations = await self._process_single_type_batch(
                    batch_entities, entity_type, threshold
                )
                unified_entities.extend(batch_unified)
                all_merge_operations.extend(batch_operations)
                
                logger.debug(f"æ‰¹æ¬¡ {i//max_batch_size + 1} å®Œæˆ: {len(batch_entities)} -> {len(batch_unified)}")
            
            return unified_entities, all_merge_operations
    
    async def _process_single_type_batch(self, entities: List[Any], entity_type: str, threshold: float) -> Tuple[List[Any], List[Dict[str, Any]]]:
        """
        ğŸ†• å¤„ç†å•ä¸ªç±»å‹æ‰¹æ¬¡çš„å®ä½“ç»Ÿä¸€
        
        Args:
            entities: åŒç±»å‹å®ä½“åˆ—è¡¨
            entity_type: å®ä½“ç±»å‹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            (ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨, åˆå¹¶æ“ä½œè®°å½•)
        """
        if len(entities) < 2:
            return entities, []
        
        # ğŸš€ ä¼˜å…ˆä½¿ç”¨è‡ªä¸»Agentï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.autonomous_agent_integration:
            try:
                logger.info(f"ä½¿ç”¨è‡ªä¸»Agentå¤„ç† {entity_type} ç±»å‹çš„ {len(entities)} ä¸ªå®ä½“")
                
                # ç›´æ¥è°ƒç”¨è‡ªä¸»Agenté›†æˆå™¨ï¼Œè¿”å›UnificationResult
                unification_result = await self.autonomous_agent_integration.unify_entities_with_autonomous_agent(entities)
                
                # è½¬æ¢ä¸ºå…ƒç»„æ ¼å¼ä»¥å…¼å®¹ç°æœ‰æ¥å£
                return unification_result.unified_entities, unification_result.merge_operations
                
            except Exception as e:
                logger.error(f"è‡ªä¸»Agentå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
                # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¤‡é€‰
        
        # ğŸš€ ä½¿ç”¨ä¼ ç»ŸLangGraph Agentï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.langgraph_agent:
            try:
                logger.info(f"ä½¿ç”¨ä¼ ç»ŸLangGraph Agentå¤„ç† {entity_type} ç±»å‹çš„ {len(entities)} ä¸ªå®ä½“")
                
                # è½¬æ¢å®ä½“æ ¼å¼ä¸ºAgentå…¼å®¹æ ¼å¼
                agent_entities = self._convert_entities_for_agent(entities)
                
                # è°ƒç”¨ä¼ ç»ŸLangGraph Agent
                agent_result = await self.langgraph_agent.deduplicate_entities(agent_entities, entity_type)
                
                # è½¬æ¢Agentç»“æœä¸ºç»Ÿä¸€æ ¼å¼
                unified_entities, merge_operations = self._convert_agent_result_to_unification_format(
                    agent_result, entities
                )
                
                logger.info(f"ä¼ ç»ŸLangGraph Agentå¤„ç†å®Œæˆ: {len(entities)} -> {len(unified_entities)} ä¸ªå®ä½“")
                return unified_entities, merge_operations
                
            except Exception as e:
                logger.error(f"ä¼ ç»ŸLangGraph Agentå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
                # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¤‡é€‰
        
        # ä¼ ç»Ÿæ–¹æ³•ï¼ˆå‘é‡ç›¸ä¼¼åº¦çŸ©é˜µï¼‰
        logger.info(f"ä½¿ç”¨ä¼ ç»Ÿå‘é‡ç›¸ä¼¼åº¦æ–¹æ³•å¤„ç† {entity_type} ç±»å‹çš„ {len(entities)} ä¸ªå®ä½“")
        
        # ä½¿ç”¨ç±»å‹ç‰¹å®šçš„é…ç½®æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix_result = await self._build_similarity_matrix_with_threshold(entities, threshold)
        
        # èšç±»å’Œåˆå¹¶
        unified_entities, merge_operations = await self._cluster_and_merge_entities(
            entities, similarity_matrix_result
        )
        
        return unified_entities, merge_operations
    
    def _convert_entities_for_agent(self, entities: List[Any]) -> List[Dict[str, Any]]:
        """
        ğŸš€ è½¬æ¢å®ä½“æ ¼å¼ä¸ºAgentå…¼å®¹æ ¼å¼
        
        Args:
            entities: å®ä½“å¯¹è±¡åˆ—è¡¨
            
        Returns:
            Agentå…¼å®¹çš„å­—å…¸æ ¼å¼å®ä½“åˆ—è¡¨
        """
        agent_entities = []
        
        for entity in entities:
            agent_entity = {
                "name": getattr(entity, 'name', str(entity)),
                "type": getattr(entity, 'type', 'unknown'),
                "description": getattr(entity, 'description', ''),
                "properties": getattr(entity, 'properties', {}),
                "confidence": getattr(entity, 'confidence', 1.0),
                "quality_score": getattr(entity, 'quality_score', 1.0),
                "source_text": getattr(entity, 'source_text', ''),
                "id": getattr(entity, 'id', f"entity_{len(agent_entities)}")
            }
            
            # ä¿ç•™embeddingå¦‚æœå­˜åœ¨
            if hasattr(entity, 'embedding'):
                agent_entity['embedding'] = entity.embedding
            
            agent_entities.append(agent_entity)
        
        return agent_entities
    
    def _convert_agent_result_to_unification_format(self, agent_result: Dict[str, Any], 
                                                   original_entities: List[Any]) -> Tuple[List[Any], List[Dict[str, Any]]]:
        """
        ğŸš€ è½¬æ¢Agentç»“æœä¸ºç»Ÿä¸€æ ¼å¼
        
        Args:
            agent_result: Agentè¿”å›çš„ç»“æœ
            original_entities: åŸå§‹å®ä½“åˆ—è¡¨
            
        Returns:
            (ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨, åˆå¹¶æ“ä½œè®°å½•)
        """
        unified_entities = []
        merge_operations = []
        
        # å¤„ç†åˆå¹¶ç»„
        merge_groups = agent_result.get("merge_groups", [])
        processed_entity_indices = set()
        
        for group in merge_groups:
            # è·å–ä¸»å®ä½“ç´¢å¼•ï¼ˆAgentè¿”å›çš„æ˜¯1å¼€å§‹çš„ç´¢å¼•ï¼Œéœ€è¦è½¬æ¢ä¸º0å¼€å§‹ï¼‰
            primary_index = int(group.get("primary_entity", "1")) - 1
            duplicate_indices = [int(idx) - 1 for idx in group.get("duplicates", [])]
            
            # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
            if 0 <= primary_index < len(original_entities):
                primary_entity = original_entities[primary_index]
                
                # æ›´æ–°ä¸»å®ä½“ä¿¡æ¯
                if group.get("merged_name"):
                    primary_entity.name = group["merged_name"]
                if group.get("merged_description"):
                    primary_entity.description = group["merged_description"]
                
                # åˆå¹¶å±æ€§ä¿¡æ¯
                for dup_idx in duplicate_indices:
                    if 0 <= dup_idx < len(original_entities):
                        dup_entity = original_entities[dup_idx]
                        
                        # åˆå¹¶å±æ€§
                        if hasattr(primary_entity, 'properties') and hasattr(dup_entity, 'properties'):
                            primary_entity.properties.update(dup_entity.properties)
                        
                        # å¢åŠ å‡ºç°æ¬¡æ•°
                        chunk_ids = primary_entity.properties.get('chunk_ids', [])
                        dup_chunk_ids = dup_entity.properties.get('chunk_ids', [])
                        primary_entity.properties['chunk_ids'] = list(set(chunk_ids + dup_chunk_ids))
                        
                        processed_entity_indices.add(dup_idx)
                
                unified_entities.append(primary_entity)
                processed_entity_indices.add(primary_index)
                
                # è®°å½•åˆå¹¶æ“ä½œ
                merge_operations.append({
                    "operation_type": "agent_merge",
                    "primary_entity_id": getattr(primary_entity, 'id', f"entity_{primary_index}"),
                    "secondary_entity_ids": [
                        getattr(original_entities[idx], 'id', f"entity_{idx}") 
                        for idx in duplicate_indices 
                        if 0 <= idx < len(original_entities)
                    ],
                    "merged_entity_name": group.get("merged_name", primary_entity.name),
                    "confidence": group.get("confidence", 0.0),
                    "reason": group.get("reason", ""),
                    "wikipedia_evidence": group.get("wikipedia_evidence", ""),
                    "agent_decision": True
                })
        
        # æ·»åŠ ç‹¬ç«‹å®ä½“
        independent_indices = agent_result.get("independent_entities", [])
        for idx_str in independent_indices:
            idx = int(idx_str) - 1  # è½¬æ¢ä¸º0å¼€å§‹çš„ç´¢å¼•
            if 0 <= idx < len(original_entities) and idx not in processed_entity_indices:
                unified_entities.append(original_entities[idx])
                processed_entity_indices.add(idx)
        
        # æ·»åŠ ä»»ä½•æœªå¤„ç†çš„å®ä½“ï¼ˆå®‰å…¨æªæ–½ï¼‰
        for i, entity in enumerate(original_entities):
            if i not in processed_entity_indices:
                unified_entities.append(entity)
                logger.warning(f"å®ä½“ {i} æœªè¢«Agentå¤„ç†ï¼Œè‡ªåŠ¨æ·»åŠ ä¸ºç‹¬ç«‹å®ä½“")
        
        return unified_entities, merge_operations
    
    async def _build_similarity_matrix_with_threshold(self, entities: List[Any], threshold: float) -> Dict[str, Any]:
        """
        ğŸ†• ä½¿ç”¨æŒ‡å®šé˜ˆå€¼æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µç»“æœ
        """
        # æ£€æŸ¥å®ä½“æ•°é‡é™åˆ¶
        if len(entities) * len(entities) > self.config.max_matrix_size:
            logger.warning(f"å®ä½“æ•°é‡è¿‡å¤§ ({len(entities)}^2 > {self.config.max_matrix_size})ï¼Œä½¿ç”¨åˆ†å—ç­–ç•¥")
        
        matrix_result = await self.similarity_matrix_builder.build_similarity_matrix(
            entities,
            threshold=threshold,
            max_matrix_size=self.config.max_matrix_size
        )
        
        return matrix_result
    
    def _update_performance_stats(self, entity_count: int, merge_count: int, processing_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats["total_entities_processed"] += entity_count
        self.performance_stats["total_merge_operations"] += merge_count
        self.performance_stats["total_processing_time"] += processing_time
        
        # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
        embedding_stats = self.embedding_service.get_cache_statistics()
        self.performance_stats["cache_hit_rate"] = embedding_stats.get("hit_rate", 0.0)
    
    def get_performance_statistics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.performance_stats.copy()


# ğŸ†• å…¨å±€å®ä¾‹å’Œå·¥å‚å‡½æ•°
_entity_unification_service_instance = None

def get_entity_unification_service(config: Optional[UnificationConfig] = None) -> EntityUnificationService:
    """
    è·å–å®ä½“ç»Ÿä¸€æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        config: å¯é€‰é…ç½®ï¼Œä»…åœ¨é¦–æ¬¡åˆ›å»ºæ—¶ä½¿ç”¨
        
    Returns:
        EntityUnificationService: ç»Ÿä¸€æœåŠ¡å®ä¾‹
    """
    global _entity_unification_service_instance
    if _entity_unification_service_instance is None:
        _entity_unification_service_instance = EntityUnificationService(config)
    return _entity_unification_service_instance 