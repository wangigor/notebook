import logging
import json
import re
import asyncio
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from langchain_core.messages import HumanMessage
from app.core.config import settings
from app.services.llm_client_service import LLMClientService

# ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„Entityæ¨¡å‹
from app.models.entity import Entity

logger = logging.getLogger(__name__)

class EntityExtractionService:
    """LLMå®ä½“æŠ½å–æœåŠ¡ - é‡æ„ç‰ˆä½¿ç”¨æ™ºèƒ½å®ä½“ç»Ÿä¸€
    
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»æ–‡æ¡£åˆ†å—ä¸­æŠ½å–å®ä½“ï¼ŒåŒ…æ‹¬ï¼š
    - å®ä½“è¯†åˆ«å’Œç±»å‹åˆ†ç±»
    - æ™ºèƒ½å®ä½“ç»Ÿä¸€å’Œæ ‡å‡†åŒ–ï¼ˆæ›¿ä»£ä¼ ç»Ÿå»é‡ï¼‰
    - å®ä½“å±æ€§æå–
    - ç½®ä¿¡åº¦è¯„ä¼°
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å®ä½“æŠ½å–æœåŠ¡"""
        self.llm_service = LLMClientService()
        self.entity_types = self._load_entity_types()
        self.extracted_entities = set()  # ç”¨äºå»é‡
        logger.info("å®ä½“æŠ½å–æœåŠ¡å·²åˆå§‹åŒ– - ä½¿ç”¨æ™ºèƒ½å®ä½“ç»Ÿä¸€")
    
    def _load_entity_types(self) -> List[str]:
        """åŠ è½½å®ä½“ç±»å‹é…ç½®
        
        Returns:
            å®ä½“ç±»å‹åˆ—è¡¨
        """
        # å¯é…ç½®çš„å®ä½“ç±»å‹ï¼Œåç»­å¯ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½
        return [
            "äººç‰©", "ç»„ç»‡", "åœ°ç‚¹", "äº‹ä»¶", "æ¦‚å¿µ", 
            "æŠ€æœ¯", "äº§å“", "æ—¶é—´", "æ•°å­—", "æ³•å¾‹æ¡æ–‡",
            "æ”¿ç­–", "é¡¹ç›®", "ç³»ç»Ÿ", "æ–¹æ³•", "ç†è®º"
        ]
    
    async def extract_entities_from_chunks(self, chunks: List[Dict[str, Any]]) -> List[Entity]:
        """ä»æ–‡æ¡£åˆ†å—ä¸­æŠ½å–å®ä½“
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            
        Returns:
            æŠ½å–çš„å®ä½“åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹ä» {len(chunks)} ä¸ªåˆ†å—ä¸­æŠ½å–å®ä½“")
        
        all_entities = []
        
        try:
            # å¤„ç†æ¯ä¸ªåˆ†å—
            for i, chunk in enumerate(chunks):
                chunk_content = chunk.get('content', '')
                if not chunk_content.strip():
                    continue
                
                logger.info(f"å¤„ç†åˆ†å— {i+1}/{len(chunks)}")
                
                # ä»å•ä¸ªåˆ†å—æŠ½å–å®ä½“
                chunk_entities = await self._extract_entities_from_text(
                    text=chunk_content,
                    chunk_id=chunk.get('id', f'chunk_{i}'),
                    chunk_index=i
                )
                
                all_entities.extend(chunk_entities)
                
                # é¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
                if i < len(chunks) - 1:
                    await asyncio.sleep(0.1)
            
            # ğŸš€ ä½¿ç”¨æ™ºèƒ½å®ä½“ç»Ÿä¸€æ›¿ä»£ä¼ ç»Ÿå»é‡
            unified_entities = await self._unify_entities_intelligent(all_entities)
            
            logger.info(f"å®ä½“æŠ½å–å®Œæˆï¼šåŸå§‹ {len(all_entities)} ä¸ªï¼Œç»Ÿä¸€å {len(unified_entities)} ä¸ª")
            
            return unified_entities
            
        except Exception as e:
            logger.error(f"å®ä½“æŠ½å–å¤±è´¥: {str(e)}")
            raise
    
    async def _extract_entities_from_text(self, text: str, chunk_id: str, 
                                        chunk_index: int) -> List[Entity]:
        """ä»å•ä¸ªæ–‡æœ¬ä¸­æŠ½å–å®ä½“
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            chunk_id: åˆ†å—ID
            chunk_index: åˆ†å—ç´¢å¼•
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_entity_extraction_prompt(text)
            
            # è·å–LLMå®ä¾‹ï¼ˆéæµå¼ï¼‰
            llm = self.llm_service.get_processing_llm(streaming=False)
            
            # è°ƒç”¨LLM
            message = HumanMessage(content=prompt)
            response = await llm.ainvoke([message])
            
            # è·å–å“åº”å†…å®¹
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æLLMå“åº”
            entities = self._parse_llm_response(response_content, text, chunk_id, chunk_index)
            
            logger.info(f"ä»åˆ†å— {chunk_index} æŠ½å–åˆ° {len(entities)} ä¸ªå®ä½“")
            return entities
            
        except Exception as e:
            logger.error(f"ä»æ–‡æœ¬æŠ½å–å®ä½“å¤±è´¥: {str(e)}")
            return []
    
    def _build_entity_extraction_prompt(self, text: str) -> str:
        """æ„å»ºå®ä½“æŠ½å–æç¤ºè¯
        
        Args:
            text: è¦å¤„ç†çš„æ–‡æœ¬
            
        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        entity_types_str = "ã€".join(self.entity_types)
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“ä¿¡æ¯ã€‚

æ”¯æŒçš„å®ä½“ç±»å‹ï¼š{entity_types_str}

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ¯ä¸ªå®ä½“åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- name: å®ä½“åç§°ï¼ˆå¿…é¡»ï¼‰
- type: å®ä½“ç±»å‹ï¼ˆä»ä¸Šè¿°ç±»å‹ä¸­é€‰æ‹©ï¼‰
- description: å®ä½“æè¿°ï¼ˆç®€çŸ­è¯´æ˜ï¼‰
- properties: å®ä½“å±æ€§ï¼ˆé”®å€¼å¯¹ï¼Œå¯é€‰ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰
- start_pos: åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®
- end_pos: åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä½ç½®

è¿”å›æ ¼å¼ï¼š
```json
{{
    "entities": [
        {{
            "name": "å®ä½“åç§°",
            "type": "å®ä½“ç±»å‹",
            "description": "å®ä½“æè¿°",
            "properties": {{}},
            "confidence": 0.95,
            "start_pos": 0,
            "end_pos": 10
        }}
    ]
}}
```

æ³¨æ„äº‹é¡¹ï¼š
1. åªæŠ½å–é‡è¦çš„ã€æœ‰æ„ä¹‰çš„å®ä½“
2. ç¡®ä¿å®ä½“åç§°å‡†ç¡®å®Œæ•´
3. å®ä½“ç±»å‹å¿…é¡»ä»æä¾›çš„ç±»å‹ä¸­é€‰æ‹©
4. ç½®ä¿¡åº¦è¦æ ¹æ®ä¸Šä¸‹æ–‡åˆç†è¯„ä¼°
5. ä½ç½®ä¿¡æ¯è¦å‡†ç¡®
6. é¿å…é‡å¤æŠ½å–ç›¸åŒå®ä½“
"""
        return prompt
    
    def _parse_llm_response(self, response: str, source_text: str, 
                          chunk_id: str, chunk_index: int) -> List[Entity]:
        """è§£æLLMå“åº”
        
        Args:
            response: LLMå“åº”å†…å®¹
            source_text: åŸå§‹æ–‡æœ¬
            chunk_id: åˆ†å—ID
            chunk_index: åˆ†å—ç´¢å¼•
            
        Returns:
            è§£æåçš„å®ä½“åˆ—è¡¨
        """
        entities = []
        
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()
            
            # è§£æJSON
            data = json.loads(json_str)
            
            if 'entities' in data and isinstance(data['entities'], list):
                for i, entity_data in enumerate(data['entities']):
                    try:
                        # éªŒè¯å¿…éœ€å­—æ®µ
                        if not entity_data.get('name') or not entity_data.get('type'):
                            continue
                        
                        # éªŒè¯å®ä½“ç±»å‹
                        entity_type = entity_data.get('type')
                        if entity_type not in self.entity_types:
                            # å°è¯•åŒ¹é…æœ€ç›¸ä¼¼çš„ç±»å‹
                            entity_type = self._match_entity_type(entity_type)
                        
                        # ğŸ†• æ”¯æŒå¢å¼ºå­—æ®µçš„å®ä½“åˆ›å»º
                        entity = Entity(
                            id=f"{chunk_id}_entity_{i}",
                            name=entity_data.get('name', '').strip(),
                            type=entity_type,
                            description=entity_data.get('description', ''),
                            properties=entity_data.get('properties', {}),
                            confidence=float(entity_data.get('confidence', 0.8)),
                            source_text=source_text[:200] + '...' if len(source_text) > 200 else source_text,
                            start_pos=int(entity_data.get('start_pos', 0)),
                            end_pos=int(entity_data.get('end_pos', 0)),
                            # ğŸ†• æ˜¾å¼åˆå§‹åŒ–å¢å¼ºå­—æ®µï¼Œç¡®ä¿å‘å‰å…¼å®¹
                            aliases=entity_data.get('aliases', []),  # æ”¯æŒä»LLMå“åº”ä¸­è·å–åˆ«å
                            embedding=None,  # å°†åœ¨åç»­æ­¥éª¤ä¸­ç”Ÿæˆ
                            quality_score=float(entity_data.get('quality_score', 0.8))  # é»˜è®¤è´¨é‡åˆ†æ•°
                        )
                        
                        # éªŒè¯å®ä½“æœ‰æ•ˆæ€§
                        if self._validate_entity(entity, source_text):
                            entities.append(entity)
                        
                    except Exception as e:
                        logger.warning(f"è§£æå®ä½“æ•°æ®å¤±è´¥: {str(e)}")
                        continue
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
            # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å®ä½“
            entities = self._fallback_entity_extraction(response, source_text, chunk_id)
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {str(e)}")
        
        return entities
    
    def _match_entity_type(self, entity_type: str) -> str:
        """åŒ¹é…å®ä½“ç±»å‹
        
        Args:
            entity_type: åŸå§‹å®ä½“ç±»å‹
            
        Returns:
            åŒ¹é…çš„æ ‡å‡†å®ä½“ç±»å‹
        """
        entity_type_lower = entity_type.lower()
        
        # ç±»å‹æ˜ å°„è¡¨
        type_mapping = {
            'person': 'äººç‰©', 'people': 'äººç‰©', 'äººå‘˜': 'äººç‰©', 'äººå': 'äººç‰©',
            'organization': 'ç»„ç»‡', 'org': 'ç»„ç»‡', 'æœºæ„': 'ç»„ç»‡', 'å…¬å¸': 'ç»„ç»‡',
            'location': 'åœ°ç‚¹', 'place': 'åœ°ç‚¹', 'ä½ç½®': 'åœ°ç‚¹', 'åœ°å': 'åœ°ç‚¹',
            'event': 'äº‹ä»¶', 'æ´»åŠ¨': 'äº‹ä»¶', 'ä¼šè®®': 'äº‹ä»¶',
            'concept': 'æ¦‚å¿µ', 'è§‚å¿µ': 'æ¦‚å¿µ', 'æƒ³æ³•': 'æ¦‚å¿µ',
            'technology': 'æŠ€æœ¯', 'tech': 'æŠ€æœ¯', 'ç§‘æŠ€': 'æŠ€æœ¯',
            'product': 'äº§å“', 'å•†å“': 'äº§å“', 'è´§ç‰©': 'äº§å“',
            'time': 'æ—¶é—´', 'date': 'æ—¶é—´', 'æ—¥æœŸ': 'æ—¶é—´',
            'number': 'æ•°å­—', 'numeric': 'æ•°å­—', 'æ•°é‡': 'æ•°å­—',
            'law': 'æ³•å¾‹æ¡æ–‡', 'legal': 'æ³•å¾‹æ¡æ–‡', 'æ³•è§„': 'æ³•å¾‹æ¡æ–‡',
            'policy': 'æ”¿ç­–', 'æ–¹é’ˆ': 'æ”¿ç­–', 'è§„å®š': 'æ”¿ç­–',
            'project': 'é¡¹ç›®', 'å·¥ç¨‹': 'é¡¹ç›®', 'è®¡åˆ’': 'é¡¹ç›®',
            'system': 'ç³»ç»Ÿ', 'ä½“ç³»': 'ç³»ç»Ÿ', 'åˆ¶åº¦': 'ç³»ç»Ÿ',
            'method': 'æ–¹æ³•', 'æ–¹å¼': 'æ–¹æ³•', 'æ‰‹æ®µ': 'æ–¹æ³•',
            'theory': 'ç†è®º', 'å­¦è¯´': 'ç†è®º', 'è§‚ç‚¹': 'ç†è®º'
        }
        
        # æŸ¥æ‰¾åŒ¹é…
        for key, value in type_mapping.items():
            if key in entity_type_lower:
                return value
        
        # é»˜è®¤è¿”å›æ¦‚å¿µç±»å‹
        return 'æ¦‚å¿µ'
    
    def _validate_entity(self, entity: Entity, source_text: str) -> bool:
        """éªŒè¯å®ä½“æœ‰æ•ˆæ€§
        
        Args:
            entity: å®ä½“å¯¹è±¡
            source_text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # æ£€æŸ¥åç§°é•¿åº¦
        if len(entity.name) < 2 or len(entity.name) > 100:
            return False
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        if entity.confidence < 0.3:
            return False
        
        # æ£€æŸ¥ä½ç½®ä¿¡æ¯
        if entity.start_pos < 0 or entity.end_pos <= entity.start_pos:
            return False
        
        if entity.end_pos > len(source_text):
            return False
        
        # æ£€æŸ¥å®ä½“åç§°æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
        extracted_text = source_text[entity.start_pos:entity.end_pos]
        if entity.name.lower() not in extracted_text.lower():
            # å°è¯•åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æŸ¥æ‰¾
            if entity.name.lower() not in source_text.lower():
                return False
        
        return True
    
    def _fallback_entity_extraction(self, response: str, source_text: str, 
                                  chunk_id: str) -> List[Entity]:
        """å¤‡é€‰å®ä½“æŠ½å–æ–¹æ³•
        
        Args:
            response: LLMå“åº”
            source_text: åŸå§‹æ–‡æœ¬
            chunk_id: åˆ†å—ID
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        entities = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾å¯èƒ½çš„å®ä½“
            lines = response.split('\n')
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # å¯»æ‰¾åŒ…å«å®ä½“ä¿¡æ¯çš„è¡Œ
                if any(keyword in line for keyword in ['åç§°', 'name', 'å®ä½“', 'ç±»å‹']):
                    # æå–å¯èƒ½çš„å®ä½“åç§°
                    name_match = re.search(r'[ï¼š:]\s*([^ï¼Œ,ã€‚.]+)', line)
                    if name_match:
                        entity_name = name_match.group(1).strip()
                        
                        if len(entity_name) >= 2:
                            # ğŸ†• æ”¯æŒå¢å¼ºå­—æ®µçš„fallbackå®ä½“åˆ›å»º
                            entity = Entity(
                                id=f"{chunk_id}_fallback_{i}",
                                name=entity_name,
                                type='æ¦‚å¿µ',  # é»˜è®¤ç±»å‹
                                description=f"é€šè¿‡å¤‡é€‰æ–¹æ³•æŠ½å–ï¼š{line}",
                                properties={},
                                confidence=0.5,  # è¾ƒä½ç½®ä¿¡åº¦
                                source_text=source_text[:100] + '...',
                                start_pos=0,
                                end_pos=len(entity_name),
                                # ğŸ†• æ˜¾å¼åˆå§‹åŒ–å¢å¼ºå­—æ®µ
                                aliases=[],  # å¤‡é€‰æ–¹æ³•æ— æ³•è·å–åˆ«å
                                embedding=None,  # å°†åœ¨åç»­æ­¥éª¤ä¸­ç”Ÿæˆ
                                quality_score=0.5  # å¤‡é€‰æ–¹æ³•è´¨é‡è¾ƒä½
                            )
                            entities.append(entity)
            
        except Exception as e:
            logger.error(f"å¤‡é€‰å®ä½“æŠ½å–å¤±è´¥: {str(e)}")
        
        return entities
    
    async def _unify_entities_intelligent(self, entities: List[Entity]) -> List[Entity]:
        """æ™ºèƒ½å®ä½“ç»Ÿä¸€ - ç›´æ¥ä½¿ç”¨æ™ºèƒ½ç»Ÿä¸€æœåŠ¡
        
        Args:
            entities: åŸå§‹å®ä½“åˆ—è¡¨
            
        Returns:
            ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨
        """
        from app.services.entity_unification_service import get_entity_unification_service
        
        logger.info(f"å¼€å§‹æ™ºèƒ½å®ä½“ç»Ÿä¸€ {len(entities)} ä¸ªå®ä½“")
        
        # è·å–ç»Ÿä¸€æœåŠ¡å®ä¾‹
        unification_service = get_entity_unification_service()
        
        # æ‰§è¡Œç»Ÿä¸€
        unification_result = await unification_service.unify_entities(entities)
        
        logger.info(f"æ™ºèƒ½å®ä½“ç»Ÿä¸€å®Œæˆï¼šåŸå§‹ {len(entities)} ä¸ªï¼Œç»Ÿä¸€å {len(unification_result.unified_entities)} ä¸ª")
        
        return unification_result.unified_entities
    
    def _normalize_entity_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å®ä½“åç§°
        
        Args:
            name: åŸå§‹åç§°
            
        Returns:
            æ ‡å‡†åŒ–åçš„åç§°
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', name.strip())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        normalized = re.sub(r'[""''ã€Šã€‹ã€ã€‘ï¼ˆï¼‰()]', '', normalized)
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        return normalized.lower()
    
    # å†—ä½™æ–¹æ³•å·²ç§»é™¤ï¼Œå…¨é¢ä½¿ç”¨æ™ºèƒ½ç»Ÿä¸€æœåŠ¡
    
    async def get_extraction_statistics(self, entities: List[Entity]) -> Dict[str, Any]:
        """è·å–æŠ½å–ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_entities': len(entities),
            'by_type': {},
            'confidence_distribution': {
                'high': 0,  # > 0.8
                'medium': 0,  # 0.5 - 0.8
                'low': 0  # < 0.5
            },
            'avg_confidence': 0.0
        }
        
        if not entities:
            return stats
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        for entity in entities:
            entity_type = entity.type
            if entity_type not in stats['by_type']:
                stats['by_type'][entity_type] = 0
            stats['by_type'][entity_type] += 1
            
            # ç½®ä¿¡åº¦åˆ†å¸ƒ
            if entity.confidence > 0.8:
                stats['confidence_distribution']['high'] += 1
            elif entity.confidence > 0.5:
                stats['confidence_distribution']['medium'] += 1
            else:
                stats['confidence_distribution']['low'] += 1
        
        # å¹³å‡ç½®ä¿¡åº¦
        stats['avg_confidence'] = sum(e.confidence for e in entities) / len(entities)
        
        return stats