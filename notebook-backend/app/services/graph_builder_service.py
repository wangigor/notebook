import logging
import hashlib
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from dataclasses import asdict
from app.core.config import settings
from app.services.neo4j_service import Neo4jService

# ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„Entityå’ŒRelationshipæ¨¡å‹
from app.models.entity import Entity, Relationship
from app.services.chunk_service import DocumentChunk

logger = logging.getLogger(__name__)

class GraphBuilderService:
    """å›¾è°±æ„å»ºæœåŠ¡
    
    è´Ÿè´£å°†æŠ½å–çš„å®ä½“å’Œå…³ç³»æ„å»ºæˆçŸ¥è¯†å›¾è°±å¹¶å­˜å‚¨åˆ°Neo4jï¼ŒåŒ…æ‹¬ï¼š
    - å®ä½“èŠ‚ç‚¹åˆ›å»ºå’Œå»é‡
    - å…³ç³»è¾¹åˆ›å»ºå’ŒéªŒè¯
    - å›¾è°±è´¨é‡è¯„ä¼°
    - æ•°æ®ä¸€è‡´æ€§ä¿è¯
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å›¾è°±æ„å»ºæœåŠ¡"""
        self.neo4j_service = Neo4jService()
        self.created_entities = set()  # ç”¨äºå»é‡
        self.created_relationships = set()  # ç”¨äºå»é‡
        self.created_documents = set()
        self.created_chunks = set()
        self.chunk_entity_mapping = []
        logger.info("å›¾è°±æ„å»ºæœåŠ¡å·²åˆå§‹åŒ–")
    
    def _create_document_node(self, document_id: int, name: str, file_type: str, 
                             file_size: int, created_at: datetime) -> str:
        """åˆ›å»ºDocumentèŠ‚ç‚¹åˆ°Neo4j
        
        Args:
            document_id: PostgreSQLæ–‡æ¡£ID
            name: æ–‡æ¡£åç§°
            file_type: æ–‡ä»¶ç±»å‹
            file_size: æ–‡ä»¶å¤§å°
            created_at: åˆ›å»ºæ—¶é—´
            
        Returns:
            Neo4jèŠ‚ç‚¹ID
        """
        try:
            neo4j_node_id = self.neo4j_service.create_document_node(
                postgresql_id=document_id,
                name=name,
                file_type=file_type,
                file_size=file_size,
                created_at=created_at
            )
            self.created_documents.add(neo4j_node_id)
            logger.info(f"DocumentèŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {neo4j_node_id}")
            return neo4j_node_id
        except Exception as e:
            logger.error(f"åˆ›å»ºDocumentèŠ‚ç‚¹å¤±è´¥: {str(e)}")
            raise
    
    def _create_chunk_nodes(self, chunks: List[DocumentChunk], 
                           document_neo4j_id: str) -> List[str]:
        """åˆ›å»ºChunkèŠ‚ç‚¹åˆ°Neo4j
        
        Args:
            chunks: åˆ†å—åˆ—è¡¨
            document_neo4j_id: Documentçš„Neo4jèŠ‚ç‚¹ID
            
        Returns:
            ChunkèŠ‚ç‚¹IDåˆ—è¡¨
        """
        try:
            chunks_data = []
            for chunk in chunks:
                chunk_data = {
                    "chunk_id": chunk.metadata.chunk_id,
                    "content": chunk.content,
                    "position": f"{chunk.metadata.start_char}-{chunk.metadata.end_char}",
                    "chunk_index": chunk.metadata.chunk_index,
                    "start_char": chunk.metadata.start_char,
                    "end_char": chunk.metadata.end_char,
                    "content_length": chunk.metadata.content_length,
                    "word_count": chunk.metadata.word_count,
                    "paragraph_count": chunk.metadata.paragraph_count,
                    "chunk_type": chunk.metadata.chunk_type,
                    "created_at": chunk.metadata.created_at,
                    "postgresql_document_id": chunk.metadata.postgresql_document_id,
                    "embedding": chunk.metadata.embedding,
                    "vector_dimension": chunk.metadata.vector_dimension
                }
                chunks_data.append(chunk_data)
            
            chunk_neo4j_ids = self.neo4j_service.batch_create_chunk_nodes(chunks_data)
            
            # åˆ›å»ºPART_OFå…³ç³»
            if chunk_neo4j_ids:
                self.neo4j_service.create_chunk_document_relationships(
                    chunk_neo4j_ids, document_neo4j_id
                )
            
            # è®°å½•åˆ›å»ºçš„chunks
            self.created_chunks.update(chunk_neo4j_ids)
            
            logger.info(f"ChunkèŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {len(chunk_neo4j_ids)} ä¸ª")
            return chunk_neo4j_ids
            
        except Exception as e:
            logger.error(f"åˆ›å»ºChunkèŠ‚ç‚¹å¤±è´¥: {str(e)}")
            raise
    
    def _link_entities_to_chunks(self, entities: List[Entity], 
                                chunk_neo4j_ids: List[str],
                                chunks: Optional[List[DocumentChunk]] = None) -> None:
        """å…³è”å®ä½“åˆ°å¯¹åº”çš„chunk
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            chunk_neo4j_ids: Chunkçš„Neo4jèŠ‚ç‚¹IDåˆ—è¡¨
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨ï¼ˆç”¨äºå»ºç«‹æ˜ å°„å…³ç³»ï¼‰
        """
        try:
            chunk_entity_pairs = []
            
            # åˆ›å»ºchunk_idåˆ°Neo4j IDçš„æ˜ å°„
            chunk_id_to_neo4j_id = {}
            if chunks:
                for i, chunk in enumerate(chunks):
                    if i < len(chunk_neo4j_ids):
                        # ä¸»è¦ä½¿ç”¨chunk_idæ˜ å°„
                        chunk_id_to_neo4j_id[chunk.metadata.chunk_id] = chunk_neo4j_ids[i]
                        # ä¹Ÿæ”¯æŒchunkç´¢å¼•æ˜ å°„ï¼ˆå¤‡ç”¨ï¼‰
                        chunk_id_to_neo4j_id[chunk.metadata.chunk_index] = chunk_neo4j_ids[i]
            else:
                # å¦‚æœæ²¡æœ‰chunksä¿¡æ¯ï¼Œå‡è®¾é¡ºåºå¯¹åº”
                for i, chunk_neo4j_id in enumerate(chunk_neo4j_ids):
                    chunk_id_to_neo4j_id[i] = chunk_neo4j_id
            
            # éå†å®ä½“ï¼Œæ ¹æ®å®ä½“IDä¸­çš„chunkä¿¡æ¯å»ºç«‹å…³è”
            for entity in entities:
                try:
                    # ä»å®ä½“IDä¸­æå–chunkä¿¡æ¯
                    chunk_id = self._extract_chunk_info_from_entity_id(entity.id)
                    
                    if chunk_id is not None:
                        # æŸ¥æ‰¾å¯¹åº”çš„chunk Neo4j ID
                        chunk_neo4j_id = chunk_id_to_neo4j_id.get(chunk_id)
                        
                        if chunk_neo4j_id:
                            # ç”Ÿæˆå®ä½“çš„Neo4jèŠ‚ç‚¹ID
                            entity_neo4j_id = self._get_entity_neo4j_id(entity)
                            
                            if entity_neo4j_id:
                                chunk_entity_pairs.append({
                                    "chunk_id": chunk_neo4j_id,
                                    "entity_id": entity_neo4j_id
                                })
                                logger.debug(f"å…³è”å®ä½“ {entity.name} (ID: {entity_neo4j_id}) åˆ° chunk {chunk_id} (Neo4j ID: {chunk_neo4j_id})")
                            else:
                                logger.warning(f"æ— æ³•ç”Ÿæˆå®ä½“ {entity.name} çš„Neo4jèŠ‚ç‚¹ID")
                        else:
                            logger.warning(f"æ‰¾ä¸åˆ°chunk {chunk_id} å¯¹åº”çš„Neo4jèŠ‚ç‚¹ID")
                    else:
                        logger.warning(f"æ— æ³•ä»å®ä½“ID {entity.id} ä¸­æå–chunkä¿¡æ¯")
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†å®ä½“ {entity.name} çš„chunkå…³è”æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æ‰¹é‡åˆ›å»ºHAS_ENTITYå…³ç³»
            if chunk_entity_pairs:
                relationship_count = self.neo4j_service.create_chunk_entity_relationships(
                    chunk_entity_pairs
                )
                logger.info(f"Chunk-Entityå…³ç³»åˆ›å»ºæˆåŠŸ: {relationship_count} ä¸ªå…³ç³»ï¼Œå¤„ç†äº† {len(chunk_entity_pairs)} ä¸ªé…å¯¹")
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ä»¥å…³è”çš„chunk-entityé…å¯¹")
            
        except Exception as e:
            logger.error(f"å…³è”å®ä½“åˆ°Chunkå¤±è´¥: {str(e)}")
            raise
    
    def _extract_chunk_info_from_entity_id(self, entity_id: str) -> Optional[str]:
        """ä»å®ä½“IDä¸­æå–chunkä¿¡æ¯
        
        å®ä½“IDæ ¼å¼ï¼š{chunk_id}_entity_{entity_index}
        chunk_idæ ¼å¼ï¼šdoc{document_id}_chunk{chunk_index}_{content_hash}
        
        Args:
            entity_id: å®ä½“ID
            
        Returns:
            å®Œæ•´çš„chunk_idï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            # å®ä½“IDæ ¼å¼ï¼š{chunk_id}_entity_{entity_index}
            if "_entity_" in entity_id:
                chunk_id = entity_id.split("_entity_")[0]
                
                # éªŒè¯chunk_idæ ¼å¼
                if "chunk" in chunk_id:
                    logger.debug(f"ä»å®ä½“ID {entity_id} ä¸­æå–åˆ°chunk_id: {chunk_id}")
                    return chunk_id
                else:
                    logger.warning(f"æå–çš„chunk_idæ ¼å¼ä¸æ­£ç¡®: {chunk_id}")
                    return None
            
            logger.warning(f"å®ä½“IDæ ¼å¼ä¸æ­£ç¡®: {entity_id}")
            return None
            
        except Exception as e:
            logger.warning(f"è§£æå®ä½“ID {entity_id} å¤±è´¥: {str(e)}")
            return None

    def _get_entity_neo4j_id(self, entity: Entity) -> Optional[str]:
        """æ ¹æ®å®ä½“è·å–å…¶Neo4jèŠ‚ç‚¹ID
        
        Args:
            entity: å®ä½“å¯¹è±¡
            
        Returns:
            Neo4jèŠ‚ç‚¹IDæˆ–None
        """
        try:
            # ç”Ÿæˆä¸_create_entity_nodesä¸­ç›¸åŒçš„èŠ‚ç‚¹ID
            node_id = self._generate_node_id(entity.name, entity.type)
            return node_id
        except Exception as e:
            logger.warning(f"è·å–å®ä½“Neo4j IDå¤±è´¥: {entity.name}, é”™è¯¯: {str(e)}")
            return None
    
    async def build_graph_from_extracted_data(self, entities: List[Entity], 
                                            relationships: List[Relationship],
                                            document_id: int,
                                            document_info: Optional[Dict[str, Any]] = None,
                                            chunks: Optional[List[DocumentChunk]] = None,
                                            auto_store: bool = False) -> Dict[str, Any]:
        """ä»æŠ½å–çš„å®ä½“å’Œå…³ç³»æ„å»ºå›¾è°±
        
        Args:
            entities: æŠ½å–çš„å®ä½“åˆ—è¡¨
            relationships: æŠ½å–çš„å…³ç³»åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            document_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«name, file_type, file_size, created_atç­‰
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            
        Returns:
            æ„å»ºç»“æœ
        """
        logger.info(f"å¼€å§‹æ„å»ºæ–‡æ¡£{document_id}çš„çŸ¥è¯†å›¾è°±")
        
        try:
            # 1. å‡†å¤‡DocumentèŠ‚ç‚¹æ•°æ®ï¼ˆå¦‚æœæä¾›äº†æ–‡æ¡£ä¿¡æ¯ï¼‰
            document_node_data = None
            if document_info:
                document_node_data = {
                    "id": f"doc_{document_id}",
                    "name": document_info.get('name', f'Document_{document_id}'),
                    "type": "Document",
                    "labels": ["Document"],
                    "properties": {
                        "node_id": f"doc_{document_id}",  # åœ¨propertiesä¸­åŒ…å«node_idä½œä¸ºå¤‡ä»½
                        "postgresql_id": document_id,
                        "file_type": document_info.get('file_type', 'unknown'),
                        "file_size": document_info.get('file_size', 0),
                        "created_at": document_info.get('created_at', datetime.now()).isoformat() if isinstance(document_info.get('created_at'), datetime) else str(document_info.get('created_at', datetime.now()))
                    }
                }
                logger.info(f"å‡†å¤‡DocumentèŠ‚ç‚¹æ•°æ®: {document_node_data['id']}")
            
            # 2. å‡†å¤‡ChunkèŠ‚ç‚¹æ•°æ®ï¼ˆå¦‚æœæä¾›äº†chunksæ•°æ®ï¼‰
            chunk_nodes_data = []
            if chunks:
                for i, chunk in enumerate(chunks):
                    chunk_node_data = {
                        "id": f"chunk_{chunk.metadata.chunk_id}",
                        "name": f"Chunk_{chunk.metadata.chunk_index}",
                        "type": "Chunk",
                        "labels": ["Chunk"],
                        "properties": {
                            "node_id": f"chunk_{chunk.metadata.chunk_id}",  # åœ¨propertiesä¸­åŒ…å«node_idä½œä¸ºå¤‡ä»½
                            "chunk_id": chunk.metadata.chunk_id,
                            "content": chunk.content,
                            "position": f"{chunk.metadata.start_char}-{chunk.metadata.end_char}",
                            "chunk_index": chunk.metadata.chunk_index,
                            "start_char": chunk.metadata.start_char,
                            "end_char": chunk.metadata.end_char,
                            "content_length": chunk.metadata.content_length,
                            "word_count": chunk.metadata.word_count,
                            "paragraph_count": chunk.metadata.paragraph_count,
                            "chunk_type": chunk.metadata.chunk_type,
                            "created_at": chunk.metadata.created_at.isoformat() if isinstance(chunk.metadata.created_at, datetime) else str(chunk.metadata.created_at),
                            "postgresql_document_id": chunk.metadata.postgresql_document_id,
                            "embedding": chunk.metadata.embedding,
                            "vector_dimension": chunk.metadata.vector_dimension
                        }
                    }
                    chunk_nodes_data.append(chunk_node_data)
                logger.info(f"å‡†å¤‡ChunkèŠ‚ç‚¹æ•°æ®: {len(chunk_nodes_data)} ä¸ª")
            
            # 3. å‡†å¤‡å®ä½“èŠ‚ç‚¹æ•°æ®
            entity_nodes = await self._create_entity_nodes(entities, document_id)
            
            # 4. å‡†å¤‡å…³ç³»è¾¹æ•°æ®
            relationship_edges = await self._create_relationship_edges(relationships, entity_nodes, document_id)
            
            # 5. å‡†å¤‡Document-Chunk PART_OFå…³ç³»æ•°æ®
            document_chunk_relationships = []
            if document_node_data and chunk_nodes_data:
                for chunk_data in chunk_nodes_data:
                    document_chunk_relationships.append({
                        "id": f"rel_doc_chunk_{chunk_data['properties']['chunk_index']}",
                        "source_id": chunk_data["id"],
                        "target_id": document_node_data["id"],
                        "type": "PART_OF",
                        "properties": {
                            "created_at": datetime.now().isoformat()
                        }
                    })
                logger.info(f"å‡†å¤‡Document-Chunk PART_OFå…³ç³»æ•°æ®: {len(document_chunk_relationships)} ä¸ª")
            
            # 6. å‡†å¤‡Document-FIRST_CHUNKå…³ç³»æ•°æ®
            document_first_chunk_relationships = []
            if document_node_data and chunks:
                document_first_chunk_relationships = self._prepare_document_chunk_relationships(chunks, document_node_data["id"])
                logger.info(f"å‡†å¤‡Document-FIRST_CHUNKå…³ç³»æ•°æ®: {len(document_first_chunk_relationships)} ä¸ª")
            
            # 7. å‡†å¤‡Chunk-NEXT_CHUNKå…³ç³»æ•°æ®
            chunk_sequence_relationships = []
            if chunks:
                chunk_sequence_relationships = self._prepare_chunk_sequence_relationships(chunks)
                logger.info(f"å‡†å¤‡Chunk-NEXT_CHUNKå…³ç³»æ•°æ®: {len(chunk_sequence_relationships)} ä¸ª")
            
            # 8. å‡†å¤‡å¤šé‡Chunk-Entityå…³ç³»æ•°æ®
            chunk_entity_relationships = self._prepare_multi_chunk_entity_relationships(entities, chunk_nodes_data, chunks)
            logger.info(f"å‡†å¤‡å¤šé‡Chunk-Entityå…³ç³»æ•°æ®: {len(chunk_entity_relationships)} ä¸ª")
            
            # 9. è¯„ä¼°å›¾è°±è´¨é‡
            quality_metrics = self._evaluate_graph_quality(entity_nodes, relationship_edges)
            
            # 10. å‡†å¤‡å®Œæ•´çš„å›¾è°±æ•°æ®
            all_nodes = []
            all_relationships = []
            
            # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
            if document_node_data:
                all_nodes.append(document_node_data)
            all_nodes.extend(chunk_nodes_data)
            all_nodes.extend(entity_nodes)
            
            # æ·»åŠ æ‰€æœ‰å…³ç³»
            all_relationships.extend(document_chunk_relationships)          # PART_OFå…³ç³»
            all_relationships.extend(document_first_chunk_relationships)    # FIRST_CHUNKå…³ç³»
            all_relationships.extend(chunk_sequence_relationships)          # NEXT_CHUNKå…³ç³»
            all_relationships.extend(chunk_entity_relationships)            # HAS_ENTITYå…³ç³»
            all_relationships.extend(relationship_edges)                    # å®ä½“é—´å…³ç³»
            
            graph_data = {
                "document_id": document_id,
                "nodes": all_nodes,
                "edges": all_relationships,
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "total_nodes": len(all_nodes),
                    "total_edges": len(all_relationships),
                    "total_chunks": len(chunk_nodes_data),
                    "total_entities": len(entity_nodes),
                    "total_part_of_relationships": len(document_chunk_relationships),
                    "total_first_chunk_relationships": len(document_first_chunk_relationships),
                    "total_next_chunk_relationships": len(chunk_sequence_relationships),
                    "total_chunk_entity_relationships": len(chunk_entity_relationships),
                    "total_entity_relationships": len(relationship_edges),
                    "quality_metrics": quality_metrics
                }
            }
            
            logger.info(f"å›¾è°±æ„å»ºå®Œæˆï¼š{len(all_nodes)} ä¸ªèŠ‚ç‚¹ï¼ˆ{len(entity_nodes)} ä¸ªå®ä½“ï¼Œ{len(chunk_nodes_data)} ä¸ªChunkï¼‰ï¼Œ{len(all_relationships)} æ¡å…³ç³»ï¼ˆ{len(document_chunk_relationships)} ä¸ªPART_OFï¼Œ{len(document_first_chunk_relationships)} ä¸ªFIRST_CHUNKï¼Œ{len(chunk_sequence_relationships)} ä¸ªNEXT_CHUNKï¼Œ{len(chunk_entity_relationships)} ä¸ªHAS_ENTITYï¼Œ{len(relationship_edges)} ä¸ªå®ä½“é—´å…³ç³»ï¼‰")
            
            # 11. è‡ªåŠ¨å­˜å‚¨åˆ°Neo4jï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if auto_store:
                logger.info("å¼€å§‹è‡ªåŠ¨å­˜å‚¨å›¾è°±æ•°æ®åˆ°Neo4j")
                store_result = await self.neo4j_service.store_graph_data(graph_data)
                graph_data["store_result"] = store_result
                if store_result["success"]:
                    logger.info(f"å›¾è°±æ•°æ®å­˜å‚¨æˆåŠŸï¼š{store_result['nodes_created']} ä¸ªèŠ‚ç‚¹ï¼Œ{store_result['relationships_created']} æ¡å…³ç³»")
                else:
                    logger.error(f"å›¾è°±æ•°æ®å­˜å‚¨å¤±è´¥ï¼š{store_result.get('errors', [])}")
            
            return graph_data
            
        except Exception as e:
            logger.error(f"å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
            raise
    
    async def _create_entity_nodes(self, entities: List[Entity], document_id: int) -> List[Dict[str, Any]]:
        """åˆ›å»ºå®ä½“èŠ‚ç‚¹
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            
        Returns:
            èŠ‚ç‚¹åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹åˆ›å»º {len(entities)} ä¸ªå®ä½“èŠ‚ç‚¹")
        
        nodes = []
        entity_map = {}  # å®ä½“åç§°åˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
        
        for entity in entities:
            try:
                # ç”ŸæˆèŠ‚ç‚¹ID
                node_id = self._generate_node_id(entity.name, entity.type)
                
                # æ£€æŸ¥å»é‡
                if node_id in self.created_entities:
                    # å¦‚æœå®ä½“å·²å­˜åœ¨ï¼Œæ›´æ–°æ˜ å°„
                    entity_map[entity.name] = node_id
                    continue
                
                # åˆ›å»ºèŠ‚ç‚¹æ•°æ®
                node_data = {
                    "id": node_id,
                    "name": entity.name,
                    "type": entity.type,
                    "description": entity.description,
                    "confidence": entity.confidence,
                    "source_text": entity.source_text,
                    "document_id": document_id,
                    "chunk_id": getattr(entity, 'chunk_neo4j_id', None),
                    "aliases": getattr(entity, 'aliases', []),
                    "importance_score": getattr(entity, 'importance_score', 0.5),
                    "quality_score": getattr(entity, 'quality_score', entity.confidence),
                    "properties": {
                        **entity.properties,
                        "node_id": node_id,  # åœ¨propertiesä¸­åŒ…å«node_idä½œä¸ºå¤‡ä»½
                        "created_at": datetime.now().isoformat()
                    },
                    "labels": [entity.type, "Entity"],  # Neo4jæ ‡ç­¾
                    "embedding": getattr(entity, 'embedding', None)  # å¦‚æœæœ‰å‘é‡
                }
                
                nodes.append(node_data)
                entity_map[entity.name] = node_id
                self.created_entities.add(node_id)
                
            except Exception as e:
                logger.warning(f"åˆ›å»ºå®ä½“èŠ‚ç‚¹å¤±è´¥: {entity.name}, é”™è¯¯: {str(e)}")
                continue
        
        logger.info(f"æˆåŠŸåˆ›å»º {len(nodes)} ä¸ªå®ä½“èŠ‚ç‚¹")
        return nodes
    
    async def _create_relationship_edges(self, relationships: List[Relationship], 
                                       entity_nodes: List[Dict[str, Any]], 
                                       document_id: int) -> List[Dict[str, Any]]:
        """åˆ›å»ºå…³ç³»è¾¹
        
        Args:
            relationships: å…³ç³»åˆ—è¡¨
            entity_nodes: å®ä½“èŠ‚ç‚¹åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            
        Returns:
            è¾¹åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹åˆ›å»º {len(relationships)} æ¡å…³ç³»è¾¹")
        
        edges = []
        # åˆ›å»ºå®ä½“åç§°åˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
        entity_name_to_id = {}
        for node in entity_nodes:
            entity_name_to_id[node["name"]] = node["id"]
        
        for relationship in relationships:
            try:
                # æŸ¥æ‰¾æºå’Œç›®æ ‡å®ä½“çš„èŠ‚ç‚¹ID
                source_node_id = entity_name_to_id.get(relationship.source_entity_name)
                target_node_id = entity_name_to_id.get(relationship.target_entity_name)
                
                if not source_node_id or not target_node_id:
                    logger.warning(f"å…³ç³»ä¸­çš„å®ä½“æœªæ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹: {relationship.source_entity_name} -> {relationship.target_entity_name}")
                    continue
                
                # ç”Ÿæˆè¾¹ID
                edge_id = self._generate_edge_id(source_node_id, target_node_id, relationship.relationship_type)
                
                # æ£€æŸ¥å»é‡
                if edge_id in self.created_relationships:
                    continue
                
                # åˆ›å»ºè¾¹æ•°æ®
                edge_data = {
                    "id": edge_id,
                    "source_id": source_node_id,
                    "target_id": target_node_id,
                    "source_name": relationship.source_entity_name,
                    "target_name": relationship.target_entity_name,
                    "type": relationship.relationship_type,
                    "description": relationship.description,
                    "properties": {
                        **relationship.properties,
                        "confidence": relationship.confidence,
                        "context": relationship.context,
                        "source_text": relationship.source_text,
                        "document_id": document_id,
                        "created_at": datetime.now().isoformat()
                    }
                }
                
                edges.append(edge_data)
                self.created_relationships.add(edge_id)
                
            except Exception as e:
                logger.warning(f"åˆ›å»ºå…³ç³»è¾¹å¤±è´¥: {relationship.relationship_type}, é”™è¯¯: {str(e)}")
                continue
        
        logger.info(f"æˆåŠŸåˆ›å»º {len(edges)} æ¡å…³ç³»è¾¹")
        return edges
    
    def _generate_node_id(self, entity_name: str, entity_type: str) -> str:
        """ç”ŸæˆèŠ‚ç‚¹ID
        
        Args:
            entity_name: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹
            
        Returns:
            èŠ‚ç‚¹ID
        """
        # ä½¿ç”¨å®ä½“åç§°å’Œç±»å‹ç”Ÿæˆå”¯ä¸€ID
        content = f"{entity_name.lower()}_{entity_type}"
        return f"entity_{hashlib.md5(content.encode()).hexdigest()[:8]}"
    
    def _generate_edge_id(self, source_id: str, target_id: str, relationship_type: str) -> str:
        """ç”Ÿæˆè¾¹ID
        
        Args:
            source_id: æºèŠ‚ç‚¹ID
            target_id: ç›®æ ‡èŠ‚ç‚¹ID
            relationship_type: å…³ç³»ç±»å‹
            
        Returns:
            è¾¹ID
        """
        content = f"{source_id}_{target_id}_{relationship_type}"
        return f"rel_{hashlib.md5(content.encode()).hexdigest()[:8]}"
    
    def _evaluate_graph_quality(self, nodes: List[Dict[str, Any]], 
                               edges: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°å›¾è°±è´¨é‡
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            edges: è¾¹åˆ—è¡¨
            
        Returns:
            è´¨é‡æŒ‡æ ‡
        """
        if not nodes:
            return {"quality_score": 0.0, "issues": ["æ— å®ä½“èŠ‚ç‚¹"]}
        
        metrics = {
            "node_count": len(nodes),
            "edge_count": len(edges),
            "density": 0.0,  # å›¾å¯†åº¦
            "avg_node_degree": 0.0,  # å¹³å‡èŠ‚ç‚¹åº¦æ•°
            "isolated_nodes": 0,  # å­¤ç«‹èŠ‚ç‚¹æ•°
            "entity_type_distribution": {},  # å®ä½“ç±»å‹åˆ†å¸ƒ
            "relationship_type_distribution": {},  # å…³ç³»ç±»å‹åˆ†å¸ƒ
            "quality_score": 0.0,  # æ€»ä½“è´¨é‡åˆ†æ•°
            "issues": []  # è´¨é‡é—®é¢˜
        }
        
        try:
            # è®¡ç®—å®ä½“ç±»å‹åˆ†å¸ƒ
            for node in nodes:
                entity_type = node.get("type", "æœªçŸ¥")
                if entity_type not in metrics["entity_type_distribution"]:
                    metrics["entity_type_distribution"][entity_type] = 0
                metrics["entity_type_distribution"][entity_type] += 1
            
            # è®¡ç®—å…³ç³»ç±»å‹åˆ†å¸ƒ
            for edge in edges:
                rel_type = edge.get("type", "æœªçŸ¥")
                if rel_type not in metrics["relationship_type_distribution"]:
                    metrics["relationship_type_distribution"][rel_type] = 0
                metrics["relationship_type_distribution"][rel_type] += 1
            
            # è®¡ç®—èŠ‚ç‚¹åº¦æ•°
            node_degrees = {}
            for edge in edges:
                source_id = edge.get("source_id")
                target_id = edge.get("target_id")
                
                if source_id:
                    node_degrees[source_id] = node_degrees.get(source_id, 0) + 1
                if target_id:
                    node_degrees[target_id] = node_degrees.get(target_id, 0) + 1
            
            # è®¡ç®—å­¤ç«‹èŠ‚ç‚¹
            connected_nodes = set(node_degrees.keys())
            all_nodes = {node["id"] for node in nodes}
            isolated_nodes = all_nodes - connected_nodes
            metrics["isolated_nodes"] = len(isolated_nodes)
            
            # è®¡ç®—å¹³å‡åº¦æ•°
            if node_degrees:
                metrics["avg_node_degree"] = sum(node_degrees.values()) / len(node_degrees)
            
            # è®¡ç®—å›¾å¯†åº¦
            if len(nodes) > 1:
                max_edges = len(nodes) * (len(nodes) - 1)  # æœ‰å‘å›¾
                metrics["density"] = len(edges) / max_edges if max_edges > 0 else 0
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_factors = []
            
            # å› å­1ï¼šè¿é€šæ€§ (æƒé‡: 0.3)
            connectivity_score = 1.0 - (metrics["isolated_nodes"] / len(nodes))
            quality_factors.append(("connectivity", connectivity_score, 0.3))
            
            # å› å­2ï¼šå®ä½“ç½®ä¿¡åº¦ (æƒé‡: 0.3)
            entity_confidences = [
                node.get("properties", {}).get("confidence", 0.5) 
                for node in nodes
            ]
            avg_entity_confidence = sum(entity_confidences) / len(entity_confidences) if entity_confidences else 0
            quality_factors.append(("entity_confidence", avg_entity_confidence, 0.3))
            
            # å› å­3ï¼šå…³ç³»ç½®ä¿¡åº¦ (æƒé‡: 0.2)
            if edges:
                relationship_confidences = [
                    edge.get("properties", {}).get("confidence", 0.5) 
                    for edge in edges
                ]
                avg_rel_confidence = sum(relationship_confidences) / len(relationship_confidences)
            else:
                avg_rel_confidence = 0
            quality_factors.append(("relationship_confidence", avg_rel_confidence, 0.2))
            
            # å› å­4ï¼šå›¾å¯†åº¦é€‚ä¸­æ€§ (æƒé‡: 0.2)
            # å¯†åº¦åœ¨0.1-0.5ä¹‹é—´è®¤ä¸ºæ˜¯åˆç†çš„
            density_score = 1.0 if 0.1 <= metrics["density"] <= 0.5 else max(0, 1 - abs(metrics["density"] - 0.3) / 0.3)
            quality_factors.append(("density", density_score, 0.2))
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            weighted_score = sum(score * weight for _, score, weight in quality_factors)
            metrics["quality_score"] = round(weighted_score, 3)
            
            # è¯†åˆ«è´¨é‡é—®é¢˜
            issues = []
            if metrics["isolated_nodes"] > len(nodes) * 0.3:
                issues.append(f"å­¤ç«‹èŠ‚ç‚¹è¿‡å¤š: {metrics['isolated_nodes']}")
            if avg_entity_confidence < 0.6:
                issues.append(f"å®ä½“ç½®ä¿¡åº¦è¾ƒä½: {avg_entity_confidence:.2f}")
            if avg_rel_confidence < 0.6:
                issues.append(f"å…³ç³»ç½®ä¿¡åº¦è¾ƒä½: {avg_rel_confidence:.2f}")
            if metrics["density"] < 0.05:
                issues.append("å›¾è°±è¿æ¥ç¨€ç–")
            
            metrics["issues"] = issues
            
        except Exception as e:
            logger.error(f"å›¾è°±è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            metrics["issues"].append(f"è´¨é‡è¯„ä¼°é”™è¯¯: {str(e)}")
        
        return metrics
    
    async def deduplicate_graph_data(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """å»é‡å›¾è°±æ•°æ®
        
        Args:
            graph_data: åŸå§‹å›¾è°±æ•°æ®
            
        Returns:
            å»é‡åçš„å›¾è°±æ•°æ®
        """
        logger.info("å¼€å§‹å›¾è°±æ•°æ®å»é‡")
        
        try:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            # å»é‡èŠ‚ç‚¹
            seen_nodes = set()
            deduplicated_nodes = []
            
            for node in nodes:
                node_key = (node.get("name", "").lower(), node.get("type", ""))
                if node_key not in seen_nodes:
                    seen_nodes.add(node_key)
                    deduplicated_nodes.append(node)
            
            # å»é‡è¾¹
            seen_edges = set()
            deduplicated_edges = []
            
            for edge in edges:
                edge_key = (
                    edge.get("source_name", "").lower(),
                    edge.get("target_name", "").lower(),
                    edge.get("type", "")
                )
                if edge_key not in seen_edges:
                    seen_edges.add(edge_key)
                    deduplicated_edges.append(edge)
            
            # æ›´æ–°å›¾è°±æ•°æ®
            graph_data["nodes"] = deduplicated_nodes
            graph_data["edges"] = deduplicated_edges
            graph_data["metadata"]["total_nodes"] = len(deduplicated_nodes)
            graph_data["metadata"]["total_edges"] = len(deduplicated_edges)
            
            removed_nodes = len(nodes) - len(deduplicated_nodes)
            removed_edges = len(edges) - len(deduplicated_edges)
            
            logger.info(f"å»é‡å®Œæˆï¼šç§»é™¤ {removed_nodes} ä¸ªé‡å¤èŠ‚ç‚¹ï¼Œ{removed_edges} æ¡é‡å¤è¾¹")
            return graph_data
            
        except Exception as e:
            logger.error(f"å›¾è°±æ•°æ®å»é‡å¤±è´¥: {str(e)}")
            return graph_data
    
    async def validate_graph_integrity(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å›¾è°±æ•°æ®å®Œæ•´æ€§
        
        Args:
            graph_data: å›¾è°±æ•°æ®
            
        Returns:
            éªŒè¯ç»“æœ
        """
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": []
        }
        
        try:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            # æ£€æŸ¥èŠ‚ç‚¹
            node_ids = set()
            for i, node in enumerate(nodes):
                if not node.get("id"):
                    validation_result["errors"].append(f"èŠ‚ç‚¹ {i} ç¼ºå°‘ID")
                    validation_result["is_valid"] = False
                else:
                    if node["id"] in node_ids:
                        validation_result["errors"].append(f"é‡å¤çš„èŠ‚ç‚¹ID: {node['id']}")
                        validation_result["is_valid"] = False
                    node_ids.add(node["id"])
                
                if not node.get("name"):
                    validation_result["warnings"].append(f"èŠ‚ç‚¹ {node.get('id', i)} ç¼ºå°‘åç§°")
                
                if not node.get("type"):
                    validation_result["warnings"].append(f"èŠ‚ç‚¹ {node.get('id', i)} ç¼ºå°‘ç±»å‹")
            
            # æ£€æŸ¥è¾¹
            edge_ids = set()
            for i, edge in enumerate(edges):
                if not edge.get("id"):
                    validation_result["errors"].append(f"è¾¹ {i} ç¼ºå°‘ID")
                    validation_result["is_valid"] = False
                else:
                    if edge["id"] in edge_ids:
                        validation_result["errors"].append(f"é‡å¤çš„è¾¹ID: {edge['id']}")
                        validation_result["is_valid"] = False
                    edge_ids.add(edge["id"])
                
                # æ£€æŸ¥è¾¹çš„èŠ‚ç‚¹å¼•ç”¨
                source_id = edge.get("source_id")
                target_id = edge.get("target_id")
                
                if not source_id:
                    validation_result["errors"].append(f"è¾¹ {edge.get('id', i)} ç¼ºå°‘æºèŠ‚ç‚¹ID")
                    validation_result["is_valid"] = False
                elif source_id not in node_ids:
                    validation_result["errors"].append(f"è¾¹ {edge.get('id', i)} å¼•ç”¨äº†ä¸å­˜åœ¨çš„æºèŠ‚ç‚¹: {source_id}")
                    validation_result["is_valid"] = False
                
                if not target_id:
                    validation_result["errors"].append(f"è¾¹ {edge.get('id', i)} ç¼ºå°‘ç›®æ ‡èŠ‚ç‚¹ID")
                    validation_result["is_valid"] = False
                elif target_id not in node_ids:
                    validation_result["errors"].append(f"è¾¹ {edge.get('id', i)} å¼•ç”¨äº†ä¸å­˜åœ¨çš„ç›®æ ‡èŠ‚ç‚¹: {target_id}")
                    validation_result["is_valid"] = False
                
                if not edge.get("type"):
                    validation_result["warnings"].append(f"è¾¹ {edge.get('id', i)} ç¼ºå°‘å…³ç³»ç±»å‹")
            
        except Exception as e:
            validation_result["errors"].append(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            validation_result["is_valid"] = False
        
        return validation_result
    
    def _prepare_multi_chunk_entity_relationships(self, entities: List[Entity], 
                                                 chunk_nodes_data: List[Dict[str, Any]],
                                                 chunks: Optional[List[DocumentChunk]] = None) -> List[Dict[str, Any]]:
        """åŸºäºå®ä½“çš„chunkæ˜ å°„ä¿¡æ¯åˆ›å»ºå¤šé‡HAS_ENTITYå…³ç³»
        
        Args:
            entities: å»é‡åçš„å®ä½“åˆ—è¡¨ï¼ˆåŒ…å«chunk_idså±æ€§ï¼‰
            chunk_nodes_data: ChunkèŠ‚ç‚¹æ•°æ®åˆ—è¡¨
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨ï¼ˆç”¨äºå»ºç«‹æ˜ å°„å…³ç³»ï¼‰
            
        Returns:
            å¤šé‡Chunk-Entityå…³ç³»æ•°æ®åˆ—è¡¨
        """
        try:
            chunk_entity_relationships = []
            
            # åˆ›å»ºchunk_idåˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
            chunk_id_to_node_id = {}
            for chunk_node in chunk_nodes_data:
                chunk_id = chunk_node["properties"]["chunk_id"]
                chunk_index = chunk_node["properties"]["chunk_index"]
                chunk_id_to_node_id[chunk_id] = chunk_node["id"]
                # ä¹Ÿæ”¯æŒé€šè¿‡chunk_indexæ˜ å°„ï¼ˆå¤‡ç”¨ï¼‰
                chunk_id_to_node_id[chunk_index] = chunk_node["id"]
            
            # éå†å®ä½“ï¼Œä¸ºæ¯ä¸ªchunkåˆ›å»ºå…³ç³»
            for entity in entities:
                try:
                    # è·å–å®ä½“å‡ºç°çš„æ‰€æœ‰chunk_ids
                    chunk_ids = entity.properties.get('chunk_ids', [])
                    
                    if not chunk_ids:
                        # å¦‚æœæ²¡æœ‰chunk_idså±æ€§ï¼Œå°è¯•ä»å®ä½“IDä¸­æå–ï¼ˆå‘åå…¼å®¹ï¼‰
                        chunk_id = self._extract_chunk_info_from_entity_id(entity.id)
                        if chunk_id:
                            chunk_ids = [chunk_id]
                    
                    # ä¸ºæ¯ä¸ªchunkåˆ›å»ºHAS_ENTITYå…³ç³»
                    for chunk_id in chunk_ids:
                        chunk_node_id = chunk_id_to_node_id.get(chunk_id)
                        
                        if chunk_node_id:
                            # ç”Ÿæˆå®ä½“çš„èŠ‚ç‚¹ID
                            entity_node_id = self._generate_node_id(entity.name, entity.type)
                            
                            # åˆ›å»ºå…³ç³»æ•°æ®
                            relationship_data = {
                                "id": f"rel_chunk_entity_{chunk_id}_{entity_node_id}",
                                "source_id": chunk_node_id,
                                "target_id": entity_node_id,
                                "type": "HAS_ENTITY",
                                "properties": {
                                    "created_at": datetime.now().isoformat(),
                                    "entity_name": entity.name,
                                    "entity_type": entity.type,
                                    "confidence": entity.confidence,
                                    "chunk_id": chunk_id
                                }
                            }
                            chunk_entity_relationships.append(relationship_data)
                            
                            logger.debug(f"å‡†å¤‡Chunk-Entityå…³ç³»: {chunk_node_id} -> {entity_node_id} ({entity.name}) for chunk {chunk_id}")
                        else:
                            logger.warning(f"æ‰¾ä¸åˆ°chunk {chunk_id} å¯¹åº”çš„èŠ‚ç‚¹ID")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†å®ä½“ {entity.name} çš„å¤šchunkå…³è”æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            logger.info(f"æˆåŠŸå‡†å¤‡ {len(chunk_entity_relationships)} ä¸ªå¤šé‡Chunk-Entityå…³ç³»")
            return chunk_entity_relationships
            
        except Exception as e:
            logger.error(f"å‡†å¤‡å¤šé‡Chunk-Entityå…³ç³»å¤±è´¥: {str(e)}")
            return []
    
    def _prepare_chunk_entity_relationships(self, entities: List[Entity], 
                                           chunk_nodes_data: List[Dict[str, Any]],
                                           chunks: Optional[List[DocumentChunk]] = None) -> List[Dict[str, Any]]:
        """å‡†å¤‡Chunk-Entityå…³ç³»æ•°æ®
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            chunk_nodes_data: ChunkèŠ‚ç‚¹æ•°æ®åˆ—è¡¨
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨ï¼ˆç”¨äºå»ºç«‹æ˜ å°„å…³ç³»ï¼‰
            
        Returns:
            Chunk-Entityå…³ç³»æ•°æ®åˆ—è¡¨
        """
        try:
            chunk_entity_relationships = []
            
            # åˆ›å»ºchunk_idåˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
            chunk_id_to_node_id = {}
            for chunk_node in chunk_nodes_data:
                chunk_id = chunk_node["properties"]["chunk_id"]
                chunk_index = chunk_node["properties"]["chunk_index"]
                chunk_id_to_node_id[chunk_id] = chunk_node["id"]
                # ä¹Ÿæ”¯æŒé€šè¿‡chunk_indexæ˜ å°„ï¼ˆå¤‡ç”¨ï¼‰
                chunk_id_to_node_id[chunk_index] = chunk_node["id"]
            
            # éå†å®ä½“ï¼Œå»ºç«‹ä¸chunkçš„å…³è”
            for entity in entities:
                try:
                    # ä»å®ä½“IDä¸­æå–chunkä¿¡æ¯
                    chunk_id = self._extract_chunk_info_from_entity_id(entity.id)
                    
                    if chunk_id is not None:
                        # æŸ¥æ‰¾å¯¹åº”çš„chunkèŠ‚ç‚¹ID
                        chunk_node_id = chunk_id_to_node_id.get(chunk_id)
                        
                        if chunk_node_id:
                            # ç”Ÿæˆå®ä½“çš„èŠ‚ç‚¹ID
                            entity_node_id = self._generate_node_id(entity.name, entity.type)
                            
                            # åˆ›å»ºå…³ç³»æ•°æ®
                            relationship_data = {
                                "id": f"rel_chunk_entity_{chunk_id}_{entity_node_id}",
                                "source_id": chunk_node_id,
                                "target_id": entity_node_id,
                                "type": "HAS_ENTITY",
                                "properties": {
                                    "created_at": datetime.now().isoformat(),
                                    "entity_name": entity.name,
                                    "entity_type": entity.type,
                                    "confidence": entity.confidence
                                }
                            }
                            chunk_entity_relationships.append(relationship_data)
                            
                            logger.debug(f"å‡†å¤‡Chunk-Entityå…³ç³»: {chunk_node_id} -> {entity_node_id} ({entity.name})")
                        else:
                            logger.warning(f"æ‰¾ä¸åˆ°chunk {chunk_id} å¯¹åº”çš„èŠ‚ç‚¹ID")
                    else:
                        logger.warning(f"æ— æ³•ä»å®ä½“ID {entity.id} ä¸­æå–chunkä¿¡æ¯")
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†å®ä½“ {entity.name} çš„chunkå…³è”æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            logger.info(f"æˆåŠŸå‡†å¤‡ {len(chunk_entity_relationships)} ä¸ªChunk-Entityå…³ç³»")
            return chunk_entity_relationships
            
        except Exception as e:
            logger.error(f"å‡†å¤‡Chunk-Entityå…³ç³»å¤±è´¥: {str(e)}")
            return []
    
    def _prepare_document_chunk_relationships(self, chunks: List[DocumentChunk], document_node_id: str) -> List[Dict[str, Any]]:
        """åˆ›å»ºDocumentçš„FIRST_CHUNKå…³ç³»æ•°æ®
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            document_node_id: DocumentèŠ‚ç‚¹çš„ID
            
        Returns:
            Document-Chunkå…³ç³»æ•°æ®åˆ—è¡¨
        """
        try:
            document_chunk_relationships = []
            
            if chunks:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªchunkï¼ˆchunk_index=0ï¼‰
                first_chunk = None
                for chunk in chunks:
                    if chunk.metadata.chunk_index == 0:
                        first_chunk = chunk
                        break
                
                if first_chunk:
                    # åˆ›å»ºFIRST_CHUNKå…³ç³»
                    first_chunk_node_id = f"chunk_{first_chunk.metadata.chunk_id}"
                    
                    relationship_data = {
                        "id": f"rel_first_chunk_{document_node_id}_{first_chunk_node_id}",
                        "source_id": document_node_id,
                        "target_id": first_chunk_node_id,
                        "type": "FIRST_CHUNK",
                        "properties": {
                            "created_at": datetime.now().isoformat(),
                            "chunk_index": first_chunk.metadata.chunk_index,
                            "chunk_id": first_chunk.metadata.chunk_id
                        }
                    }
                    document_chunk_relationships.append(relationship_data)
                    
                    logger.debug(f"å‡†å¤‡Document-FIRST_CHUNKå…³ç³»: {document_node_id} -> {first_chunk_node_id}")
                else:
                    logger.warning("æœªæ‰¾åˆ°ç¬¬ä¸€ä¸ªchunkï¼ˆchunk_index=0ï¼‰")
            
            logger.info(f"æˆåŠŸå‡†å¤‡ {len(document_chunk_relationships)} ä¸ªDocument-FIRST_CHUNKå…³ç³»")
            return document_chunk_relationships
            
        except Exception as e:
            logger.error(f"å‡†å¤‡Document-FIRST_CHUNKå…³ç³»å¤±è´¥: {str(e)}")
            return []
    
    def _prepare_chunk_sequence_relationships(self, chunks: List[DocumentChunk]) -> List[Dict[str, Any]]:
        """åˆ›å»ºChunkä¹‹é—´çš„NEXT_CHUNKå…³ç³»æ•°æ®
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨ï¼ˆåº”è¯¥æŒ‰chunk_indexæ’åºï¼‰
            
        Returns:
            Chunkåºåˆ—å…³ç³»æ•°æ®åˆ—è¡¨
        """
        try:
            chunk_sequence_relationships = []
            
            if len(chunks) > 1:
                # æŒ‰chunk_indexæ’åºç¡®ä¿é¡ºåºæ­£ç¡®
                sorted_chunks = sorted(chunks, key=lambda x: x.metadata.chunk_index)
                
                # åˆ›å»ºç›¸é‚»chunkä¹‹é—´çš„NEXT_CHUNKå…³ç³»
                for i in range(len(sorted_chunks) - 1):
                    current_chunk = sorted_chunks[i]
                    next_chunk = sorted_chunks[i + 1]
                    
                    current_chunk_node_id = f"chunk_{current_chunk.metadata.chunk_id}"
                    next_chunk_node_id = f"chunk_{next_chunk.metadata.chunk_id}"
                    
                    relationship_data = {
                        "id": f"rel_next_chunk_{current_chunk_node_id}_{next_chunk_node_id}",
                        "source_id": current_chunk_node_id,
                        "target_id": next_chunk_node_id,
                        "type": "NEXT_CHUNK",
                        "properties": {
                            "created_at": datetime.now().isoformat(),
                            "current_chunk_index": current_chunk.metadata.chunk_index,
                            "next_chunk_index": next_chunk.metadata.chunk_index,
                            "current_chunk_id": current_chunk.metadata.chunk_id,
                            "next_chunk_id": next_chunk.metadata.chunk_id
                        }
                    }
                    chunk_sequence_relationships.append(relationship_data)
                    
                    logger.debug(f"å‡†å¤‡Chunk-NEXT_CHUNKå…³ç³»: {current_chunk_node_id} -> {next_chunk_node_id}")
            
            logger.info(f"æˆåŠŸå‡†å¤‡ {len(chunk_sequence_relationships)} ä¸ªChunk-NEXT_CHUNKå…³ç³»")
            return chunk_sequence_relationships
            
        except Exception as e:
            logger.error(f"å‡†å¤‡Chunk-NEXT_CHUNKå…³ç³»å¤±è´¥: {str(e)}")
            return [] 