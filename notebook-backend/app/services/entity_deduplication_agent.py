# -*- coding: utf-8 -*-
"""
å®ä½“å»é‡Agent
ä½¿ç”¨LangGraphå®ç°åŸºäºWikipediaæœç´¢çš„æ™ºèƒ½å®ä½“å»é‡
"""
import json
import logging
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from app.services.llm_client_service import LLMClientService
from app.services.wikipedia_mcp_server import get_wikipedia_mcp_server
from app.models.agent_state import EntityAnalysisState, AgentConfig, EntityPair, SearchResult, MergeDecision

logger = logging.getLogger(__name__)

class EntityDeduplicationAgent:
    """å®ä½“å»é‡Agent"""
    
    def __init__(self, config: Optional[AgentConfig] = None):
        """
        åˆå§‹åŒ–å®ä½“å»é‡Agent
        
        Args:
            config: Agenté…ç½®
        """
        self.config = config or AgentConfig()
        
        # éªŒè¯é…ç½®
        if not self.config.validate_config():
            raise ValueError("Agenté…ç½®æ— æ•ˆ")
        
        # ä½¿ç”¨ç°æœ‰çš„LLMæœåŠ¡åˆ›å»º
        llm_service = LLMClientService()
        self.llm = llm_service.get_processing_llm(streaming=False)
        
        # åˆ›å»ºWikipedia MCPæœåŠ¡å™¨
        self.wikipedia_server = get_wikipedia_mcp_server()
        
        logger.info("å®ä½“å»é‡Agentåˆå§‹åŒ–å®Œæˆ")
    
    def deduplicate_entities(self, entities: List[Dict[str, Any]], entity_type: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®ä½“å»é‡åˆ†æ
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            entity_type: å®ä½“ç±»å‹
            
        Returns:
            å»é‡åˆ†æç»“æœ
        """
        # åˆå§‹åŒ–çŠ¶æ€
        state = EntityAnalysisState(
            entities=entities,
            entity_type=entity_type,
            total_entities=len(entities),
            started_at=datetime.now()
        )
        
        try:
            logger.info(f"å¼€å§‹Agentå»é‡åˆ†æï¼š{entity_type} ç±»å‹ï¼Œ{len(entities)} ä¸ªå®ä½“")
            
            # æ‰§è¡Œä¸‰é˜¶æ®µåˆ†æ
            state = self._initial_analysis(state)
            state = self._search_verification(state)
            state = self._final_decision(state)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            state.processing_time = (datetime.now() - state.started_at).total_seconds()
            state.processing_step = "complete"
            
            # è¿”å›æ ‡å‡†æ ¼å¼ç»“æœ
            return self._format_result(state)
            
        except Exception as e:
            error_msg = f"Agentå»é‡åˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state.add_error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            return self._format_error_result(state)
    
    def _initial_analysis(self, state: EntityAnalysisState) -> EntityAnalysisState:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šåˆæ­¥ç›¸ä¼¼æ€§åˆ†æ
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            state.processing_step = "initial"
            logger.info("å¼€å§‹åˆæ­¥ç›¸ä¼¼æ€§åˆ†æ")
            
            # æ„å»ºåˆæ­¥åˆ†æPrompt
            prompt = self._build_initial_analysis_prompt(state.entities, state.entity_type)
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            response = self.llm.invoke(prompt)
            response_content = response.content
            
            # è§£æå“åº”
            analysis_result = self._parse_initial_analysis(response_content)
            state.initial_analysis = analysis_result
            
            # æå–å®ä½“å¯¹å¹¶å¼ºåˆ¶æ›´å¤šè¿›å…¥éªŒè¯
            if "entity_pairs" in analysis_result:
                for pair_data in analysis_result["entity_pairs"]:
                    original_confidence = pair_data["confidence"]
                    
                    # å¼ºåˆ¶æ›´ä¿å®ˆçš„éªŒè¯ç­–ç•¥ï¼š
                    # åªæœ‰æå°‘æ•°æ˜æ˜¾ç›¸åŒçš„æ‰è·³è¿‡éªŒè¯
                    if original_confidence == "high":
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦çœŸçš„åº”è¯¥æ˜¯high confidence
                        entity1_name = pair_data["entity1_name"]
                        entity2_name = pair_data["entity2_name"]
                        
                        # åªæœ‰è¿™äº›æƒ…å†µæ‰ä¿æŒhigh confidenceï¼š
                        # 1. å®Œå…¨ç›¸åŒçš„åç§°
                        # 2. æ˜æ˜¾çš„ç¼©å†™å…³ç³»ï¼ˆå¦‚ IBM vs International Business Machinesï¼‰
                        is_truly_high_confidence = (
                            entity1_name.lower() == entity2_name.lower() or
                            (len(entity1_name) <= 5 and entity1_name.upper() in entity2_name.upper()) or
                            (len(entity2_name) <= 5 and entity2_name.upper() in entity1_name.upper())
                        )
                        
                        if not is_truly_high_confidence:
                            # é™çº§åˆ°mediumï¼Œå¼ºåˆ¶è¿›å…¥WikipediaéªŒè¯
                            logger.info(f"é™çº§ç½®ä¿¡åº¦: {entity1_name} vs {entity2_name} ä» high -> medium")
                            original_confidence = "medium"
                    
                    entity_pair = EntityPair(
                        entity1_index=pair_data["entity1_index"],
                        entity2_index=pair_data["entity2_index"],
                        entity1_name=pair_data["entity1_name"],
                        entity2_name=pair_data["entity2_name"],
                        confidence=original_confidence,
                        similarity_score=pair_data.get("similarity_score", 0.0),
                        reason=pair_data.get("reason", ""),
                        needs_verification=original_confidence in ["medium", "low"]  # ä½¿ç”¨è°ƒæ•´åçš„ç½®ä¿¡åº¦
                    )
                    state.entity_pairs.append(entity_pair)
            
            state.pairs_analyzed = len(state.entity_pairs)
            
            logger.info(f"åˆæ­¥åˆ†æå®Œæˆï¼šå‘ç° {len(state.entity_pairs)} ä¸ªå®ä½“å¯¹")
            return state
            
        except Exception as e:
            error_msg = f"åˆæ­¥åˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state.add_error(error_msg)
            return state
    
    def _search_verification(self, state: EntityAnalysisState) -> EntityAnalysisState:
        """
        ç¬¬äºŒé˜¶æ®µï¼šWikipediaæœç´¢éªŒè¯
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            state.processing_step = "search"
            logger.info("å¼€å§‹Wikipediaæœç´¢éªŒè¯")
            
            # æ‰¾å‡ºéœ€è¦éªŒè¯çš„å®ä½“å¯¹
            verification_pairs = [pair for pair in state.entity_pairs if pair.needs_verification]
            
            if not verification_pairs:
                logger.info("æ— éœ€è¿›è¡ŒWikipediaæœç´¢éªŒè¯")
                return state
            
            # æ”¶é›†éœ€è¦æœç´¢çš„å®ä½“
            entities_to_search = set()
            for pair in verification_pairs:
                entities_to_search.add(pair.entity1_index)
                entities_to_search.add(pair.entity2_index)
            
            # æœç´¢å®ä½“ä¿¡æ¯
            search_results = {}
            for entity_index in entities_to_search:
                entity = state.get_entity_by_index(entity_index)
                if entity:
                    try:
                        result = self._search_entity_info(entity, state.entity_type)
                        search_results[entity_index] = result
                        state.searches_performed += 1
                    except Exception as e:
                        logger.warning(f"æœç´¢å®ä½“ {entity_index} å¤±è´¥: {str(e)}")
                        state.add_warning(f"æœç´¢å®ä½“ {entity_index} å¤±è´¥: {str(e)}")
            
            state.search_results = search_results
            
            logger.info(f"Wikipediaæœç´¢å®Œæˆï¼šæœç´¢äº† {len(search_results)} ä¸ªå®ä½“")
            return state
            
        except Exception as e:
            error_msg = f"æœç´¢éªŒè¯å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state.add_error(error_msg)
            return state
    
    def _final_decision(self, state: EntityAnalysisState) -> EntityAnalysisState:
        """
        ç¬¬ä¸‰é˜¶æ®µï¼šåŸºäºæœç´¢ç»“æœçš„æœ€ç»ˆå†³ç­–
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            state.processing_step = "decision"
            logger.info("å¼€å§‹æœ€ç»ˆå†³ç­–åˆ†æ")
            
            # æ„å»ºæœ€ç»ˆå†³ç­–Prompt
            prompt = self._build_final_decision_prompt(state)
            
            # è°ƒç”¨LLMè¿›è¡Œæœ€ç»ˆå†³ç­–
            response = self.llm.invoke(prompt)
            response_content = response.content
            
            # è§£ææœ€ç»ˆå†³ç­–
            decision_result = self._parse_final_decision(response_content)
            state.final_decision = decision_result
            
            # æå–åˆå¹¶å†³ç­–
            if "merge_groups" in decision_result:
                for group_data in decision_result["merge_groups"]:
                    merge_decision = MergeDecision(
                        primary_entity_index=group_data["primary_entity_index"],
                        duplicate_indices=group_data["duplicate_indices"],
                        merged_name=group_data["merged_name"],
                        merged_description=group_data["merged_description"],
                        confidence=group_data["confidence"],
                        reason=group_data["reason"],
                        wikipedia_evidence=group_data.get("wikipedia_evidence", "")
                    )
                    state.merge_groups.append(merge_decision)
            
            # æå–ç‹¬ç«‹å®ä½“
            if "independent_entities" in decision_result:
                state.independent_entities = decision_result["independent_entities"]
            
            # æå–ä¸ç¡®å®šæ¡ˆä¾‹
            if "uncertain_cases" in decision_result:
                state.uncertain_cases = decision_result["uncertain_cases"]
            
            logger.info(f"æœ€ç»ˆå†³ç­–å®Œæˆï¼š{len(state.merge_groups)} ä¸ªåˆå¹¶ç»„ï¼Œ{len(state.independent_entities)} ä¸ªç‹¬ç«‹å®ä½“")
            return state
            
        except Exception as e:
            error_msg = f"æœ€ç»ˆå†³ç­–å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state.add_error(error_msg)
            return state
    
    def _search_entity_info(self, entity: Dict[str, Any], entity_type: str) -> SearchResult:
        """
        æœç´¢å•ä¸ªå®ä½“çš„Wikipediaä¿¡æ¯
        
        Args:
            entity: å®ä½“æ•°æ®
            entity_type: å®ä½“ç±»å‹
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            # è°ƒç”¨Wikipediaæœç´¢
            search_result = self.wikipedia_server.search_entity(
                entity_name=entity["name"],
                entity_type=entity_type
            )
            
            # è½¬æ¢ä¸ºSearchResultæ¨¡å‹
            result = SearchResult(
                entity_name=entity["name"],
                found=search_result.get("found", False),
                title=search_result.get("title"),
                summary=search_result.get("summary"),
                url=search_result.get("url"),
                categories=search_result.get("categories", []),
                entity_type=search_result.get("entity_type"),
                type_relevance=search_result.get("type_relevance", 0.0),
                disambiguation=search_result.get("disambiguation", False),
                options=search_result.get("options", []),
                error=search_result.get("error")
            )
            
            return result
            
        except Exception as e:
            logger.error(f"æœç´¢å®ä½“ä¿¡æ¯å¤±è´¥: {entity['name']}, é”™è¯¯: {str(e)}")
            return SearchResult(
                entity_name=entity["name"],
                found=False,
                error=str(e)
            )
    
    def _build_initial_analysis_prompt(self, entities: List[Dict[str, Any]], entity_type: str) -> str:
        """æ„å»ºåˆæ­¥åˆ†æPrompt - æåº¦ä¿å®ˆçš„è‹±æ–‡ç‰ˆæœ¬"""
        
        # å®ä½“ç±»å‹æ˜ å°„
        type_mapping = {
            "ç»„ç»‡": "Organization", "äººç‰©": "Person", "åœ°ç‚¹": "Location", 
            "äº§å“": "Product", "æŠ€æœ¯": "Technology", "æ—¶é—´": "Time", "äº‹ä»¶": "Event"
        }
        english_type = type_mapping.get(entity_type, entity_type)
        
        prompt = f"""You are an EXTREMELY CONSERVATIVE {english_type} entity deduplication expert.

CRITICAL PRINCIPLE: Only suggest merging if you are 100% certain they refer to the EXACT SAME real-world object.

â›” ABSOLUTELY NEVER MERGE:
- Different companies (Apple â‰  Google â‰  Microsoft â‰  Amazon â‰  Stanford University)
- Different people (Steve Jobs â‰  Tim Cook â‰  Sundar Pichai â‰  Satya Nadella)
- Competitors in same industry
- Different organization types (University â‰  Corporation â‰  Government)
- Similar but distinct entities
- Entities from different contexts without clear connection

âœ… ONLY CONSIDER MERGING:
- Exact same entity with different language names (Apple Inc â†” è‹¹æœå…¬å¸)
- Official name vs common abbreviation of SAME entity (International Business Machines â†” IBM)
- Clear aliases of identical entity with Wikipedia confirmation

CONFIDENCE LEVELS:
- high: 100% certain same entity (e.g., "Apple Inc" vs "Apple Incorporated") - USE VERY RARELY
- medium: Possible same entity, needs Wikipedia verification (e.g., Chinese vs English names)
- low: Uncertain, needs deep research and verification

TARGET: Maximum 10% of pairs should be 'high' confidence. Be extremely conservative.

Entity List:
"""
        
        for i, entity in enumerate(entities):
            prompt += f"{i+1}. **{entity['name']}**\n"
            prompt += f"   - Description: {entity.get('description', 'No description')}\n"
            prompt += f"   - Source: {entity.get('source_text', 'No source')[:100]}...\n"
            prompt += f"   - Properties: {entity.get('properties', {})}\n\n"
        
        prompt += """
Return JSON format analysis results:
```json
{
  "analysis_summary": "Conservative analysis summary",
  "entity_pairs": [
    {
      "entity1_index": 0,
      "entity2_index": 1,
      "entity1_name": "Entity 1 name",
      "entity2_name": "Entity 2 name", 
      "confidence": "medium",
      "similarity_score": 0.7,
      "reason": "Reason for potential merge"
    }
  ]
}
```

IMPORTANT: 
- Only include pairs with genuine merge possibility
- Default to 'medium' or 'low' confidence
- When in doubt, DON'T include the pair
- Be extremely conservative - better to miss a merge than create a wrong one"""
        
        return prompt
    
    def _build_final_decision_prompt(self, state: EntityAnalysisState) -> str:
        """æ„å»ºæœ€ç»ˆå†³ç­–Prompt - å¼ºåŒ–éªŒè¯çš„è‹±æ–‡ç‰ˆæœ¬"""
        
        # å®ä½“ç±»å‹æ˜ å°„
        type_mapping = {
            "ç»„ç»‡": "Organization", "äººç‰©": "Person", "åœ°ç‚¹": "Location", 
            "äº§å“": "Product", "æŠ€æœ¯": "Technology", "æ—¶é—´": "Time", "äº‹ä»¶": "Event"
        }
        english_type = type_mapping.get(state.entity_type, state.entity_type)
        
        prompt = f"""Based on initial analysis and Wikipedia verification, make FINAL {english_type} entity merge decisions.

ğŸš¨ ULTRA-CONSERVATIVE MERGE POLICY ğŸš¨

Initial Analysis Results:
{json.dumps(state.initial_analysis, ensure_ascii=False, indent=2)}

Wikipedia Verification Results:
"""
        
        for entity_index, search_result in state.search_results.items():
            prompt += f"\nEntity {entity_index} ({search_result.entity_name}) Wikipedia Info:\n"
            if search_result.found:
                prompt += f"- Title: {search_result.title}\n"
                prompt += f"- Summary: {search_result.summary[:200]}...\n"
                prompt += f"- URL: {search_result.url}\n"
                prompt += f"- Type Relevance: {search_result.type_relevance}\n"
            else:
                prompt += f"- No Wikipedia entry found\n"
                if search_result.error:
                    prompt += f"- Error: {search_result.error}\n"
        
        prompt += f"""
ğŸ”’ STRICT MERGE CONDITIONS - ALL MUST BE TRUE:
1. Wikipedia EXPLICITLY confirms they are the SAME entity
2. One redirects to the other OR explicitly states they are aliases
3. NO contradictory information found
4. Confidence â‰¥ 0.95 (95% certainty)
5. Common sense verification passes

âŒ IMMEDIATELY REJECT IF:
- Different Wikipedia pages exist for both entities
- No Wikipedia confirmation found
- Any conflicting information
- Different organization types
- Competitors/rivals
- Different people with similar roles
- ANY doubt whatsoever

ğŸ” PERFORM CONTRADICTION CHECK:
Before suggesting any merge, actively look for reasons NOT to merge:
- Are they competitors?
- Do they have different roles/functions?
- Are they from different time periods?
- Do they belong to different categories?

âš–ï¸ FINAL DECISION RULE:
When in doubt, KEEP SEPARATE. 
Better to have duplicates than wrong merges.

Return JSON format final decision:
```json
{{
  "decision_summary": "Ultra-conservative merge analysis with contradiction checks",
  "merge_groups": [
    {{
      "primary_entity_index": 0,
      "duplicate_indices": [1],
      "merged_name": "Verified merged name",
      "merged_description": "Verified merged description", 
      "confidence": 0.95,
      "reason": "Wikipedia explicitly confirms same entity",
      "wikipedia_evidence": "Specific Wikipedia evidence",
      "contradiction_check": "Verified no contradictions exist"
    }}
  ],
  "independent_entities": [2, 3, 4, 5],
  "uncertain_cases": [
    {{
      "entities": [6, 7], 
      "reason": "Insufficient evidence for safe merging"
    }}
  ]
}}
```

ğŸ¯ EXPECTED RESULT: Most entities should remain independent unless there is overwhelming evidence they are identical."""
        
        return prompt
    
    def _parse_initial_analysis(self, response_content: str) -> Dict[str, Any]:
        """è§£æåˆæ­¥åˆ†æå“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = self._extract_json_from_response(response_content)
            if json_match:
                return json.loads(json_match)
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
                
        except Exception as e:
            logger.error(f"è§£æåˆæ­¥åˆ†æå“åº”å¤±è´¥: {str(e)}")
            return {"analysis_summary": "è§£æå¤±è´¥", "entity_pairs": []}
    
    def _parse_final_decision(self, response_content: str) -> Dict[str, Any]:
        """è§£ææœ€ç»ˆå†³ç­–å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = self._extract_json_from_response(response_content)
            if json_match:
                return json.loads(json_match)
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
                
        except Exception as e:
            logger.error(f"è§£ææœ€ç»ˆå†³ç­–å“åº”å¤±è´¥: {str(e)}")
            return {
                "decision_summary": "è§£æå¤±è´¥",
                "merge_groups": [],
                "independent_entities": list(range(len(self.entities))),
                "uncertain_cases": []
            }
    
    def _extract_json_from_response(self, response_content: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
        import re
        
        # å°è¯•æå–```json...```æ ¼å¼
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_content, re.DOTALL)
        if json_match:
            return json_match.group(1)
        
        # å°è¯•ç›´æ¥æŸ¥æ‰¾JSONå¯¹è±¡
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_content, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        return None
    
    def _format_result(self, state: EntityAnalysisState) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æœ€ç»ˆç»“æœ"""
        return {
            "analysis_summary": state.final_decision.get("decision_summary", "Agentåˆ†æå®Œæˆ") if state.final_decision else "Agentåˆ†æå®Œæˆ",
            "merge_groups": [
                {
                    "primary_entity": str(group.primary_entity_index + 1),
                    "duplicates": [str(idx + 1) for idx in group.duplicate_indices],
                    "merged_name": group.merged_name,
                    "merged_description": group.merged_description,
                    "confidence": group.confidence,
                    "reason": group.reason
                }
                for group in state.merge_groups
            ],
            "independent_entities": [str(idx + 1) for idx in state.independent_entities],
            "uncertain_cases": state.uncertain_cases,
            "statistics": state.get_processing_summary(),
            "errors": state.errors,
            "warnings": state.warnings
        }
    
    def _format_error_result(self, state: EntityAnalysisState) -> Dict[str, Any]:
        """æ ¼å¼åŒ–é”™è¯¯ç»“æœ"""
        return {
            "analysis_summary": "Agentåˆ†æå¤±è´¥",
            "merge_groups": [],
            "independent_entities": [str(i + 1) for i in range(len(state.entities))],
            "uncertain_cases": [],
            "statistics": state.get_processing_summary(),
            "errors": state.errors,
            "warnings": state.warnings
        }


# å…¨å±€å®ä¾‹
_agent_instance = None

def get_entity_deduplication_agent(config: Optional[AgentConfig] = None) -> EntityDeduplicationAgent:
    """è·å–å®ä½“å»é‡Agentå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _agent_instance
    if _agent_instance is None:
        _agent_instance = EntityDeduplicationAgent(config)
    return _agent_instance