import logging
import json
import re
import asyncio
import hashlib
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass
from langchain_core.messages import HumanMessage
from app.core.config import settings
from app.services.llm_client_service import LLMClientService

# ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„Entityå’ŒRelationshipæ¨¡å‹
from app.models.entity import Entity, Relationship, KnowledgeExtractionResult

logger = logging.getLogger(__name__)

class KnowledgeExtractionService:
    """çŸ¥è¯†æŠ½å–æœåŠ¡
    
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»æ–‡æ¡£åˆ†å—ä¸­åŒæ—¶æŠ½å–å®ä½“å’Œå…³ç³»
    æŠ½å–åçš„å®ä½“ç›´æ¥å…¥åº“ï¼Œç”±åç»­å…¨å±€ç»Ÿä¸€ä»»åŠ¡è¿›è¡Œå»é‡å¤„ç†
    """
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†æŠ½å–æœåŠ¡"""
        self.llm_service = LLMClientService()
        self.entity_types = self._load_entity_types()
        self.relationship_types = self._load_relationship_types()
        logger.info("çŸ¥è¯†æŠ½å–æœåŠ¡å·²åˆå§‹åŒ– - å®ä½“ç›´æ¥å…¥åº“æ¨¡å¼")
    
    def _load_entity_types(self) -> List[str]:
        """åŠ è½½å®ä½“ç±»å‹é…ç½®"""
        return [
            "äººç‰©", "ç»„ç»‡", "åœ°ç‚¹", "äº‹ä»¶", "æ¦‚å¿µ", 
            "æŠ€æœ¯", "äº§å“", "æ—¶é—´", "æ•°å­—", "æ³•å¾‹æ¡æ–‡",
            "æ”¿ç­–", "é¡¹ç›®", "ç³»ç»Ÿ", "æ–¹æ³•", "ç†è®º"
        ]
    
    def _load_relationship_types(self) -> List[str]:
        """åŠ è½½å…³ç³»ç±»å‹é…ç½®"""
        return [
            "å±äº", "åŒ…å«", "ä½äº", "å·¥ä½œäº", "åˆ›ç«‹", "ç®¡ç†",
            "åˆä½œ", "æåŠ", "æè¿°", "å¼•ç”¨", "å¯¼è‡´", "å½±å“",
            "ä½¿ç”¨", "ä¾èµ–", "å®ç°", "ç›¸å…³", "è¿æ¥", "å…³è”"
        ]
    
    async def extract_knowledge_from_chunks(self, chunks: List[Dict[str, Any]]) -> Tuple[List[Entity], List[Relationship]]:
        """ä»æ–‡æ¡£åˆ†å—ä¸­æŠ½å–çŸ¥è¯†ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰"""
        logger.info(f"å¼€å§‹ä» {len(chunks)} ä¸ªåˆ†å—ä¸­æŠ½å–çŸ¥è¯†")
        
        all_entities = []
        all_relationships = []
        
        try:
            # å¤„ç†æ¯ä¸ªåˆ†å—
            for i, chunk in enumerate(chunks):
                chunk_content = chunk.get('content', '')
                if not chunk_content.strip():
                    continue
                
                logger.info(f"å¤„ç†åˆ†å— {i+1}/{len(chunks)}")
                
                # ä»å•ä¸ªåˆ†å—æŠ½å–çŸ¥è¯†
                result = await self._extract_knowledge_from_text(
                    text=chunk_content,
                    chunk_id=chunk.get('id', f'chunk_{i}'),
                    chunk_index=i,
                    chunk_metadata=chunk
                )
                
                if result.success:
                    all_entities.extend(result.entities)
                    all_relationships.extend(result.relationships)
                else:
                    logger.warning(f"åˆ†å— {i} çŸ¥è¯†æŠ½å–å¤±è´¥: {result.error_message}")
                
                # é¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
                if i < len(chunks) - 1:
                    await asyncio.sleep(0.1)
            
            # ğŸ”„ ç§»é™¤æ–‡æ¡£å†…å®ä½“ç»Ÿä¸€ï¼Œå®ä½“ç›´æ¥å‡†å¤‡å…¥åº“
            logger.info("å®ä½“æŠ½å–å®Œæˆï¼Œå‡†å¤‡ç›´æ¥å…¥åº“ï¼ˆç”±åç»­å…¨å±€ç»Ÿä¸€ä»»åŠ¡å¤„ç†å»é‡ï¼‰")
            
            # ä¸ºåŸå§‹å®ä½“åˆ›å»ºchunkæ˜ å°„ï¼ˆä¸è¿›è¡Œç»Ÿä¸€ï¼‰
            entity_chunk_mapping = self._create_chunk_mapping_for_raw_entities(all_entities)
            
            # è¿‡æ»¤å…³ç³»ï¼ˆåŸºäºåŸå§‹å®ä½“ï¼‰
            filtered_relationships = self._filter_relationships(all_relationships, all_entities)
            
            # å°†chunkæ˜ å°„ä¿¡æ¯æ·»åŠ åˆ°å®ä½“å±æ€§ä¸­
            for entity in all_entities:
                entity_key = (self._normalize_entity_name(entity.name), entity.type)
                chunk_ids = entity_chunk_mapping.get(entity_key, [])
                entity.properties['chunk_ids'] = chunk_ids
                entity.properties['appears_in_chunks_count'] = len(chunk_ids)
            
            logger.info(f"çŸ¥è¯†æŠ½å–å®Œæˆï¼šå®ä½“ {len(all_entities)}ï¼Œå…³ç³» {len(all_relationships)} -> {len(filtered_relationships)}")
            
            # ğŸ†• è§¦å‘æ–‡æ¡£è§£æåçš„å…¨å±€å®ä½“ç»Ÿä¸€ä»»åŠ¡ï¼ˆä½¿ç”¨LangGraph Agentï¼‰
            await self._trigger_post_extraction_unification(all_entities, chunks)
            
            return all_entities, filtered_relationships
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†æŠ½å–å¤±è´¥: {str(e)}")
            raise
    
    async def _extract_knowledge_from_text(self, text: str, chunk_id: str, 
                                         chunk_index: int, chunk_metadata: Dict[str, Any]) -> KnowledgeExtractionResult:
        """ä»å•ä¸ªæ–‡æœ¬ä¸­æŠ½å–çŸ¥è¯†"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_knowledge_extraction_prompt(text)
            
            # è·å–LLMå®ä¾‹
            llm = self.llm_service.get_processing_llm(streaming=False)
            
            # è°ƒç”¨LLM
            message = HumanMessage(content=prompt)
            response = await llm.ainvoke([message])
            
            # è·å–å“åº”å†…å®¹
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æå“åº”
            entities, relationships = self._parse_knowledge_response(
                response_content, text, chunk_id, chunk_index, chunk_metadata
            )
            
            logger.info(f"ä»åˆ†å— {chunk_index} æŠ½å–åˆ° {len(entities)} ä¸ªå®ä½“ï¼Œ{len(relationships)} ä¸ªå…³ç³»")
            
            return KnowledgeExtractionResult(
                entities=entities,
                relationships=relationships,
                chunk_id=chunk_id,
                chunk_index=chunk_index,
                success=True
            )
            
        except Exception as e:
            logger.error(f"ä»æ–‡æœ¬æŠ½å–çŸ¥è¯†å¤±è´¥: {str(e)}")
            return KnowledgeExtractionResult(
                entities=[],
                relationships=[],
                chunk_id=chunk_id,
                chunk_index=chunk_index,
                success=False,
                error_message=str(e)
            )
    
    def _build_knowledge_extraction_prompt(self, text: str) -> str:
        """æ„å»ºçŸ¥è¯†æŠ½å–æç¤ºè¯ï¼ˆåŒæ—¶æŠ½å–å®ä½“å’Œå…³ç³»ï¼‰"""
        entity_types_str = "ã€".join(self.entity_types)
        relationship_types_str = "ã€".join(self.relationship_types)
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ä¿¡æ¯ã€‚

æ”¯æŒçš„å®ä½“ç±»å‹ï¼š{entity_types_str}
æ”¯æŒçš„å…³ç³»ç±»å‹ï¼š{relationship_types_str}

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒæ—¶åŒ…å«å®ä½“å’Œå…³ç³»ä¿¡æ¯ï¼š

```json
{{
    "entities": [
        {{
            "name": "å®ä½“åç§°",
            "type": "å®ä½“ç±»å‹",
            "description": "å®ä½“æè¿°",
            "properties": {{}},
            "confidence": 0.95,
            "start_pos": 0,
            "end_pos": 10
        }}
    ],
    "relationships": [
        {{
            "source_entity": "æºå®ä½“åç§°",
            "target_entity": "ç›®æ ‡å®ä½“åç§°",
            "relationship_type": "å…³ç³»ç±»å‹",
            "description": "å…³ç³»æè¿°",
            "properties": {{}},
            "confidence": 0.85,
            "context": "æ”¯æŒå…³ç³»çš„æ–‡æœ¬ç‰‡æ®µ"
        }}
    ]
}}
```

æ³¨æ„äº‹é¡¹ï¼š
1. å®ä½“æŠ½å–ï¼š
   - åªæŠ½å–é‡è¦çš„ã€æœ‰æ„ä¹‰çš„å®ä½“
   - ç¡®ä¿å®ä½“åç§°å‡†ç¡®å®Œæ•´
   - å®ä½“ç±»å‹å¿…é¡»ä»æä¾›çš„ç±»å‹ä¸­é€‰æ‹©
   - ç½®ä¿¡åº¦è¦æ ¹æ®ä¸Šä¸‹æ–‡åˆç†è¯„ä¼°
   - ä½ç½®ä¿¡æ¯è¦å‡†ç¡®
   - é¿å…é‡å¤æŠ½å–ç›¸åŒå®ä½“

2. å…³ç³»æŠ½å–ï¼š
   - åªæŠ½å–æ–‡æœ¬ä¸­æ˜ç¡®ä½“ç°çš„å…³ç³»
   - ç¡®ä¿æºå®ä½“å’Œç›®æ ‡å®ä½“éƒ½åœ¨æŠ½å–çš„å®ä½“åˆ—è¡¨ä¸­
   - å…³ç³»ç±»å‹å¿…é¡»ä»æä¾›çš„ç±»å‹ä¸­é€‰æ‹©
   - ç½®ä¿¡åº¦è¦æ ¹æ®æ–‡æœ¬è¯æ®å¼ºåº¦è¯„ä¼°
   - ä¸Šä¸‹æ–‡è¦å‡†ç¡®åæ˜ å…³ç³»çš„æ–‡æœ¬ä¾æ®
   - é¿å…é‡å¤æˆ–å†—ä½™çš„å…³ç³»
   - æ³¨æ„å…³ç³»çš„æ–¹å‘æ€§

3. æ•´ä½“è¦æ±‚ï¼š
   - ä¿æŒå®ä½“å’Œå…³ç³»çš„ä¸€è‡´æ€§
   - ä¼˜å…ˆæŠ½å–é«˜ç½®ä¿¡åº¦çš„ä¿¡æ¯
   - ç¡®ä¿JSONæ ¼å¼æ­£ç¡®
   - æ³¨æ„å®ä½“å’Œå…³ç³»çš„å®Œæ•´æ€§
"""
        return prompt
    
    def _parse_knowledge_response(self, response: str, source_text: str, 
                                chunk_id: str, chunk_index: int, chunk_metadata: Dict[str, Any]) -> Tuple[List[Entity], List[Relationship]]:
        """è§£æçŸ¥è¯†æŠ½å–å“åº”"""
        entities = []
        relationships = []
        
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()
            
            # è§£æJSON
            data = json.loads(json_str)
            
            # è§£æå®ä½“
            if 'entities' in data and isinstance(data['entities'], list):
                for i, entity_data in enumerate(data['entities']):
                    try:
                        entity = self._parse_entity_data(entity_data, source_text, chunk_id, i, chunk_metadata)
                        if entity and self._validate_entity(entity, source_text):
                            entities.append(entity)
                    except Exception as e:
                        logger.warning(f"è§£æå®ä½“æ•°æ®å¤±è´¥: {str(e)}")
                        continue
            
            # åˆ›å»ºå®ä½“åç§°åˆ°å®ä½“å¯¹è±¡çš„æ˜ å°„
            entity_map = {entity.name: entity for entity in entities}
            
            # è§£æå…³ç³»
            if 'relationships' in data and isinstance(data['relationships'], list):
                for i, rel_data in enumerate(data['relationships']):
                    try:
                        relationship = self._parse_relationship_data(
                            rel_data, entity_map, source_text, chunk_id, chunk_index, i, chunk_metadata
                        )
                        if relationship and self._validate_relationship(relationship, source_text):
                            relationships.append(relationship)
                    except Exception as e:
                        logger.warning(f"è§£æå…³ç³»æ•°æ®å¤±è´¥: {str(e)}")
                        continue
            
        except json.JSONDecodeError as e:
            logger.error(f"çŸ¥è¯†JSONè§£æå¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"è§£æçŸ¥è¯†å“åº”å¤±è´¥: {str(e)}")
        
        return entities, relationships
    
    def _parse_entity_data(self, entity_data: Dict[str, Any], source_text: str, 
                          chunk_id: str, entity_index: int, chunk_metadata: Dict[str, Any]) -> Optional[Entity]:
        """è§£æå®ä½“æ•°æ®"""
        try:
            name = entity_data.get('name', '').strip()
            entity_type = entity_data.get('type', '').strip()
            
            if not name or not entity_type:
                return None
            
            # éªŒè¯å®ä½“ç±»å‹
            if entity_type not in self.entity_types:
                entity_type = self._match_entity_type(entity_type)
            
            # è·å–ä½ç½®ä¿¡æ¯
            start_pos = entity_data.get('start_pos', 0)
            end_pos = entity_data.get('end_pos', len(name))
            
            # å¦‚æœä½ç½®ä¿¡æ¯ä¸å‡†ç¡®ï¼Œå°è¯•åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾
            if start_pos == 0 and end_pos == len(name):
                text_lower = source_text.lower()
                name_lower = name.lower()
                pos = text_lower.find(name_lower)
                if pos != -1:
                    start_pos = pos
                    end_pos = pos + len(name)
            
            # è·å–chunkç´¢å¼•ä¿¡æ¯
            chunk_index = chunk_metadata.get('chunk_index', 0)
            
            # ğŸ†• æ”¯æŒå¢å¼ºå­—æ®µçš„å®ä½“åˆ›å»º
            entity = Entity(
                id=f"{chunk_id}_entity_{entity_index}",
                name=name,
                type=entity_type,
                description=entity_data.get('description', ''),
                properties={
                    **entity_data.get('properties', {}),
                    "chunk_id": chunk_id,
                    "chunk_index": chunk_index,
                    "entity_index": entity_index
                },
                confidence=float(entity_data.get('confidence', 0.7)),
                source_text=source_text[:300] + '...' if len(source_text) > 300 else source_text,
                start_pos=start_pos,
                end_pos=end_pos,
                chunk_neo4j_id=None,
                document_postgresql_id=chunk_metadata.get('postgresql_document_id'),
                document_neo4j_id=None,
                # ğŸ†• æ˜¾å¼åˆå§‹åŒ–å¢å¼ºå­—æ®µï¼Œç¡®ä¿å‘å‰å…¼å®¹
                aliases=entity_data.get('aliases', []),  # æ”¯æŒä»LLMå“åº”ä¸­è·å–åˆ«å
                embedding=None,  # å°†åœ¨åç»­æ­¥éª¤ä¸­ç”Ÿæˆ
                quality_score=float(entity_data.get('quality_score', 0.8))  # é»˜è®¤è´¨é‡åˆ†æ•°
            )
            
            # æ·»åŠ chunk_indexä½œä¸ºå®ä½“å±æ€§ï¼Œä¾¿äºåç»­å…³è”
            entity.chunk_index = chunk_index
            
            return entity
            
        except Exception as e:
            logger.warning(f"è§£æå®ä½“æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def _parse_relationship_data(self, rel_data: Dict[str, Any], entity_map: Dict[str, Entity], 
                               source_text: str, chunk_id: str, chunk_index: int, 
                               rel_index: int, chunk_metadata: Dict[str, Any]) -> Optional[Relationship]:
        """è§£æå…³ç³»æ•°æ®"""
        try:
            source_name = rel_data.get('source_entity', '').strip()
            target_name = rel_data.get('target_entity', '').strip()
            rel_type = rel_data.get('relationship_type', '').strip()
            
            if not source_name or not target_name or not rel_type:
                return None
            
            # éªŒè¯å®ä½“å­˜åœ¨
            if source_name not in entity_map or target_name not in entity_map:
                return None
            
            # éªŒè¯å…³ç³»ç±»å‹
            if rel_type not in self.relationship_types:
                rel_type = self._match_relationship_type(rel_type)
            
            # è·å–å®ä½“å¯¹è±¡
            source_entity = entity_map[source_name]
            target_entity = entity_map[target_name]
            
            relationship = Relationship(
                id=f"{chunk_id}_rel_{rel_index}",
                source_entity_id=source_entity.id,
                target_entity_id=target_entity.id,
                source_entity_name=source_name,
                target_entity_name=target_name,
                relationship_type=rel_type,
                description=rel_data.get('description', ''),
                properties=rel_data.get('properties', {}),
                confidence=float(rel_data.get('confidence', 0.7)),
                source_text=source_text[:300] + '...' if len(source_text) > 300 else source_text,
                context=rel_data.get('context', ''),
                chunk_neo4j_id=None,
                document_postgresql_id=chunk_metadata.get('postgresql_document_id'),
                document_neo4j_id=None
            )
            
            return relationship
            
        except Exception as e:
            logger.warning(f"è§£æå…³ç³»æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def _match_entity_type(self, entity_type: str) -> str:
        """åŒ¹é…å®ä½“ç±»å‹"""
        entity_type_lower = entity_type.lower()
        
        # ç±»å‹æ˜ å°„è¡¨
        type_mapping = {
            'person': 'äººç‰©', 'people': 'äººç‰©', 'äººå‘˜': 'äººç‰©', 'äººå': 'äººç‰©',
            'organization': 'ç»„ç»‡', 'org': 'ç»„ç»‡', 'æœºæ„': 'ç»„ç»‡', 'å…¬å¸': 'ç»„ç»‡',
            'location': 'åœ°ç‚¹', 'place': 'åœ°ç‚¹', 'ä½ç½®': 'åœ°ç‚¹', 'åœ°å': 'åœ°ç‚¹',
            'event': 'äº‹ä»¶', 'æ´»åŠ¨': 'äº‹ä»¶', 'ä¼šè®®': 'äº‹ä»¶',
            'concept': 'æ¦‚å¿µ', 'è§‚å¿µ': 'æ¦‚å¿µ', 'æƒ³æ³•': 'æ¦‚å¿µ',
            'technology': 'æŠ€æœ¯', 'tech': 'æŠ€æœ¯', 'ç§‘æŠ€': 'æŠ€æœ¯',
            'product': 'äº§å“', 'å•†å“': 'äº§å“', 'è´§ç‰©': 'äº§å“',
            'time': 'æ—¶é—´', 'date': 'æ—¶é—´', 'æ—¥æœŸ': 'æ—¶é—´',
            'number': 'æ•°å­—', 'numeric': 'æ•°å­—', 'æ•°é‡': 'æ•°å­—',
            'law': 'æ³•å¾‹æ¡æ–‡', 'legal': 'æ³•å¾‹æ¡æ–‡', 'æ³•è§„': 'æ³•å¾‹æ¡æ–‡',
            'policy': 'æ”¿ç­–', 'æ–¹é’ˆ': 'æ”¿ç­–', 'è§„å®š': 'æ”¿ç­–',
            'project': 'é¡¹ç›®', 'å·¥ç¨‹': 'é¡¹ç›®', 'è®¡åˆ’': 'é¡¹ç›®',
            'system': 'ç³»ç»Ÿ', 'ä½“ç³»': 'ç³»ç»Ÿ', 'åˆ¶åº¦': 'ç³»ç»Ÿ',
            'method': 'æ–¹æ³•', 'æ–¹å¼': 'æ–¹æ³•', 'æ‰‹æ®µ': 'æ–¹æ³•',
            'theory': 'ç†è®º', 'å­¦è¯´': 'ç†è®º', 'è§‚ç‚¹': 'ç†è®º'
        }
        
        # æŸ¥æ‰¾åŒ¹é…
        for key, value in type_mapping.items():
            if key in entity_type_lower:
                return value
        
        # é»˜è®¤è¿”å›æ¦‚å¿µç±»å‹
        return 'æ¦‚å¿µ'
    
    def _match_relationship_type(self, rel_type: str) -> str:
        """åŒ¹é…å…³ç³»ç±»å‹"""
        rel_type_lower = rel_type.lower()
        
        # å…³ç³»ç±»å‹æ˜ å°„è¡¨
        type_mapping = {
            'belongs': 'å±äº', 'belong_to': 'å±äº', 'éš¶å±': 'å±äº',
            'contains': 'åŒ…å«', 'include': 'åŒ…å«', 'åŒ…æ‹¬': 'åŒ…å«',
            'located': 'ä½äº', 'location': 'ä½äº', 'åœ°å¤„': 'ä½äº',
            'works': 'å·¥ä½œäº', 'work_for': 'å·¥ä½œäº', 'ä»»èŒ': 'å·¥ä½œäº',
            'founded': 'åˆ›ç«‹', 'establish': 'åˆ›ç«‹', 'å»ºç«‹': 'åˆ›ç«‹',
            'manages': 'ç®¡ç†', 'manage': 'ç®¡ç†', 'è´Ÿè´£': 'ç®¡ç†',
            'cooperate': 'åˆä½œ', 'collaborate': 'åˆä½œ', 'åä½œ': 'åˆä½œ',
            'mentions': 'æåŠ', 'mention': 'æåŠ', 'æ¶‰åŠ': 'æåŠ',
            'describes': 'æè¿°', 'describe': 'æè¿°', 'è¯´æ˜': 'æè¿°',
            'references': 'å¼•ç”¨', 'reference': 'å¼•ç”¨', 'å‚è€ƒ': 'å¼•ç”¨',
            'causes': 'å¯¼è‡´', 'cause': 'å¯¼è‡´', 'å¼•èµ·': 'å¯¼è‡´',
            'influences': 'å½±å“', 'influence': 'å½±å“', 'ä½œç”¨': 'å½±å“',
            'uses': 'ä½¿ç”¨', 'use': 'ä½¿ç”¨', 'åˆ©ç”¨': 'ä½¿ç”¨',
            'depends': 'ä¾èµ–', 'depend': 'ä¾èµ–', 'ä¾é ': 'ä¾èµ–',
            'implements': 'å®ç°', 'implement': 'å®ç°', 'æ‰§è¡Œ': 'å®ç°',
            'relates': 'ç›¸å…³', 'relate': 'ç›¸å…³', 'å…³ç³»': 'ç›¸å…³',
            'connects': 'è¿æ¥', 'connect': 'è¿æ¥', 'è¿é€š': 'è¿æ¥',
            'associates': 'å…³è”', 'associate': 'å…³è”', 'è”ç³»': 'å…³è”'
        }
        
        # æŸ¥æ‰¾åŒ¹é…
        for key, value in type_mapping.items():
            if key in rel_type_lower:
                return value
        
        # é»˜è®¤è¿”å›å…³è”ç±»å‹
        return 'å…³è”'
    
    def _validate_entity(self, entity: Entity, source_text: str) -> bool:
        """éªŒè¯å®ä½“æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥åç§°é•¿åº¦
        if len(entity.name) < 2 or len(entity.name) > 100:
            return False
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        if entity.confidence < 0.3:
            return False
        
        # æ£€æŸ¥ä½ç½®ä¿¡æ¯
        if entity.start_pos < 0 or entity.end_pos <= entity.start_pos:
            return False
        
        if entity.end_pos > len(source_text):
            return False
        
        return True
    
    def _validate_relationship(self, relationship: Relationship, source_text: str) -> bool:
        """éªŒè¯å…³ç³»æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥ç½®ä¿¡åº¦
        if relationship.confidence < 0.3:
            return False
        
        # æ£€æŸ¥å®ä½“åç§°ä¸èƒ½ç›¸åŒ
        if relationship.source_entity_name == relationship.target_entity_name:
            return False
        
        # æ£€æŸ¥å…³ç³»ç±»å‹æœ‰æ•ˆæ€§
        if relationship.relationship_type not in self.relationship_types:
            return False
        
        return True
    
    # ä¼ ç»Ÿå»é‡æ–¹æ³•å·²ç§»é™¤ï¼Œå…¨é¢ä½¿ç”¨æ™ºèƒ½å®ä½“ç»Ÿä¸€
    
    def _extract_chunk_id_from_entity_id(self, entity_id: str) -> Optional[str]:
        """ä»å®ä½“IDä¸­æå–chunk_id
        
        å®ä½“IDæ ¼å¼ï¼š{chunk_id}_entity_{entity_index}
        
        Args:
            entity_id: å®ä½“ID
            
        Returns:
            chunk_idï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            if "_entity_" in entity_id:
                chunk_id = entity_id.split("_entity_")[0]
                if "chunk" in chunk_id:
                    return chunk_id
            return None
        except Exception as e:
            logger.warning(f"ä»å®ä½“ID {entity_id} æå–chunk_idå¤±è´¥: {str(e)}")
            return None
    
    def _filter_relationships(self, relationships: List[Relationship], 
                            entities: List[Entity]) -> List[Relationship]:
        """è¿‡æ»¤å’Œå»é‡å…³ç³»"""
        logger.info(f"å¼€å§‹è¿‡æ»¤ {len(relationships)} ä¸ªå…³ç³»")
        
        # åˆ›å»ºå®ä½“åç§°æ˜ å°„
        entity_names = {entity.name for entity in entities}
        
        # è¿‡æ»¤å…³ç³»ï¼šç¡®ä¿å…³ç³»çš„å®ä½“éƒ½å­˜åœ¨
        valid_relationships = []
        for relationship in relationships:
            if (relationship.source_entity_name in entity_names and 
                relationship.target_entity_name in entity_names):
                valid_relationships.append(relationship)
        
        # æŒ‰å…³ç³»é”®åˆ†ç»„ï¼ˆæºå®ä½“-ç›®æ ‡å®ä½“-å…³ç³»ç±»å‹ï¼‰
        relationship_groups = {}
        
        for relationship in valid_relationships:
            key = (
                relationship.source_entity_name,
                relationship.target_entity_name,
                relationship.relationship_type
            )
            
            if key not in relationship_groups:
                relationship_groups[key] = []
            relationship_groups[key].append(relationship)
        
        # æ¯ç»„é€‰æ‹©æœ€ä½³å…³ç³»
        filtered = []
        
        for key, group in relationship_groups.items():
            if len(group) == 1:
                filtered.append(group[0])
            else:
                # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„å…³ç³»
                best_relationship = max(group, key=lambda x: x.confidence)
                filtered.append(best_relationship)
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³ç³»
        high_confidence_relationships = [
            rel for rel in filtered if rel.confidence >= 0.5
        ]
        
        logger.info(f"å…³ç³»è¿‡æ»¤å®Œæˆï¼š{len(high_confidence_relationships)} ä¸ªé«˜è´¨é‡å…³ç³»")
        return high_confidence_relationships
    
    def _normalize_entity_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å®ä½“åç§°"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', name.strip())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        normalized = re.sub(r'[""''ã€Šã€‹ã€ã€‘ï¼ˆï¼‰()]', '', normalized)
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        return normalized.lower()
    
    # ğŸ†• æ™ºèƒ½å®ä½“ç»Ÿä¸€æ–¹æ³•
    # ğŸš« DEPRECATED: æ­¤æ–¹æ³•å·²å¼ƒç”¨ï¼Œå®ä½“ç»Ÿä¸€ç§»è‡³å…¨å±€ç»Ÿä¸€ä»»åŠ¡
    async def _unify_entities_intelligent(self, entities: List[Entity]) -> List[Entity]:
        """
        [å·²å¼ƒç”¨] ä½¿ç”¨æ™ºèƒ½å®ä½“ç»Ÿä¸€ç®—æ³•è¿›è¡Œå®ä½“æ ‡å‡†åŒ–
        
        æ­¤æ–¹æ³•å·²è¢«ç§»é™¤ï¼Œå®ä½“ç»Ÿä¸€ç°åœ¨åœ¨å…¨å±€ç»Ÿä¸€ä»»åŠ¡ä¸­ä½¿ç”¨LangGraph Agentæ‰§è¡Œã€‚
        æ–‡æ¡£å¤„ç†ä¸­çš„å®ä½“ç›´æ¥å…¥åº“ï¼Œä¸å†è¿›è¡Œæ–‡æ¡£å†…ç»Ÿä¸€ã€‚
        
        Args:
            entities: åŸå§‹å®ä½“åˆ—è¡¨
            
        Returns:
            ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨
        """
        logger.warning("_unify_entities_intelligentæ–¹æ³•å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨å…¨å±€ç»Ÿä¸€ä»»åŠ¡")
        return entities  # ç›´æ¥è¿”å›åŸå®ä½“ï¼Œä¸è¿›è¡Œç»Ÿä¸€
    
    def _create_chunk_mapping_for_raw_entities(self, raw_entities: List[Entity]) -> Dict[str, List[str]]:
        """
        ä¸ºåŸå§‹å®ä½“åˆ›å»ºchunkæ˜ å°„ï¼ˆä¸ç»Ÿä¸€ï¼‰
        
        Args:
            raw_entities: åŸå§‹å®ä½“åˆ—è¡¨
            
        Returns:
            å®ä½“é”®åˆ°chunk IDåˆ—è¡¨çš„æ˜ å°„
        """
        entity_chunk_mapping = {}
        
        for entity in raw_entities:
            # ä¸ºåŸå§‹å®ä½“åˆ›å»ºé”®
            entity_key = (self._normalize_entity_name(entity.name), entity.type)
            
            # ä»å®ä½“IDä¸­æå–chunkä¿¡æ¯
            chunk_id = self._extract_chunk_id_from_entity_id(entity.id)
            
            if entity_key not in entity_chunk_mapping:
                entity_chunk_mapping[entity_key] = []
            
            if chunk_id and chunk_id not in entity_chunk_mapping[entity_key]:
                entity_chunk_mapping[entity_key].append(chunk_id)
        
        return entity_chunk_mapping
    
    def _create_chunk_mapping_for_unified_entities(self, unified_entities: List[Entity]) -> Dict[str, List[str]]:
        """
        ä¸ºç»Ÿä¸€åçš„å®ä½“åˆ›å»ºchunkæ˜ å°„
        
        Args:
            unified_entities: ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨
            
        Returns:
            å®ä½“é”®åˆ°chunk IDåˆ—è¡¨çš„æ˜ å°„
        """
        entity_chunk_mapping = {}
        
        for entity in unified_entities:
            # ä¸ºç»Ÿä¸€åçš„å®ä½“åˆ›å»ºé”®
            entity_key = (self._normalize_entity_name(entity.name), entity.type)
            
            # ä»å®ä½“å±æ€§ä¸­æå–chunkä¿¡æ¯
            chunk_ids = []
            
            # æ£€æŸ¥å®ä½“æ˜¯å¦æ˜¯åˆå¹¶çš„ç»“æœ
            if hasattr(entity, 'merged_from') and entity.merged_from:
                # å¦‚æœæ˜¯åˆå¹¶å®ä½“ï¼Œä»merged_fromä¸­æå–chunkä¿¡æ¯
                for original_entity_id in entity.merged_from:
                    chunk_id = self._extract_chunk_id_from_entity_id(original_entity_id)
                    if chunk_id and chunk_id not in chunk_ids:
                        chunk_ids.append(chunk_id)
            else:
                # å¦‚æœä¸æ˜¯åˆå¹¶å®ä½“ï¼Œä»å…¶IDä¸­æå–chunkä¿¡æ¯
                chunk_id = self._extract_chunk_id_from_entity_id(entity.id)
                if chunk_id:
                    chunk_ids.append(chunk_id)
            
            # ä¹Ÿæ£€æŸ¥å®ä½“å±æ€§ä¸­æ˜¯å¦å·²æœ‰chunk_idsä¿¡æ¯
            existing_chunk_ids = entity.properties.get('chunk_ids', [])
            for chunk_id in existing_chunk_ids:
                if chunk_id not in chunk_ids:
                    chunk_ids.append(chunk_id)
            
            entity_chunk_mapping[entity_key] = chunk_ids
        
        logger.debug(f"ä¸º {len(unified_entities)} ä¸ªç»Ÿä¸€å®ä½“åˆ›å»ºäº†chunkæ˜ å°„")
        return entity_chunk_mapping
    
    async def _trigger_post_extraction_unification(self, entities: List[Entity], chunks: List[Any]):
        """
        è§¦å‘æ–‡æ¡£è§£æåçš„å®ä½“ç»Ÿä¸€ä»»åŠ¡
        
        Args:
            entities: ç»Ÿä¸€åçš„å®ä½“åˆ—è¡¨
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        try:
            # è·å–æ–‡æ¡£ID
            document_id = None
            if chunks and hasattr(chunks[0], 'metadata'):
                document_id = chunks[0].metadata.postgresql_document_id
            
            if not document_id:
                logger.warning("æ— æ³•è·å–æ–‡æ¡£IDï¼Œè·³è¿‡å®ä½“ç»Ÿä¸€è§¦å‘")
                return
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å®ä½“ç»Ÿä¸€ï¼ˆåŸºäºé…ç½®ï¼‰
            if not getattr(settings, 'ENABLE_POST_EXTRACTION_UNIFICATION', True):
                logger.info("æ–‡æ¡£è§£æåå®ä½“ç»Ÿä¸€å·²ç¦ç”¨ï¼Œè·³è¿‡è§¦å‘")
                return
            
            # è½¬æ¢å®ä½“ä¸ºå­—å…¸æ ¼å¼
            entities_data = []
            for entity in entities:
                entity_data = {
                    'id': entity.id,
                    'name': entity.name,
                    'type': entity.type,
                    'entity_type': entity.entity_type,
                    'description': entity.description,
                    'properties': entity.properties,
                    'confidence': entity.confidence,
                    'source_text': entity.source_text,
                    'start_pos': entity.start_pos,
                    'end_pos': entity.end_pos,
                    'chunk_neo4j_id': entity.chunk_neo4j_id,
                    'document_postgresql_id': entity.document_postgresql_id,
                    'document_neo4j_id': entity.document_neo4j_id,
                    'aliases': entity.aliases,
                    'embedding': entity.embedding,
                    'quality_score': entity.quality_score,
                    'importance_score': entity.importance_score
                }
                entities_data.append(entity_data)
            
            # è§¦å‘å¼‚æ­¥å®ä½“ç»Ÿä¸€ä»»åŠ¡
            from app.worker.celery_tasks import trigger_document_entity_unification
            
            # ğŸ†• ä½¿ç”¨å…¨å±€è¯­ä¹‰ç»Ÿä¸€æ¨¡å¼ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„LangGraph Agent
            unification_mode = getattr(settings, 'DEFAULT_UNIFICATION_MODE', 'global_semantic')
            
            result = trigger_document_entity_unification(
                document_id=document_id,
                extracted_entities=entities_data,
                unification_mode=unification_mode
            )
            
            logger.info(f"å·²è§¦å‘æ–‡æ¡£è§£æåå®ä½“ç»Ÿä¸€ä»»åŠ¡: {result['task_id']}, æ¨¡å¼: {unification_mode}")
            
        except Exception as e:
            logger.error(f"è§¦å‘æ–‡æ¡£è§£æåå®ä½“ç»Ÿä¸€å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹ 