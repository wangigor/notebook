# -*- coding: utf-8 -*-
"""
å…¨å±€å®ä½“ç»Ÿä¸€æœåŠ¡ - é‡æ„ç‰ˆæœ¬
æ•´åˆNeo4jé‡‡æ ·ã€LLMè¯­ä¹‰åˆ†æå’Œæ•°æ®åº“æ›´æ–°çš„å®Œæ•´æµç¨‹
"""
import logging
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from app.services.neo4j_entity_sampler import get_neo4j_entity_sampler
from app.services.langgraph_entity_agent import get_langgraph_entity_deduplication_agent
from app.services.neo4j_entity_updater import get_neo4j_entity_updater
from app.models.entity import Entity

logger = logging.getLogger(__name__)


@dataclass
class GlobalUnificationConfig:
    """å…¨å±€å®ä½“ç»Ÿä¸€é…ç½®"""
    max_sample_entities_per_type: int = 50  # æ¯ç§ç±»å‹æœ€å¤§é‡‡æ ·æ•°é‡
    min_entities_for_unification: int = 2   # å¯åŠ¨ç»Ÿä¸€çš„æœ€å°å®ä½“æ•°
    enable_cross_document_sampling: bool = True  # å¯ç”¨è·¨æ–‡æ¡£é‡‡æ ·
    llm_confidence_threshold: float = 0.7  # LLMç½®ä¿¡åº¦é˜ˆå€¼
    max_batch_size: int = 20  # æœ€å¤§æ‰¹å¤„ç†å¤§å°
    enable_quality_boost: bool = True  # å¯ç”¨è´¨é‡åˆ†æ•°æå‡


@dataclass
class GlobalUnificationResult:
    """å…¨å±€å®ä½“ç»Ÿä¸€ç»“æœ"""
    success: bool
    total_entities_processed: int
    entities_merged: int
    entities_deleted: int
    relationships_updated: int
    processing_time: float
    type_statistics: Dict[str, Any]
    errors: List[str]


class GlobalEntityUnificationService:
    """å…¨å±€å®ä½“ç»Ÿä¸€æœåŠ¡"""
    
    def __init__(self, config: Optional[GlobalUnificationConfig] = None):
        """
        åˆå§‹åŒ–å…¨å±€å®ä½“ç»Ÿä¸€æœåŠ¡
        
        Args:
            config: ç»Ÿä¸€é…ç½®
        """
        self.config = config or GlobalUnificationConfig()
        self.entity_sampler = get_neo4j_entity_sampler()
        self.langgraph_agent = get_langgraph_entity_deduplication_agent()
        self.entity_updater = get_neo4j_entity_updater()
        
        logger.info("å…¨å±€å®ä½“ç»Ÿä¸€æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨LangGraph Agentï¼‰")
    
    async def unify_entities_for_document(
        self,
        new_entities: List[Entity],
        document_id: int
    ) -> GlobalUnificationResult:
        """
        ä¸ºæ–°æ–‡æ¡£çš„å®ä½“æ‰§è¡Œå…¨å±€ç»Ÿä¸€
        
        Args:
            new_entities: æ–°æå–çš„å®ä½“åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            
        Returns:
            ç»Ÿä¸€ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} æ‰§è¡Œå…¨å±€å®ä½“ç»Ÿä¸€ï¼Œæ–°å®ä½“æ•°: {len(new_entities)}")
        
        try:
            # 1. æŒ‰ç±»å‹åˆ†ç»„æ–°å®ä½“
            new_entities_by_type = self._group_entities_by_type(new_entities)
            
            logger.info(f"æ–°å®ä½“æŒ‰ç±»å‹åˆ†ç»„: {[(t, len(entities)) for t, entities in new_entities_by_type.items()]}")
            
            # 2. ä¸ºæ¯ç§ç±»å‹æ‰§è¡Œç»Ÿä¸€
            total_processed = 0
            total_merged = 0
            total_deleted = 0
            total_relationships_updated = 0
            type_statistics = {}
            all_errors = []
            
            for entity_type, type_new_entities in new_entities_by_type.items():
                try:
                    type_result = await self._unify_entities_by_type(
                        type_new_entities, entity_type, document_id
                    )
                    
                    total_processed += type_result['entities_processed']
                    total_merged += type_result['entities_merged']
                    total_deleted += type_result['entities_deleted']
                    total_relationships_updated += type_result['relationships_updated']
                    type_statistics[entity_type] = type_result
                    
                    if type_result['errors']:
                        all_errors.extend(type_result['errors'])
                    
                    logger.info(f"{entity_type} ç±»å‹ç»Ÿä¸€å®Œæˆ: {type_result}")
                    
                except Exception as e:
                    error_msg = f"{entity_type} ç±»å‹ç»Ÿä¸€å¤±è´¥: {str(e)}"
                    logger.error(error_msg)
                    all_errors.append(error_msg)
            
            processing_time = time.time() - start_time
            
            result = GlobalUnificationResult(
                success=len(all_errors) == 0,
                total_entities_processed=total_processed,
                entities_merged=total_merged,
                entities_deleted=total_deleted,
                relationships_updated=total_relationships_updated,
                processing_time=processing_time,
                type_statistics=type_statistics,
                errors=all_errors
            )
            
            logger.info(f"æ–‡æ¡£ {document_id} å…¨å±€å®ä½“ç»Ÿä¸€å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"å…¨å±€å®ä½“ç»Ÿä¸€æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            return GlobalUnificationResult(
                success=False,
                total_entities_processed=0,
                entities_merged=0,
                entities_deleted=0,
                relationships_updated=0,
                processing_time=processing_time,
                type_statistics={},
                errors=[error_msg]
            )
    
    async def _unify_entities_by_type(
        self,
        new_entities: List[Entity],
        entity_type: str,
        document_id: int
    ) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šç±»å‹çš„å®ä½“æ‰§è¡Œç»Ÿä¸€
        
        Args:
            new_entities: æ–°å®ä½“åˆ—è¡¨
            entity_type: å®ä½“ç±»å‹
            document_id: æ–‡æ¡£ID
            
        Returns:
            ç±»å‹ç»Ÿä¸€ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç† {entity_type} ç±»å‹çš„ {len(new_entities)} ä¸ªæ–°å®ä½“")
        
        result = {
            'entities_processed': len(new_entities),
            'entities_merged': 0,
            'entities_deleted': 0,
            'relationships_updated': 0,
            'errors': []
        }
        
        if len(new_entities) < self.config.min_entities_for_unification:
            logger.info(f"{entity_type} ç±»å‹æ–°å®ä½“æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç»Ÿä¸€")
            return result
        
        try:
            # 1. ä»Neo4jé‡‡æ ·ç°æœ‰å®ä½“
            sampled_entities = self._sample_existing_entities(
                entity_type, document_id
            )
            
            logger.info(f"ä»Neo4jé‡‡æ ·äº† {len(sampled_entities)} ä¸ª {entity_type} ç±»å‹çš„ç°æœ‰å®ä½“")
            
            # 2. åˆå¹¶æ–°å®ä½“å’Œé‡‡æ ·å®ä½“
            all_entities = self._combine_entities(new_entities, sampled_entities)
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šåˆå¹¶åçš„å¾…åˆå¹¶å®ä½“é›†åˆ
            logger.info("=" * 80)
            logger.info(f"ğŸ” åˆå¹¶åçš„å¾…åˆå¹¶å®ä½“é›†åˆ - {entity_type} ç±»å‹")
            logger.info("=" * 80)
            logger.info(f"æ–°å®ä½“æ•°é‡: {len(new_entities)}")
            logger.info(f"é‡‡æ ·ç°æœ‰å®ä½“æ•°é‡: {len(sampled_entities)}")
            logger.info(f"åˆå¹¶åæ€»å®ä½“æ•°é‡: {len(all_entities)}")
            
            # è¯¦ç»†æ˜¾ç¤ºåˆå¹¶åçš„å®ä½“ä¿¡æ¯ï¼ˆå‰15ä¸ªï¼‰
            logger.info(f"ğŸ“ åˆå¹¶åå®ä½“è¯¦æƒ…ï¼ˆå‰15ä¸ªï¼‰:")
            for i, entity in enumerate(all_entities[:15]):
                source = entity.get('source', 'unknown')
                logger.info(f"  å®ä½“ {i+1} ({source}):")
                logger.info(f"    - åç§°: {entity.get('name', 'N/A')}")
                logger.info(f"    - ç±»å‹: {entity.get('type', 'N/A')}")
                logger.info(f"    - æè¿°: {entity.get('description', 'N/A')[:100]}..." if entity.get('description') else "    - æè¿°: æ— ")
                logger.info(f"    - ID: {entity.get('id', 'N/A')}")
                logger.info(f"    - æ¥æº: {source}")
                logger.info(f"    - è´¨é‡åˆ†æ•°: {entity.get('quality_score', 'N/A')}")
                logger.info(f"    - ç½®ä¿¡åº¦: {entity.get('confidence', 'N/A')}")
                logger.info(f"    - åˆ«å: {entity.get('aliases', [])}")
                
            if len(all_entities) > 15:
                logger.info(f"  ... è¿˜æœ‰ {len(all_entities) - 15} ä¸ªå®ä½“")
            
            # æŒ‰æ¥æºç»Ÿè®¡
            source_stats = {}
            for entity in all_entities:
                source = entity.get('source', 'unknown')
                source_stats[source] = source_stats.get(source, 0) + 1
            
            logger.info(f"ğŸ“Š å®ä½“æ¥æºç»Ÿè®¡:")
            for source, count in source_stats.items():
                logger.info(f"  - {source}: {count} ä¸ª")
            
            logger.info("=" * 80)
            
            if len(all_entities) < self.config.min_entities_for_unification:
                logger.info(f"{entity_type} ç±»å‹æ€»å®ä½“æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç»Ÿä¸€")
                return result
            
            # 3. LangGraph Agentè¯­ä¹‰å»é‡åˆ†æ
            logger.info(f"å¼€å§‹LLMè¯­ä¹‰å»é‡åˆ†æï¼Œå®ä½“ç±»å‹: {entity_type}ï¼Œæ•°é‡: {len(all_entities)}")
            
            # è½¬æ¢ä¸ºLangGraph Agentæ‰€éœ€çš„æ ¼å¼
            entity_dicts = []
            for entity in all_entities:
                try:
                    # å®‰å…¨åœ°è·å–å®ä½“æ•°æ®ï¼Œæ”¯æŒå­—å…¸å’ŒEntityå¯¹è±¡
                    entity_dict = {
                        'name': entity.get('name', '') if isinstance(entity, dict) else getattr(entity, 'name', ''),
                        'type': entity.get('type', '') if isinstance(entity, dict) else getattr(entity, 'type', ''),
                        'description': entity.get('description', '') if isinstance(entity, dict) else getattr(entity, 'description', ''),
                        'properties': entity.get('properties', {}) if isinstance(entity, dict) else getattr(entity, 'properties', {})
                    }
                    
                    # å¤„ç†IDå­—æ®µ
                    if isinstance(entity, dict):
                        if entity.get('id'):
                            entity_dict['id'] = entity.get('id')
                    else:
                        if hasattr(entity, 'id'):
                            entity_dict['id'] = entity.id
                    
                    entity_dicts.append(entity_dict)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“è½¬æ¢: {str(e)}")
                    continue
            
            # ä½¿ç”¨Agentæ¨¡å¼è¿›è¡Œå»é‡åˆ†æï¼ˆæ–°çš„å®ä½“åˆ—è¡¨æ¨¡å¼ï¼‰
            logger.info(f"ä½¿ç”¨Agentæ¨¡å¼è¿›è¡Œå»é‡åˆ†æï¼š{entity_type} ç±»å‹ï¼Œ{len(entity_dicts)} ä¸ªå®ä½“")
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šå‘é€ç»™Agentçš„å®ä½“æ•°æ®æ ¼å¼æ£€æŸ¥
            logger.info("=== å‘é€ç»™Agentçš„å®ä½“æ•°æ®æ ¼å¼æ£€æŸ¥ ===")
            logger.info(f"å®ä½“å­—å…¸æ•°é‡: {len(entity_dicts)}")
            
            if entity_dicts:
                logger.info("å‰3ä¸ªå®ä½“çš„æ•°æ®æ ¼å¼:")
                for i, entity_dict in enumerate(entity_dicts[:3]):
                    logger.info(f"  å®ä½“ {i+1}:")
                    logger.info(f"    - name: {entity_dict.get('name', 'N/A')}")
                    logger.info(f"    - type: {entity_dict.get('type', 'N/A')}")
                    logger.info(f"    - id: {entity_dict.get('id', 'N/A')}")
                    logger.info(f"    - description: {entity_dict.get('description', 'N/A')[:50]}...{' (truncated)' if len(entity_dict.get('description', '')) > 50 else ''}")
            
            analysis_result = await self.langgraph_agent.deduplicate_entities_list(entity_dicts, entity_type)
            
            # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šAgentè¿”å›ç»“æœæ£€æŸ¥
            logger.info("=== Agentè¿”å›ç»“æœæ£€æŸ¥ ===")
            logger.info(f"åˆ†æç»“æœç±»å‹: {type(analysis_result)}")
            logger.info(f"åˆ†æç»“æœé”®: {list(analysis_result.keys()) if isinstance(analysis_result, dict) else 'N/A'}")
            logger.info(f"åˆ†ææ‘˜è¦: {analysis_result.get('analysis_summary', 'N/A')}")
            
            merge_groups = analysis_result.get("merge_groups", [])
            logger.info(f"Agentè¯†åˆ«çš„åˆå¹¶ç»„æ•°é‡: {len(merge_groups)}")
            
            if merge_groups:
                logger.info("å‰2ä¸ªåˆå¹¶ç»„çš„è¯¦æƒ…:")
                for i, group in enumerate(merge_groups[:2]):
                    logger.info(f"  åˆå¹¶ç»„ {i+1}:")
                    logger.info(f"    - primary_entity: {group.get('primary_entity', 'N/A')}")
                    logger.info(f"    - duplicates: {group.get('duplicates', [])}")
                    logger.info(f"    - primary_entity_id: {group.get('primary_entity_id', 'N/A')}")
                    logger.info(f"    - duplicate_entity_ids: {group.get('duplicate_entity_ids', [])}")
                    logger.info(f"    - confidence: {group.get('confidence', 'N/A')}")
            
            # 4. æå–åˆå¹¶æ“ä½œ
            logger.info(f"å¼€å§‹æå–åˆå¹¶æ“ä½œï¼ŒåŸå®ä½“æ•°é‡: {len(all_entities)}")
            merge_operations = self._extract_merge_operations_from_agent_result(analysis_result, all_entities)
            
            if not merge_operations:
                logger.info(f"{entity_type} ç±»å‹æ²¡æœ‰éœ€è¦åˆå¹¶çš„å®ä½“")
                return result
            
            # 5. åº”ç”¨åˆå¹¶æ“ä½œåˆ°Neo4j
            update_result = self.entity_updater.apply_merge_operations(
                all_entities, merge_operations
            )
            
            result['entities_merged'] = update_result['merged_entities']
            result['entities_deleted'] = update_result['deleted_entities']
            result['relationships_updated'] = update_result['updated_relationships']
            result['errors'] = update_result['errors']
            
            logger.info(f"{entity_type} ç±»å‹ç»Ÿä¸€å®Œæˆ: åˆå¹¶ {result['entities_merged']} ä¸ªï¼Œåˆ é™¤ {result['entities_deleted']} ä¸ª")
            
            return result
            
        except Exception as e:
            error_msg = f"{entity_type} ç±»å‹ç»Ÿä¸€å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
            return result
    
    def _sample_existing_entities(
        self,
        entity_type: str,
        exclude_document_id: int
    ) -> List[Dict[str, Any]]:
        """
        ä»Neo4jé‡‡æ ·ç°æœ‰å®ä½“
        
        Args:
            entity_type: å®ä½“ç±»å‹
            exclude_document_id: æ’é™¤çš„æ–‡æ¡£ID
            
        Returns:
            é‡‡æ ·çš„å®ä½“åˆ—è¡¨
        """
        try:
            # è·å–è¯¥ç±»å‹çš„å®ä½“æ€»æ•°
            total_count = self.entity_sampler.get_entity_count_by_type(entity_type)
            
            if total_count == 0:
                logger.info(f"Neo4jä¸­æ²¡æœ‰ {entity_type} ç±»å‹çš„ç°æœ‰å®ä½“")
                return []
            
            # ç¡®å®šé‡‡æ ·æ•°é‡
            sample_size = min(total_count, self.config.max_sample_entities_per_type)
            
            # æ‰§è¡Œé‡‡æ · - ç»Ÿä¸€ä½¿ç”¨åŒæ­¥æ–¹æ³•
            sampled_entities = self.entity_sampler.sample_entities_by_type(
                entity_type=entity_type,
                limit=sample_size,
                exclude_document_ids=[exclude_document_id] if not self.config.enable_cross_document_sampling else None
            )
            
            logger.debug(f"æˆåŠŸé‡‡æ · {len(sampled_entities)} ä¸ª {entity_type} ç±»å‹å®ä½“")
            return sampled_entities
            
        except Exception as e:
            logger.error(f"é‡‡æ · {entity_type} ç±»å‹å®ä½“å¤±è´¥: {str(e)}")
            return []
    
    def _combine_entities(
        self,
        new_entities: List[Entity],
        sampled_entities: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        åˆå¹¶æ–°å®ä½“å’Œé‡‡æ ·å®ä½“ä¸ºç»Ÿä¸€æ ¼å¼
        
        Args:
            new_entities: æ–°å®ä½“åˆ—è¡¨
            sampled_entities: é‡‡æ ·çš„ç°æœ‰å®ä½“åˆ—è¡¨
            
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„å®ä½“åˆ—è¡¨
        """
        combined_entities = []
        
        # æ·»åŠ æ–°å®ä½“ï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
        for entity in new_entities:
            try:
                # å®‰å…¨åœ°è½¬æ¢Entityå¯¹è±¡ä¸ºå­—å…¸
                entity_dict = {
                    'id': getattr(entity, 'id', ''),
                    'name': getattr(entity, 'name', ''),
                    'type': getattr(entity, 'type', ''),
                    'entity_type': getattr(entity, 'entity_type', None) or getattr(entity, 'type', ''),
                    'description': getattr(entity, 'description', ''),
                    'properties': getattr(entity, 'properties', {}),
                    'confidence': getattr(entity, 'confidence', 0.8),
                    'source_text': getattr(entity, 'source_text', ''),
                    'quality_score': getattr(entity, 'quality_score', 0.8),
                    'importance_score': getattr(entity, 'importance_score', 0.5),
                    'aliases': getattr(entity, 'aliases', []) or [],
                    'source': 'new_document',
                    'temp_id': getattr(entity, 'id', '')
                }
                combined_entities.append(entity_dict)
            except Exception as e:
                logger.warning(f"è·³è¿‡æœ‰é—®é¢˜çš„æ–°å®ä½“: {str(e)}")
                continue
        
        # æ·»åŠ é‡‡æ ·å®ä½“ï¼ˆå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼‰
        for entity in sampled_entities:
            try:
                # åˆ›å»ºå®ä½“å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                entity_copy = dict(entity)
                entity_copy['source'] = 'neo4j_existing'
                combined_entities.append(entity_copy)
            except Exception as e:
                logger.warning(f"è·³è¿‡æœ‰é—®é¢˜çš„é‡‡æ ·å®ä½“: {str(e)}")
                continue
        
        logger.debug(f"åˆå¹¶å®ä½“å®Œæˆ: æ–°å®ä½“ {len(new_entities)} ä¸ªï¼Œé‡‡æ ·å®ä½“ {len(sampled_entities)} ä¸ªï¼Œæ€»è®¡ {len(combined_entities)} ä¸ª")
        
        return combined_entities
    
    def _group_entities_by_type(self, entities: List[Entity]) -> Dict[str, List[Entity]]:
        """
        æŒ‰ç±»å‹åˆ†ç»„å®ä½“
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            æŒ‰ç±»å‹åˆ†ç»„çš„å®ä½“å­—å…¸
        """
        grouped = {}
        
        for entity in entities:
            # å®‰å…¨åœ°è·å–å®ä½“ç±»å‹ï¼Œæ”¯æŒEntityå¯¹è±¡å’Œå­—å…¸æ ¼å¼
            if hasattr(entity, 'entity_type'):
                # Entityå¯¹è±¡
                entity_type = entity.entity_type or entity.type
            else:
                # å­—å…¸æ ¼å¼
                entity_type = entity.get('entity_type') or entity.get('type', 'unknown')
            
            if entity_type not in grouped:
                grouped[entity_type] = []
            grouped[entity_type].append(entity)
        
        return grouped
    
    async def get_unification_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿä¸€ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # è·å–å®ä½“ç»Ÿè®¡
            entity_stats = self.entity_updater.get_entity_statistics()
            
            # è·å–ç±»å‹åˆ†å¸ƒç»Ÿè®¡
            type_stats = self.entity_sampler.get_entity_types_with_counts()
            
            return {
                'entity_statistics': entity_stats,
                'type_distribution': type_stats,
                'config': {
                    'max_sample_entities_per_type': self.config.max_sample_entities_per_type,
                    'min_entities_for_unification': self.config.min_entities_for_unification,
                    'llm_confidence_threshold': self.config.llm_confidence_threshold,
                    'max_batch_size': self.config.max_batch_size
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿä¸€ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                'error': str(e),
                'entity_statistics': {},
                'type_distribution': {},
                'config': {}
            }
    
    async def manual_unify_entity_type(
        self,
        entity_type: str,
        limit: Optional[int] = None
    ) -> GlobalUnificationResult:
        """
        æ‰‹åŠ¨è§¦å‘ç‰¹å®šç±»å‹çš„å®ä½“ç»Ÿä¸€
        
        Args:
            entity_type: å®ä½“ç±»å‹
            limit: å¤„ç†æ•°é‡é™åˆ¶
            
        Returns:
            ç»Ÿä¸€ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"å¼€å§‹æ‰‹åŠ¨ç»Ÿä¸€ {entity_type} ç±»å‹å®ä½“ï¼Œé™åˆ¶: {limit}")
        
        try:
            # é‡‡æ ·è¯¥ç±»å‹çš„æ‰€æœ‰å®ä½“
            sample_limit = limit or self.config.max_sample_entities_per_type * 2
            
            # ç»Ÿä¸€ä½¿ç”¨åŒæ­¥æ–¹æ³•é‡‡æ ·
            sampled_entities = self.entity_sampler.sample_entities_by_type(
                entity_type=entity_type,
                limit=sample_limit
            )
            
            if len(sampled_entities) < self.config.min_entities_for_unification:
                logger.info(f"{entity_type} ç±»å‹å®ä½“æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç»Ÿä¸€")
                return GlobalUnificationResult(
                    success=True,
                    total_entities_processed=len(sampled_entities),
                    entities_merged=0,
                    entities_deleted=0,
                    relationships_updated=0,
                    processing_time=time.time() - start_time,
                    type_statistics={},
                    errors=[]
                )
            
            # LangGraph Agentåˆ†æ
            logger.info(f"å¼€å§‹Agentå»é‡åˆ†æï¼š{entity_type} ç±»å‹ï¼Œ{len(sampled_entities)} ä¸ªå®ä½“")
            
            # è½¬æ¢ä¸ºLangGraph Agentæ‰€éœ€çš„æ ¼å¼
            entity_dicts = []
            for entity in sampled_entities:
                try:
                    # å®‰å…¨åœ°è½¬æ¢å®ä½“ä¸ºAgentæ‰€éœ€æ ¼å¼
                    entity_dict = {
                        'name': entity.get('name', ''),
                        'type': entity.get('type', ''),
                        'description': entity.get('description', ''),
                        'properties': entity.get('properties', {})
                    }
                    if entity.get('id'):
                        entity_dict['id'] = entity.get('id')
                    entity_dicts.append(entity_dict)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“è½¬æ¢: {str(e)}")
                    continue
            
            analysis_result = await self.langgraph_agent.deduplicate_entities_list(entity_dicts, entity_type)
            
            # åº”ç”¨åˆå¹¶æ“ä½œ
            merge_operations = self._extract_merge_operations_from_agent_result(analysis_result, sampled_entities)
            update_result = self.entity_updater.apply_merge_operations(
                sampled_entities, merge_operations
            )
            
            processing_time = time.time() - start_time
            
            return GlobalUnificationResult(
                success=len(update_result['errors']) == 0,
                total_entities_processed=len(sampled_entities),
                entities_merged=update_result['merged_entities'],
                entities_deleted=update_result['deleted_entities'],
                relationships_updated=update_result['updated_relationships'],
                processing_time=processing_time,
                type_statistics={entity_type: update_result},
                errors=update_result['errors']
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"æ‰‹åŠ¨ç»Ÿä¸€ {entity_type} ç±»å‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            return GlobalUnificationResult(
                success=False,
                total_entities_processed=0,
                entities_merged=0,
                entities_deleted=0,
                relationships_updated=0,
                processing_time=processing_time,
                type_statistics={},
                errors=[error_msg]
            )
    
    def _get_entity_real_id(self, entity: Dict[str, Any]) -> Optional[str]:
        """
        è·å–å®ä½“çš„çœŸå®Neo4j ID
        
        Args:
            entity: å®ä½“æ•°æ®
            
        Returns:
            çœŸå®çš„Neo4j IDæˆ–None
        """
        # ä¼˜å…ˆçº§ï¼šelementId > identity > node_id > id
        for id_field in ['elementId', 'identity', 'node_id', 'id']:
            if entity.get(id_field):
                return str(entity[id_field])
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè®°å½•è­¦å‘Š
        logger.warning(f"å®ä½“ {entity.get('name', 'Unknown')} æ²¡æœ‰æœ‰æ•ˆçš„IDå­—æ®µ")
        return None
    
    def _extract_merge_operations_from_agent_result(self, analysis_result: Dict[str, Any], entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä»LangGraph Agentç»“æœä¸­æå–åˆå¹¶æ“ä½œï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            analysis_result: Agentåˆ†æç»“æœ
            entities: åŸå§‹å®ä½“åˆ—è¡¨ï¼ˆç»Ÿä¸€ä¸ºå­—å…¸æ ¼å¼ï¼‰
            
        Returns:
            åˆå¹¶æ“ä½œåˆ—è¡¨
        """
        logger.info("å¼€å§‹ä»Agentç»“æœä¸­æå–åˆå¹¶æ“ä½œï¼ˆå¢å¼ºç‰ˆï¼‰")
        
        merge_operations = []
        
        # ä»Agentç»“æœä¸­æå–åˆå¹¶ç»„
        merge_groups = analysis_result.get("merge_groups", [])
        
        logger.info(f"Agentè¿”å›äº† {len(merge_groups)} ä¸ªåˆå¹¶ç»„")
        
        # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šAgentç»“æœåˆ†æ
        logger.info("=== Agentåˆ†æç»“æœè¯¦æƒ… ===")
        logger.info(f"åˆ†ææ‘˜è¦: {analysis_result.get('analysis_summary', 'N/A')}")
        logger.info(f"åˆå¹¶ç»„æ•°é‡: {len(merge_groups)}")
        logger.info(f"ç‹¬ç«‹å®ä½“æ•°é‡: {len(analysis_result.get('independent_entities', []))}")
        logger.info(f"ä¸ç¡®å®šæ¡ˆä¾‹æ•°é‡: {len(analysis_result.get('uncertain_cases', []))}")
        
        for i, group in enumerate(merge_groups):
            logger.info(f"=== å¤„ç†åˆå¹¶ç»„ {i+1} ===")
            
            try:
                # ğŸ”§ å¢å¼ºçš„å­—æ®µæå–é€»è¾‘
                # ä¼˜å…ˆä½¿ç”¨å¢å¼ºåçš„IDå­—æ®µï¼Œé™çº§åˆ°ç´¢å¼•å­—æ®µ
                primary_entity_id = group.get("primary_entity_id")
                primary_entity_index = group.get("primary_entity_index") 
                primary_entity_str = group.get("primary_entity", "1")
                
                duplicate_entity_ids = group.get("duplicate_entity_ids", [])
                duplicate_indices = group.get("duplicate_indices", [])
                duplicate_strs = group.get("duplicates", [])
                
                logger.info(f"åŸå§‹å­—æ®µ:")
                logger.info(f"  - primary_entity: {primary_entity_str}")
                logger.info(f"  - primary_entity_id: {primary_entity_id}")
                logger.info(f"  - primary_entity_index: {primary_entity_index}")
                logger.info(f"  - duplicates: {duplicate_strs}")
                logger.info(f"  - duplicate_entity_ids: {duplicate_entity_ids}")
                logger.info(f"  - duplicate_indices: {duplicate_indices}")
                
                # ğŸ”§ æ™ºèƒ½ç´¢å¼•è§£æ
                if primary_entity_id and primary_entity_index is not None:
                    # ä½¿ç”¨å¢å¼ºåçš„IDå’Œç´¢å¼•
                    primary_index = primary_entity_index
                elif primary_entity_str.isdigit():
                    # ä»å­—ç¬¦ä¸²è½¬æ¢ç´¢å¼•ï¼ˆAgentè¿”å›çš„æ˜¯1-basedï¼‰
                    primary_index = int(primary_entity_str) - 1
                else:
                    logger.warning(f"æ— æ³•ç¡®å®šä¸»å®ä½“ç´¢å¼•: {primary_entity_str}")
                    continue
                
                # å¤„ç†é‡å¤å®ä½“ç´¢å¼•
                valid_duplicate_indices = []
                if duplicate_indices:
                    # ä½¿ç”¨å¢å¼ºåçš„ç´¢å¼•
                    valid_duplicate_indices = duplicate_indices
                else:
                    # ä»å­—ç¬¦ä¸²è½¬æ¢ç´¢å¼•
                    for dup_str in duplicate_strs:
                        if isinstance(dup_str, str) and dup_str.isdigit():
                            valid_duplicate_indices.append(int(dup_str) - 1)
                        elif isinstance(dup_str, int):
                            valid_duplicate_indices.append(dup_str - 1)
                
                logger.info(f"è§£æåç´¢å¼•:")
                logger.info(f"  - ä¸»å®ä½“ç´¢å¼•: {primary_index}")
                logger.info(f"  - é‡å¤å®ä½“ç´¢å¼•: {valid_duplicate_indices}")
                
                # ğŸ”§ ç´¢å¼•æœ‰æ•ˆæ€§éªŒè¯
                if primary_index < 0 or primary_index >= len(entities):
                    logger.warning(f"ä¸»å®ä½“ç´¢å¼• {primary_index} è¶…å‡ºèŒƒå›´ [0, {len(entities)-1}]ï¼Œè·³è¿‡")
                    continue
                
                # è¿‡æ»¤æœ‰æ•ˆçš„é‡å¤å®ä½“ç´¢å¼•
                filtered_duplicate_indices = []
                for dup_idx in valid_duplicate_indices:
                    if 0 <= dup_idx < len(entities) and dup_idx != primary_index:
                        filtered_duplicate_indices.append(dup_idx)
                    else:
                        logger.warning(f"é‡å¤å®ä½“ç´¢å¼• {dup_idx} æ— æ•ˆæˆ–ä¸ä¸»å®ä½“ç›¸åŒï¼Œè·³è¿‡")
                
                if not filtered_duplicate_indices:
                    logger.warning(f"åˆå¹¶ç»„ {i+1} æ— æœ‰æ•ˆé‡å¤å®ä½“ç´¢å¼•ï¼Œè·³è¿‡")
                    continue
                
                # ğŸ”§ æ™ºèƒ½ä¸»å®ä½“é€‰æ‹©ï¼šä¼˜å…ˆé€‰æ‹©Neo4jä¸­çš„ç°æœ‰å®ä½“ä½œä¸ºä¸»å®ä½“
                all_indices = [primary_index] + filtered_duplicate_indices
                all_entities_in_group = [entities[idx] for idx in all_indices]
                
                # æŸ¥æ‰¾Neo4jç°æœ‰å®ä½“ï¼ˆsourceä¸º'neo4j_existing'ï¼‰
                neo4j_entities = []
                new_entities = []
                
                for idx, entity in zip(all_indices, all_entities_in_group):
                    if entity.get('source') == 'neo4j_existing':
                        neo4j_entities.append((idx, entity))
                    else:
                        new_entities.append((idx, entity))
                
                logger.info(f"å®ä½“æ¥æºåˆ†æ:")
                logger.info(f"  - Neo4jç°æœ‰å®ä½“: {len(neo4j_entities)} ä¸ª")
                logger.info(f"  - æ–°æ–‡æ¡£å®ä½“: {len(new_entities)} ä¸ª")
                
                # ğŸ”§ æ™ºèƒ½é€‰æ‹©ä¸»å®ä½“ï¼šä¼˜å…ˆé€‰æ‹©Neo4jç°æœ‰å®ä½“
                if neo4j_entities:
                    # é€‰æ‹©ç¬¬ä¸€ä¸ªNeo4jç°æœ‰å®ä½“ä½œä¸ºä¸»å®ä½“
                    actual_primary_index, actual_primary_entity = neo4j_entities[0]
                    
                    # å…¶ä»–æ‰€æœ‰å®ä½“éƒ½ä½œä¸ºé‡å¤å®ä½“
                    actual_duplicate_indices = []
                    actual_duplicate_entities = []
                    
                    # æ­£ç¡®éå†ï¼šä½¿ç”¨zipæ¥è·å–ç´¢å¼•å’Œå®ä½“çš„å¯¹åº”å…³ç³»
                    for idx, entity in zip(all_indices, all_entities_in_group):
                        if idx != actual_primary_index:
                            actual_duplicate_indices.append(idx)
                            actual_duplicate_entities.append(entity)
                    
                    logger.info(f"ğŸ”„ æ™ºèƒ½ä¸»å®ä½“é€‰æ‹©ï¼šé€‰æ‹©Neo4jç°æœ‰å®ä½“ä½œä¸ºä¸»å®ä½“")
                    logger.info(f"  - æ–°ä¸»å®ä½“: [{actual_primary_index}] {actual_primary_entity.get('name')} (Neo4jç°æœ‰)")
                    logger.info(f"  - å¾…åˆå¹¶å®ä½“: {[(idx, entities[idx].get('name')) for idx in actual_duplicate_indices]}")
                    
                else:
                    # å¦‚æœæ²¡æœ‰Neo4jç°æœ‰å®ä½“ï¼Œä½¿ç”¨åŸå§‹é€‰æ‹©
                    actual_primary_index = primary_index
                    actual_primary_entity = entities[primary_index]
                    actual_duplicate_indices = filtered_duplicate_indices
                    actual_duplicate_entities = [entities[idx] for idx in filtered_duplicate_indices]
                    
                    logger.info(f"ä½¿ç”¨åŸå§‹ä¸»å®ä½“é€‰æ‹©ï¼ˆæ— Neo4jç°æœ‰å®ä½“ï¼‰")
                
                # ğŸ”§ ç‰¹æ®Šæƒ…å†µæ£€æŸ¥ï¼šå¦‚æœåªæœ‰å®Œå…¨ç›¸åŒçš„å®ä½“ï¼Œè·³è¿‡åˆå¹¶
                if len(actual_duplicate_indices) == 0:
                    logger.info(f"åˆå¹¶ç»„ {i+1} ç»è¿‡æ™ºèƒ½é€‰æ‹©åæ— éœ€åˆå¹¶ï¼Œè·³è¿‡")
                    continue
                
                # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ–‡æ¡£å®ä½“ä¸ç°æœ‰å®ä½“çš„åŒ¹é…æƒ…å†µ
                primary_source = entities[actual_primary_index].get('source')
                duplicate_sources = [entities[idx].get('source') for idx in actual_duplicate_indices]
                all_sources = [primary_source] + duplicate_sources
                all_names = [entities[idx].get('name') for idx in [actual_primary_index] + actual_duplicate_indices]
                
                # å¦‚æœæ‰€æœ‰å®ä½“åç§°ç›¸åŒï¼Œä¸”åŒ…å«æ–°æ–‡æ¡£å®ä½“å’ŒNeo4jç°æœ‰å®ä½“çš„ç»„åˆ
                if len(set(all_names)) == 1:
                    has_new_document = 'new_document' in all_sources
                    has_neo4j_existing = 'neo4j_existing' in all_sources
                    
                    if has_new_document and has_neo4j_existing:
                        logger.info(f"åˆå¹¶ç»„ {i+1} æ˜¯æ–°æ–‡æ¡£å®ä½“ä¸ç°æœ‰å®ä½“çš„åŒ¹é…ï¼Œè¿™æ˜¯æ­£å¸¸çš„å®ä½“ç»Ÿä¸€æƒ…å†µ")
                        # ç»§ç»­å¤„ç†ï¼Œè¿™æ˜¯æ­£å¸¸çš„ç»Ÿä¸€æ“ä½œ
                    elif len(set(all_sources)) == 1:
                        logger.info(f"åˆå¹¶ç»„ {i+1} ä¸­æ‰€æœ‰å®ä½“æ¥æºç›¸åŒä¸”åç§°ç›¸åŒï¼Œè·³è¿‡ä¸å¿…è¦çš„åˆå¹¶")
                        continue
                
                # å®‰å…¨åœ°è®¿é—®å®ä½“åç§°
                primary_name = actual_primary_entity.get('name', 'Unknown')
                duplicate_names = [dup.get('name', 'Unknown') for dup in actual_duplicate_entities]
                
                logger.info(f"æœ€ç»ˆå®ä½“æ˜ å°„:")
                logger.info(f"  - ä¸»å®ä½“: [{actual_primary_index}] {primary_name} ({actual_primary_entity.get('source', 'unknown')})")
                logger.info(f"  - é‡å¤å®ä½“: {[(idx, entities[idx].get('name', 'Unknown'), entities[idx].get('source', 'unknown')) for idx in actual_duplicate_indices]}")
                
                # ğŸ”§ åˆ›å»ºå¢å¼ºçš„åˆå¹¶æ“ä½œ
                merge_operation = {
                    "primary_entity": actual_primary_entity,
                    "duplicate_entities": actual_duplicate_entities,
                    "primary_entity_index": actual_primary_index,
                    "duplicate_indices": actual_duplicate_indices,
                    "merged_name": group.get("merged_name", primary_name),
                    "merged_description": group.get("merged_description", actual_primary_entity.get('description', '')),
                    "confidence": group.get("confidence", 0.0),
                    "reason": group.get("reason", "LangGraph Agentåˆ†æç»“æœ"),
                    "wikipedia_evidence": group.get("wikipedia_evidence", ""),
                    # æ–°å¢ï¼šåŸå§‹å­—æ®µä¿ç•™
                    "original_group": group
                }
                
                merge_operations.append(merge_operation)
                
                logger.info(f"âœ… åˆå¹¶æ“ä½œåˆ›å»ºæˆåŠŸ: {primary_name} <- {duplicate_names}")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†åˆå¹¶ç»„ {i+1} å¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                continue
        
        logger.info(f"=== åˆå¹¶æ“ä½œæå–å®Œæˆ ===")
        logger.info(f"æˆåŠŸæå– {len(merge_operations)} ä¸ªåˆå¹¶æ“ä½œ")
        
        # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šæœ€ç»ˆåˆå¹¶æ“ä½œæ‘˜è¦
        if merge_operations:
            logger.info("ğŸ“‹ åˆå¹¶æ“ä½œæ‘˜è¦:")
            for i, op in enumerate(merge_operations):
                primary_name = op["primary_entity"].get("name", "Unknown")
                duplicate_names = [dup.get("name", "Unknown") for dup in op["duplicate_entities"]]
                logger.info(f"  {i+1}. {primary_name} <- {duplicate_names} (ç½®ä¿¡åº¦: {op['confidence']})")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•åˆå¹¶æ“ä½œ")
        
        return merge_operations


# å…¨å±€å®ä¾‹
_global_unification_service = None

def get_global_entity_unification_service(
    config: Optional[GlobalUnificationConfig] = None
) -> GlobalEntityUnificationService:
    """è·å–å…¨å±€å®ä½“ç»Ÿä¸€æœåŠ¡å®ä¾‹"""
    global _global_unification_service
    if _global_unification_service is None or config is not None:
        _global_unification_service = GlobalEntityUnificationService(config)
    return _global_unification_service