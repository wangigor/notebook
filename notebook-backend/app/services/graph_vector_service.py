import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from app.core.config import settings
from app.services.neo4j_service import Neo4jService
from app.services.dashscope_singleton import get_dashscope_client

logger = logging.getLogger(__name__)

class GraphVectorService:
    """å›¾è°±å‘é‡åŒ–æœåŠ¡
    
    ä¸“é—¨ä¸ºçŸ¥è¯†å›¾è°±æ„å»ºæä¾›å‘é‡åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - åˆ†å—æ–‡æœ¬å‘é‡åŒ–
    - å®ä½“å‘é‡åŒ–
    - å‘é‡å­˜å‚¨åˆ°Neo4j
    - å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å›¾è°±å‘é‡åŒ–æœåŠ¡"""
        self.embedding_model = None
        self.neo4j_service = Neo4jService()
        self._initialize_embedding_model()
        logger.info("å›¾è°±å‘é‡åŒ–æœåŠ¡å·²åˆå§‹åŒ–")
    
    def _initialize_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨DashScopeå•ä¾‹"""
        try:
            logger.info("ğŸ”§ GraphVectorServiceä½¿ç”¨DashScopeå•ä¾‹...")
            
            # ä½¿ç”¨DashScopeå•ä¾‹
            self.embedding_model = get_dashscope_client()
            
            # æµ‹è¯•åµŒå…¥æ¨¡å‹
            test_text = "æµ‹è¯•å‘é‡åŒ–"
            test_vector = self.embedding_model.embed_query(test_text)
            logger.info(f"âœ… GraphVectorServiceåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_vector)}")
            
        except Exception as e:
            logger.error(f"âŒ GraphVectorServiceåˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
            self.embedding_model = None
    
    async def vectorize_chunks(self, chunks: List[Any]) -> List[Dict[str, Any]]:
        """å‘é‡åŒ–åˆ†å—
        
        Args:
            chunks: åˆ†å—åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«contentã€metadataç­‰
            
        Returns:
            å¸¦æœ‰å‘é‡çš„åˆ†å—åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å‘é‡åŒ– {len(chunks)} ä¸ªåˆ†å—")
        
        vectorized_chunks = []
        
        try:
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            texts = [chunk.content if hasattr(chunk, 'content') else chunk.get('content', '') for chunk in chunks]
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡
            if self.embedding_model:
                try:
                    embeddings = self.embedding_model.embed_documents(texts)
                    logger.info(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
                except Exception as e:
                    logger.error(f"å‘é‡ç”Ÿæˆå¤±è´¥: {str(e)}")
                    # ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºå¤‡é€‰
                    embeddings = [self._generate_random_vector() for _ in texts]
                    logger.warning("ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºå¤‡é€‰")
            else:
                # ä½¿ç”¨éšæœºå‘é‡
                embeddings = [self._generate_random_vector() for _ in texts]
                logger.warning("æœªé…ç½®åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨éšæœºå‘é‡")
            
            # å°†å‘é‡æ·»åŠ åˆ°åˆ†å—ä¸­
            for i, chunk in enumerate(chunks):
                if hasattr(chunk, 'to_dict'):
                    chunk_dict = chunk.to_dict()
                else:
                    chunk_dict = chunk
                    
                vectorized_chunk = {
                    **chunk_dict,
                    'embedding': embeddings[i],
                    'vector_dimension': len(embeddings[i])
                }
                vectorized_chunks.append(vectorized_chunk)
            
            logger.info(f"åˆ†å—å‘é‡åŒ–å®Œæˆ: {len(vectorized_chunks)} ä¸ª")
            return vectorized_chunks
            
        except Exception as e:
            logger.error(f"åˆ†å—å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def vectorize_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å‘é‡åŒ–å®ä½“
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            å¸¦æœ‰å‘é‡çš„å®ä½“åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å‘é‡åŒ– {len(entities)} ä¸ªå®ä½“")
        
        vectorized_entities = []
        
        try:
            # æ„å»ºå®ä½“æ–‡æœ¬è¡¨ç¤º
            entity_texts = []
            for entity in entities:
                text_parts = []
                
                # æ·»åŠ å®ä½“åç§°
                if entity.get('name'):
                    text_parts.append(entity['name'])
                
                # æ·»åŠ å®ä½“ç±»å‹
                if entity.get('type'):
                    text_parts.append(f"ç±»å‹:{entity['type']}")
                
                # æ·»åŠ å®ä½“æè¿°
                if entity.get('description'):
                    text_parts.append(entity['description'])
                
                # æ·»åŠ å®ä½“å±æ€§
                if entity.get('properties'):
                    for key, value in entity['properties'].items():
                        if isinstance(value, str) and value.strip():
                            text_parts.append(f"{key}:{value}")
                
                entity_text = " ".join(text_parts)
                entity_texts.append(entity_text)
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡
            if self.embedding_model:
                try:
                    embeddings = self.embedding_model.embed_documents(entity_texts)
                    logger.info(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå®ä½“å‘é‡")
                except Exception as e:
                    logger.error(f"å®ä½“å‘é‡ç”Ÿæˆå¤±è´¥: {str(e)}")
                    embeddings = [self._generate_random_vector() for _ in entity_texts]
            else:
                embeddings = [self._generate_random_vector() for _ in entity_texts]
            
            # å°†å‘é‡æ·»åŠ åˆ°å®ä½“ä¸­
            for i, entity in enumerate(entities):
                vectorized_entity = {
                    **entity,
                    'embedding': embeddings[i],
                    'vector_dimension': len(embeddings[i]),
                    'text_representation': entity_texts[i]
                }
                vectorized_entities.append(vectorized_entity)
            
            logger.info(f"å®ä½“å‘é‡åŒ–å®Œæˆ: {len(vectorized_entities)} ä¸ª")
            return vectorized_entities
            
        except Exception as e:
            logger.error(f"å®ä½“å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def store_vectors_to_neo4j(self, vectorized_data: List[Dict[str, Any]], 
                                   node_label: str) -> Dict[str, Any]:
        """å°†å‘é‡æ•°æ®å­˜å‚¨åˆ°Neo4j
        
        Args:
            vectorized_data: åŒ…å«å‘é‡çš„æ•°æ®åˆ—è¡¨
            node_label: Neo4jèŠ‚ç‚¹æ ‡ç­¾
            
        Returns:
            å­˜å‚¨ç»“æœç»Ÿè®¡
        """
        logger.info(f"å¼€å§‹å°† {len(vectorized_data)} ä¸ª{node_label}å‘é‡å­˜å‚¨åˆ°Neo4j")
        
        try:
            stored_count = 0
            batch_size = settings.GRAPH_BATCH_SIZE
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(vectorized_data), batch_size):
                batch = vectorized_data[i:i + batch_size]
                
                # æ„å»ºæ‰¹é‡æ’å…¥æŸ¥è¯¢
                query = f"""
                UNWIND $data AS item
                MERGE (n:{node_label} {{id: item.id}})
                SET n += item.properties,
                    n.embedding = item.embedding,
                    n.vector_dimension = item.vector_dimension,
                    n.updated_at = datetime()
                RETURN count(n) as created_count
                """
                
                # å‡†å¤‡æ•°æ®
                batch_data = []
                for item in batch:
                    node_data = {
                        'id': item.get('id', f"{node_label}_{i}_{len(batch_data)}"),
                        'properties': {
                            'name': item.get('name', ''),
                            'type': item.get('type', ''),
                            'description': item.get('description', ''),
                            'content': item.get('content', ''),
                            **item.get('properties', {})
                        },
                        'embedding': item['embedding'],
                        'vector_dimension': item['vector_dimension']
                    }
                    batch_data.append(node_data)
                
                # æ‰§è¡ŒæŸ¥è¯¢
                result = self.neo4j_service.execute_write_query(query, {'data': batch_data})
                batch_stored = result[0]['created_count'] if result else 0
                stored_count += batch_stored
                
                logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1} å­˜å‚¨å®Œæˆ: {batch_stored} ä¸ªèŠ‚ç‚¹")
            
            # åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            await self._ensure_vector_index(node_label)
            
            logger.info(f"å‘é‡å­˜å‚¨å®Œæˆ: {stored_count} ä¸ª{node_label}èŠ‚ç‚¹")
            
            return {
                'stored_count': stored_count,
                'node_label': node_label,
                'vector_dimension': vectorized_data[0]['vector_dimension'] if vectorized_data else 0
            }
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆ°Neo4jå¤±è´¥: {str(e)}")
            raise
    
    async def _ensure_vector_index(self, node_label: str):
        """ç¡®ä¿å‘é‡ç´¢å¼•å­˜åœ¨
        
        Args:
            node_label: èŠ‚ç‚¹æ ‡ç­¾
        """
        try:
            index_name = f"{node_label.lower()}_vector_index"
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            check_query = f"""
            SHOW INDEXES 
            WHERE name = '{index_name}'
            """
            
            existing = self.neo4j_service.execute_query(check_query)
            
            if not existing:
                # åˆ›å»ºå‘é‡ç´¢å¼•
                create_query = f"""
                CREATE VECTOR INDEX {index_name} IF NOT EXISTS
                FOR (n:{node_label}) ON n.embedding
                OPTIONS {{
                    indexConfig: {{
                        `vector.dimensions`: {settings.VECTOR_SIZE},
                        `vector.similarity_function`: 'cosine'
                    }}
                }}
                """
                
                self.neo4j_service.execute_write_query(create_query)
                logger.info(f"å‘é‡ç´¢å¼• {index_name} åˆ›å»ºæˆåŠŸ")
            else:
                logger.info(f"å‘é‡ç´¢å¼• {index_name} å·²å­˜åœ¨")
                
        except Exception as e:
            logger.warning(f"åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(e)}")
    
    def calculate_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
        
        Args:
            vector1: ç¬¬ä¸€ä¸ªå‘é‡
            vector2: ç¬¬äºŒä¸ªå‘é‡
            
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å€¼
        """
        try:
            v1 = np.array(vector1)
            v2 = np.array(vector2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    async def find_similar_vectors(self, query_vector: List[float], 
                                 node_label: str, limit: int = 10,
                                 min_similarity: float = 0.7) -> List[Dict[str, Any]]:
        """åœ¨Neo4jä¸­æŸ¥æ‰¾ç›¸ä¼¼å‘é‡
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            node_label: èŠ‚ç‚¹æ ‡ç­¾
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸ä¼¼èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            index_name = f"{node_label.lower()}_vector_index"
            
            query = f"""
            CALL db.index.vector.queryNodes('{index_name}', {limit}, $query_vector)
            YIELD node, score
            WHERE score >= $min_similarity
            RETURN node, score
            ORDER BY score DESC
            """
            
            result = self.neo4j_service.execute_query(query, {
                'query_vector': query_vector,
                'min_similarity': min_similarity
            })
            
            similar_nodes = []
            for record in result:
                similar_nodes.append({
                    'node': dict(record['node']),
                    'similarity': record['score']
                })
            
            logger.info(f"æ‰¾åˆ° {len(similar_nodes)} ä¸ªç›¸ä¼¼èŠ‚ç‚¹")
            return similar_nodes
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼å‘é‡å¤±è´¥: {str(e)}")
            return []
    
    def _generate_random_vector(self) -> List[float]:
        """ç”Ÿæˆéšæœºå‘é‡ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Returns:
            éšæœºå‘é‡
        """
        return np.random.randn(settings.VECTOR_SIZE).tolist()
    
    async def get_vector_statistics(self) -> Dict[str, Any]:
        """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            query = """
            MATCH (n)
            WHERE n.embedding IS NOT NULL
            WITH labels(n) as node_labels, count(n) as count, 
                 avg(n.vector_dimension) as avg_dimension
            RETURN node_labels[0] as label, count, avg_dimension
            """
            
            result = self.neo4j_service.execute_query(query)
            
            statistics = {
                'total_vectorized_nodes': 0,
                'by_label': {}
            }
            
            for record in result:
                label = record['label']
                count = record['count']
                avg_dim = record['avg_dimension']
                
                statistics['by_label'][label] = {
                    'count': count,
                    'avg_dimension': avg_dim
                }
                statistics['total_vectorized_nodes'] += count
            
            statistics['embedding_model'] = settings.DASHSCOPE_EMBEDDING_MODEL
            statistics['vector_dimension'] = settings.VECTOR_SIZE
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–å‘é‡ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {'error': str(e)}