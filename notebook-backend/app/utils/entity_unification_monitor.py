# -*- coding: utf-8 -*-
"""
å®ä½“ç»Ÿä¸€ç›‘æ§å·¥å…·
æä¾›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§ã€æ—¥å¿—å¢å¼ºå’Œåº¦é‡æ”¶é›†åŠŸèƒ½
"""
import logging
import time
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import threading

logger = logging.getLogger(__name__)


@dataclass
class UnificationMetrics:
    """ç»Ÿä¸€æŒ‡æ ‡"""
    session_id: str
    start_time: float
    end_time: Optional[float] = None
    
    # è¾“å…¥ç»Ÿè®¡
    input_entity_count: int = 0
    input_avg_quality: float = 0.0
    
    # å¤„ç†é˜¶æ®µè€—æ—¶
    preprocessing_time: float = 0.0
    embedding_time: float = 0.0
    similarity_matrix_time: float = 0.0
    clustering_time: float = 0.0
    merging_time: float = 0.0
    
    # è¾“å‡ºç»Ÿè®¡
    output_entity_count: int = 0
    merge_operation_count: int = 0
    reduction_rate: float = 0.0
    
    # è´¨é‡æŒ‡æ ‡
    avg_merge_confidence: float = 0.0
    conflict_count: int = 0
    error_count: int = 0
    
    # æ€§èƒ½æŒ‡æ ‡
    total_processing_time: float = 0.0
    entities_per_second: float = 0.0
    cache_hit_rate: float = 0.0
    memory_usage_mb: float = 0.0


class EntityUnificationMonitor:
    """å®ä½“ç»Ÿä¸€ç›‘æ§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        self._metrics_history = []
        self._current_session = None
        self._lock = threading.Lock()
        
        # é…ç½®è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
        self._setup_detailed_logging()
        
        logger.info("ğŸ” å®ä½“ç»Ÿä¸€ç›‘æ§å™¨å·²å¯åŠ¨")
    
    def _setup_detailed_logging(self):
        """é…ç½®è¯¦ç»†çš„æ—¥å¿—æ ¼å¼"""
        # åˆ›å»ºä¸“é—¨çš„ç»Ÿä¸€æ—¥å¿—è®°å½•å™¨
        self.unification_logger = logging.getLogger("entity_unification")
        self.unification_logger.setLevel(logging.DEBUG)
        
        # é¿å…é‡å¤æ·»åŠ handler
        if not self.unification_logger.handlers:
            # åˆ›å»ºæ ¼å¼åŒ–å™¨
            formatter = logging.Formatter(
                '[%(asctime)s] ğŸ”— UNIFICATION | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            
            # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            console_handler.setLevel(logging.INFO)
            
            self.unification_logger.addHandler(console_handler)
            self.unification_logger.propagate = False
    
    def start_session(self, session_id: Optional[str] = None) -> str:
        """å¼€å§‹æ–°çš„ç»Ÿä¸€ä¼šè¯"""
        if session_id is None:
            session_id = f"unification_{int(time.time() * 1000)}"
        
        with self._lock:
            self._current_session = UnificationMetrics(
                session_id=session_id,
                start_time=time.time()
            )
        
        self.unification_logger.info(f"ğŸ“Š å¼€å§‹ç»Ÿä¸€ä¼šè¯: {session_id}")
        return session_id
    
    def log_preprocessing_start(self, entity_count: int):
        """è®°å½•é¢„å¤„ç†å¼€å§‹"""
        if self._current_session:
            self._current_session.input_entity_count = entity_count
            
        self.unification_logger.info(f"ğŸ”„ é¢„å¤„ç†é˜¶æ®µå¼€å§‹ | è¾“å…¥å®ä½“æ•°é‡: {entity_count}")
    
    def log_preprocessing_complete(self, valid_entity_count: int, duration: float):
        """è®°å½•é¢„å¤„ç†å®Œæˆ"""
        if self._current_session:
            self._current_session.preprocessing_time = duration
            
        reduction = self._current_session.input_entity_count - valid_entity_count if self._current_session else 0
        
        self.unification_logger.info(
            f"âœ… é¢„å¤„ç†å®Œæˆ | æœ‰æ•ˆå®ä½“: {valid_entity_count} | "
            f"è¿‡æ»¤: {reduction} | è€—æ—¶: {duration:.3f}s"
        )
    
    def log_embedding_start(self, entities_need_embedding: int):
        """è®°å½•embeddingå¼€å§‹"""
        self.unification_logger.info(f"ğŸ§  Embeddingç”Ÿæˆå¼€å§‹ | éœ€å¤„ç†å®ä½“: {entities_need_embedding}")
    
    def log_embedding_complete(self, successful_count: int, duration: float, cache_hit_rate: float):
        """è®°å½•embeddingå®Œæˆ"""
        if self._current_session:
            self._current_session.embedding_time = duration
            self._current_session.cache_hit_rate = cache_hit_rate
        
        self.unification_logger.info(
            f"âœ… Embeddingç”Ÿæˆå®Œæˆ | æˆåŠŸ: {successful_count} | "
            f"è€—æ—¶: {duration:.3f}s | ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}"
        )
    
    def log_similarity_matrix_start(self, entity_count: int):
        """è®°å½•ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå¼€å§‹"""
        matrix_size = entity_count * entity_count
        self.unification_logger.info(f"ğŸ“ ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå¼€å§‹ | å®ä½“æ•°: {entity_count} | çŸ©é˜µå¤§å°: {matrix_size}")
    
    def log_similarity_matrix_complete(self, comparison_count: int, valid_pairs: int, duration: float):
        """è®°å½•ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå®Œæˆ"""
        if self._current_session:
            self._current_session.similarity_matrix_time = duration
        
        density = valid_pairs / comparison_count if comparison_count > 0 else 0
        
        self.unification_logger.info(
            f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µå®Œæˆ | æ¯”è¾ƒæ¬¡æ•°: {comparison_count} | "
            f"æœ‰æ•ˆç›¸ä¼¼åº¦å¯¹: {valid_pairs} | å¯†åº¦: {density:.1%} | è€—æ—¶: {duration:.3f}s"
        )
    
    def log_clustering_start(self, threshold: float):
        """è®°å½•èšç±»å¼€å§‹"""
        self.unification_logger.info(f"ğŸ”— èšç±»åˆ†æå¼€å§‹ | é˜ˆå€¼: {threshold:.3f}")
    
    def log_clustering_complete(self, cluster_count: int, duration: float):
        """è®°å½•èšç±»å®Œæˆ"""
        if self._current_session:
            self._current_session.clustering_time = duration
        
        self.unification_logger.info(f"âœ… èšç±»åˆ†æå®Œæˆ | å‘ç°èšç±»: {cluster_count} | è€—æ—¶: {duration:.3f}s")
    
    def log_merge_operation(self, primary_entity_name: str, secondary_entity_name: str, 
                          decision: str, similarity_score: float, conflicts: int):
        """è®°å½•å•æ¬¡åˆå¹¶æ“ä½œ"""
        conflict_indicator = "âš ï¸" if conflicts > 0 else "âœ…"
        
        self.unification_logger.debug(
            f"{conflict_indicator} åˆå¹¶æ“ä½œ | {primary_entity_name} + {secondary_entity_name} | "
            f"å†³ç­–: {decision} | ç›¸ä¼¼åº¦: {similarity_score:.3f} | å†²çª: {conflicts}"
        )
    
    def log_merging_complete(self, merge_count: int, conflict_count: int, duration: float):
        """è®°å½•åˆå¹¶é˜¶æ®µå®Œæˆ"""
        if self._current_session:
            self._current_session.merging_time = duration
            self._current_session.merge_operation_count = merge_count
            self._current_session.conflict_count = conflict_count
        
        self.unification_logger.info(
            f"âœ… å®ä½“åˆå¹¶å®Œæˆ | åˆå¹¶æ“ä½œ: {merge_count} | "
            f"å†²çªå¤„ç†: {conflict_count} | è€—æ—¶: {duration:.3f}s"
        )
    
    def log_error(self, error_type: str, error_message: str, context: Dict[str, Any] = None):
        """è®°å½•é”™è¯¯"""
        if self._current_session:
            self._current_session.error_count += 1
        
        context_str = f" | ä¸Šä¸‹æ–‡: {json.dumps(context, ensure_ascii=False)}" if context else ""
        
        self.unification_logger.error(f"âŒ {error_type} | {error_message}{context_str}")
    
    def complete_session(self, output_entity_count: int, avg_merge_confidence: float) -> UnificationMetrics:
        """å®Œæˆç»Ÿä¸€ä¼šè¯"""
        if not self._current_session:
            logger.warning("æ²¡æœ‰æ´»è·ƒçš„ç»Ÿä¸€ä¼šè¯")
            return None
        
        with self._lock:
            # å®Œæˆä¼šè¯
            self._current_session.end_time = time.time()
            self._current_session.output_entity_count = output_entity_count
            self._current_session.avg_merge_confidence = avg_merge_confidence
            
            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            self._current_session.total_processing_time = (
                self._current_session.end_time - self._current_session.start_time
            )
            
            if self._current_session.input_entity_count > 0:
                self._current_session.reduction_rate = (
                    (self._current_session.input_entity_count - output_entity_count) / 
                    self._current_session.input_entity_count
                )
                
                self._current_session.entities_per_second = (
                    self._current_session.input_entity_count / 
                    max(self._current_session.total_processing_time, 0.001)
                )
            
            # è®°å½•ä¼šè¯å®Œæˆ
            metrics = self._current_session
            self._metrics_history.append(metrics)
            
            # è®°å½•è¯¦ç»†çš„ä¼šè¯æ€»ç»“
            self._log_session_summary(metrics)
            
            # æ¸…ç©ºå½“å‰ä¼šè¯
            self._current_session = None
            
            return metrics
    
    def _log_session_summary(self, metrics: UnificationMetrics):
        """è®°å½•ä¼šè¯æ€»ç»“"""
        self.unification_logger.info("="*80)
        self.unification_logger.info(f"ğŸ“‹ ç»Ÿä¸€ä¼šè¯æ€»ç»“ | {metrics.session_id}")
        self.unification_logger.info("="*80)
        
        # åŸºç¡€ç»Ÿè®¡
        self.unification_logger.info(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        self.unification_logger.info(f"  â””â”€ è¾“å…¥å®ä½“: {metrics.input_entity_count}")
        self.unification_logger.info(f"  â””â”€ è¾“å‡ºå®ä½“: {metrics.output_entity_count}")
        self.unification_logger.info(f"  â””â”€ å‡å°‘ç‡: {metrics.reduction_rate:.1%}")
        self.unification_logger.info(f"  â””â”€ åˆå¹¶æ“ä½œ: {metrics.merge_operation_count}")
        
        # å¤„ç†æ—¶é—´åˆ†æ
        self.unification_logger.info(f"â±ï¸ å¤„ç†æ—¶é—´åˆ†æ:")
        self.unification_logger.info(f"  â””â”€ æ€»æ—¶é—´: {metrics.total_processing_time:.3f}s")
        self.unification_logger.info(f"  â””â”€ é¢„å¤„ç†: {metrics.preprocessing_time:.3f}s ({metrics.preprocessing_time/metrics.total_processing_time*100:.1f}%)")
        self.unification_logger.info(f"  â””â”€ Embedding: {metrics.embedding_time:.3f}s ({metrics.embedding_time/metrics.total_processing_time*100:.1f}%)")
        self.unification_logger.info(f"  â””â”€ ç›¸ä¼¼åº¦çŸ©é˜µ: {metrics.similarity_matrix_time:.3f}s ({metrics.similarity_matrix_time/metrics.total_processing_time*100:.1f}%)")
        self.unification_logger.info(f"  â””â”€ èšç±»åˆ†æ: {metrics.clustering_time:.3f}s ({metrics.clustering_time/metrics.total_processing_time*100:.1f}%)")
        self.unification_logger.info(f"  â””â”€ å®ä½“åˆå¹¶: {metrics.merging_time:.3f}s ({metrics.merging_time/metrics.total_processing_time*100:.1f}%)")
        
        # è´¨é‡æŒ‡æ ‡
        self.unification_logger.info(f"ğŸ¯ è´¨é‡æŒ‡æ ‡:")
        self.unification_logger.info(f"  â””â”€ å¹³å‡åˆå¹¶ç½®ä¿¡åº¦: {metrics.avg_merge_confidence:.3f}")
        self.unification_logger.info(f"  â””â”€ å¤„ç†é€Ÿåº¦: {metrics.entities_per_second:.1f} å®ä½“/ç§’")
        self.unification_logger.info(f"  â””â”€ ç¼“å­˜å‘½ä¸­ç‡: {metrics.cache_hit_rate:.1%}")
        self.unification_logger.info(f"  â””â”€ å†²çªæ•°é‡: {metrics.conflict_count}")
        self.unification_logger.info(f"  â””â”€ é”™è¯¯æ•°é‡: {metrics.error_count}")
        
        self.unification_logger.info("="*80)
    
    def get_performance_report(self, last_n_sessions: int = 10) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self._metrics_history:
            return {"error": "æ²¡æœ‰å†å²æ•°æ®"}
        
        recent_sessions = self._metrics_history[-last_n_sessions:]
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_reduction_rate = sum(m.reduction_rate for m in recent_sessions) / len(recent_sessions)
        avg_processing_time = sum(m.total_processing_time for m in recent_sessions) / len(recent_sessions)
        avg_entities_per_second = sum(m.entities_per_second for m in recent_sessions) / len(recent_sessions)
        avg_merge_confidence = sum(m.avg_merge_confidence for m in recent_sessions) / len(recent_sessions)
        avg_cache_hit_rate = sum(m.cache_hit_rate for m in recent_sessions) / len(recent_sessions)
        
        total_entities_processed = sum(m.input_entity_count for m in recent_sessions)
        total_merges = sum(m.merge_operation_count for m in recent_sessions)
        total_conflicts = sum(m.conflict_count for m in recent_sessions)
        total_errors = sum(m.error_count for m in recent_sessions)
        
        return {
            "sessions_analyzed": len(recent_sessions),
            "total_entities_processed": total_entities_processed,
            "averages": {
                "reduction_rate": avg_reduction_rate,
                "processing_time": avg_processing_time,
                "entities_per_second": avg_entities_per_second,
                "merge_confidence": avg_merge_confidence,
                "cache_hit_rate": avg_cache_hit_rate
            },
            "totals": {
                "merge_operations": total_merges,
                "conflicts_resolved": total_conflicts,
                "errors_encountered": total_errors
            },
            "latest_session": asdict(recent_sessions[-1]) if recent_sessions else None
        }
    
    def export_metrics_history(self, filepath: str):
        """å¯¼å‡ºæŒ‡æ ‡å†å²"""
        import json
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump([asdict(m) for m in self._metrics_history], f, 
                     ensure_ascii=False, indent=2)
        
        logger.info(f"æŒ‡æ ‡å†å²å·²å¯¼å‡ºåˆ°: {filepath}")


# ğŸ†• å…¨å±€ç›‘æ§å™¨å®ä¾‹
_unification_monitor_instance = None

def get_unification_monitor() -> EntityUnificationMonitor:
    """è·å–ç»Ÿä¸€ç›‘æ§å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _unification_monitor_instance
    if _unification_monitor_instance is None:
        _unification_monitor_instance = EntityUnificationMonitor()
    return _unification_monitor_instance 